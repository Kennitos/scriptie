{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Not really an intro from me..."},{"metadata":{},"cell_type":"markdown","source":"I choose house prices as my first kernel on Kaggle because I hope to go over the entire analytic process in this kernel. We all know that data cleaning is the dirtist part of a whole project but it is also the foundation for further study. So you can see that the data cleaning part occupies most of the space. There are far more techniques that can be incorporated and more precise analyses can be done. I just point out the basic steps I think are mostly important. Now let us start!"},{"metadata":{},"cell_type":"markdown","source":"Roadmap:\n1. Import data\n1. Preliminary analysis\n    * For numerical variables: descriptive stats, distribution and normality test, correlation analysis\n    * For categorical variables: frequency table, correlation analysis \n1. Data preparation\n    * Missing data imputation\n    * For numerical variables: standardization or Yeo-Johnson transformation\n    * For categorical varaibles: one-hot encoding\n    * Principal component analysis\n1. Modeling\n    * Random Forests\n    * XGBoost\n    * Neural Networks"},{"metadata":{"_uuid":"2d2cb903a56a62c24709fffc76b52c9747d9d7f1"},"cell_type":"markdown","source":"## Import Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('white')\nsns.set_context('notebook', font_scale=1.5)\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a86c765f054e7370470d6a7a5f47252a470174b2"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preliminary Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Numeric Variables"},{"metadata":{},"cell_type":"markdown","source":"I split the analysis based on the different types of variables. It is easier to describe the numeric variables since we can calculate descriptive statistics and apply various statistical tests."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_num = train.select_dtypes(exclude=['object']).columns\ntrain_num","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. Basic statistics"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first line \"count\" indicates several variables that have missing values given the total number of observations is 1460, - LotFrontage, MasVnrArea, GarageYrBlt. LotFrontage's missing values are the most, which is 259. Judged from mean, quantiles, etc., we can also identify many skewed variables, for example, BsmtFinSF1 (Type 1 finished square feet), BsmtFullBath (Basement full bathrooms), LotArea (Lot size in square feet), MasVnrArea (Masonry veneer area in square feet)."},{"metadata":{},"cell_type":"markdown","source":"#### 2. Check the distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import shapiro\nstat, p = shapiro(train['BsmtFinSF1'])\nprint('Skewness=%.3f' %train['BsmtFinSF1'].skew())\nprint('Statistics=%.3f, p=%.3f' %(stat, p))\n\nalpha = 0.05\nif p > alpha:\n    print('Data looks normal (fail to reject H0)')\nelse:\n    print('Data does not look normal (reject H0)')\n\nsns.distplot(train['BsmtFinSF1']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BsmtFinSF1** is highly skewed and its distribution is not normal based on the Shapiro-Wilk test."},{"metadata":{"trusted":true},"cell_type":"code","source":"stat, p = shapiro(train['BsmtFullBath'])\nprint('Skewness=%.3f' %train['BsmtFullBath'].skew())\nprint('Statistics=%.3f, p=%.3f' %(stat, p))\n\nsns.distplot(train['BsmtFullBath']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BsmtFullBath** is moderately skewed but its distribution is not normal either because p value is smaller than 0.05. Acutally we can observe its distribution in the plot which has two peaks."},{"metadata":{"trusted":true},"cell_type":"code","source":"stat, p = shapiro(train['LotArea'])\nprint('Skewness=%.3f' %train['LotArea'].skew())\nprint('Statistics=%.3f, p=%.3f' %(stat, p))\n\nsns.distplot(train['LotArea']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another highly skewed variables **LotArea** has a long left tail. The majority of values are around 10,000 but it maximum value can reach to over 200,000."},{"metadata":{"trusted":true},"cell_type":"code","source":"stat, p = shapiro(np.log(train['LotArea']))\nprint('After log transformation...')\nprint('Skewness=%.3f' %np.log(train['LotArea']).skew())\nprint('Statistics=%.3f, p=%.3f' %(stat, p))\n\nsns.distplot(np.log(train['LotArea']));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After we apply the log transformation to LotArea, the skewness is reduced a lot but it is still not a normal distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"stat, p = shapiro(train['MasVnrArea'].dropna())\nprint('Skewness=%.3f' %train['MasVnrArea'].skew())\nprint('Statistics=%.3f, p=%.3f' %(stat, p))\n\nsns.distplot(train['MasVnrArea'].dropna());","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"masvnrarea_std = (train['MasVnrArea'] - np.mean(train['MasVnrArea'])) / np.std(train['MasVnrArea'])\nstat, p = shapiro(masvnrarea_std.dropna())\nprint('Skewness=%.3f' %masvnrarea_std.skew())\nprint('Statistics=%.3f, p=%.3f' %(stat, p))\n\nsns.distplot(masvnrarea_std.dropna());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try the standardization with **MasVnrArea**, but it does not change anything. "},{"metadata":{},"cell_type":"markdown","source":"**Now let us take a look at our response variable: SalePrice**"},{"metadata":{"trusted":true},"cell_type":"code","source":"stat, p = shapiro(train['SalePrice'])\nprint('Skewness=%.3f' %train['SalePrice'].skew())\nprint('Statistics=%.3f, p=%.3f' %(stat, p))\n\nsns.distplot(train['SalePrice']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stat, p = shapiro(np.log(train['SalePrice']))\nprint('After log transformation...')\nprint('Skewness=%.3f' %np.log(train['SalePrice']).skew())\nprint('Statistics=%.3f, p=%.3f' %(stat, p))\n\nsns.distplot(np.log(train['SalePrice']));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before log transformation, SalePrice is highly skewed. While log transformation can help with reducing skewness, it can not transform the data to normal distribution. But at least from the plot, we can see that it is improved a lot compared to before."},{"metadata":{},"cell_type":"markdown","source":"#### 3. Correlation analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_num_corr = train[train_num].drop(['Id'], axis=1)\ncorr = pd.DataFrame(train_num_corr.corr(method = 'pearson')['SalePrice'])\ncorr.sort_values(['SalePrice'], ascending= False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OverallQual (Overall material and finish quality), GrLivArea (Above grade (ground) living area square feet), GarageCars (Size of garage in car capacity), GarageArea (Size of garage in square feet), TotalBsmtSF (Total square feet of basement area), 1stFlrSF (First Floor square feet), etc. are highly correlated with our response variable, SalePrice. We can then conclude that area-related variables (including ground living area, garage area, basement area, number of rooms) as well as quality and year (like construction year, remodeled year) are the major continous factors associated with the sale price of a house."},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap = sns.cubehelix_palette(light = 0.95, as_cmap = True)\nsns.set(font_scale=1.2)\nplt.figure(figsize = (11, 11))\nsns.heatmap(abs(train_num_corr.corr(method = 'pearson')), vmin = 0, vmax = 1, square = True, cmap = cmap);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the last row of the heatmap, we can get the same information as from the previous correlation table. Other than this, we can also find the multicollinearity problems with the data, for example, GarageCars and GarageArea, YearBuilt and GarageYrBlt."},{"metadata":{},"cell_type":"markdown","source":"### Categorical Variables"},{"metadata":{},"cell_type":"markdown","source":"#### 1. Check the distribution"},{"metadata":{},"cell_type":"markdown","source":"For categorical variables, I make a frequency table to show the distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cat = train.select_dtypes(include=['object']).columns\ntrain_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 300)\ndf_output = pd.DataFrame()\nfor i in range(len(train_cat)):\n    c = train_cat[i]\n    df = pd.DataFrame({'Variable':[c]*len(train[c].unique()),\n                       'Level':train[c].unique(),\n                       'Count':train[c].value_counts(dropna = False)})\n    df['Percentage'] = 100 * df['Count']  / df['Count'].sum()\n    df_output = df_output.append(df, ignore_index = True)\n    \ndf_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Judged from the frequency table, we can infer that the majority of categorical variables are unbalanced. Several classes even only have 1 or 2 observations. Worsely, Alley, FireplaceQu, PoolQC, Fence, MiscFeature contain missing values over 50%."},{"metadata":{},"cell_type":"markdown","source":"#### 2. Correlation with SalePrice"},{"metadata":{},"cell_type":"markdown","source":"Take several variables for example:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style = 'whitegrid', rc = {'figure.figsize':(10,7), 'axes.labelsize':12})\nsns.boxplot(x = 'MSZoning', y = 'SalePrice', palette = 'Set2', data = train, linewidth = 1.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MSZoning**. Most of houses are in the Residential Low Density (79%) and Residential Medium Density (15%) areas. However, there is a large variance within the Residential Low (RL) zone. Floating Village(FV) is a special area where a retirement community was developed and have the highest median price among the all. But there are only 16 observations falling within this category and their prices do not seem quite stable looking at the shape of the box of FV."},{"metadata":{"trusted":true},"cell_type":"code","source":"col_order = train.groupby(['Neighborhood'])['SalePrice'].aggregate(np.median).reset_index().sort_values('SalePrice')\np = sns.boxplot(x = 'Neighborhood', y = 'SalePrice', palette = 'Set2', data = train, order=col_order['Neighborhood'], linewidth = 1.5)\nplt.setp(p.get_xticklabels(), rotation=45);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Neighborhood**. Besides the zoning classfication, neighborhood also makes a difference. Houses located at Northridge Heights (NridgHt) have higher sale prices than those in other areas generally but the variance is large. The difference between median price of MeadowV (neighborhood with the lowest house prices) and that of NridgHt is over $200,000."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = 'HouseStyle', y = 'SalePrice', palette = 'Set2', data = train, linewidth = 1.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**HouseStyle**. Locations matter a lot when considering house prices, then what about the characteristics of house itself? Popular house styles are 2 Story (50%) and 1 Story (30%). 2 story and 2.5 story (2nd level finished) houses can be sold at relatively higher prices, around 200,000 dollars, while the prices of 1.5 story (2nd level unfinished) houses are mostly around 110,000 dollars. Notably, for multiple story houses, 2 level finished or unfinished have an obvious relationship with house prices."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = 'YearBuilt', y = 'SalePrice', data = train, hue = 'HouseStyle', style = 'HouseStyle', palette = 'colorblind');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After introducing year of built into our plot, we can see that houses built in recent years tend to have higher sale prices, especially since 1960. YearBuilt also has certain association with HouseStyle. We can identify at least three clusters in this plot, blue points (2Story) mostly near the right, yellow (1Story) most in the between, and green (1.5Fin) on the left. \n\nIntuitively, we assume that the price of house with more stories is higher. For example, when there are 2 story houses built in like 1930, they were sold usually at higher prices than green (1.5Fin) and yellow (1Story) points. But from the box plot we already see that 1.5Fin is less expensive than 1Story. This may be explained by YearBuilt since majority of green 1.5Fin were built between 1900 and 1940 while yellow 1Story between 1940 and 1980 and many were built after 2000."},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{"_uuid":"b8166c728fb8255b56f41de8ebf02cc880725e83"},"cell_type":"markdown","source":"### ** Impute Missing Values **"},{"metadata":{"_uuid":"0c22d24e98ae26ae2b48bd1e906e08d05d7efc88"},"cell_type":"markdown","source":"#### 1. Drop columns where there is a large percentage of missing data"},{"metadata":{"trusted":true,"_uuid":"c912533caaf3004622a480b89b29695a44539f87"},"cell_type":"code","source":"print(train.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How to define \"a large percentage\"? Let us try 15% first."},{"metadata":{"trusted":true,"_uuid":"7de3d5eaaa469f69602c377c71f97ef47363290e"},"cell_type":"code","source":"# training data\ntrain_missing = pd.DataFrame(train.isnull().sum()/len(train.index) * 100)\ntrain_missing.columns = ['percent']\ntrain_missing.loc[train_missing['percent'] > 15, 'column_select'] = True\ntrain_col_select = train_missing.index[train_missing['column_select'] == True].tolist()\ntrain_col_select","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"965ec9dcaf83617f90b78f3346b494aed2b065a6"},"cell_type":"code","source":"# test data\ntest_missing = pd.DataFrame(test.isnull().sum()/len(test.index) * 100)\ntest_missing.columns = ['percent']\ntest_missing.loc[test_missing['percent'] > 15, 'column_select'] = True\ntest_col_select = test_missing.index[test_missing['column_select'] == True].tolist()\ntest_col_select","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We know that Alley, FireplaceQu, PoolQC, Fence, MiscFeature are all categorical variables and their missing values occupy over 50% of total. They should be dropped. But LogFrontage has 1201 rows which I personally decide to keep."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_col_select.pop(0)\ntest_col_select.pop(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc5a1015e89854ea7e946579a8a687e18cc99795"},"cell_type":"code","source":"train.drop(train_col_select, inplace = True, axis = 1, errors = 'ignore')\ntest.drop(test_col_select, inplace = True, axis = 1, errors = 'ignore')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that we orginally have 81 variables for training set and 81 for test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"feb838ff7bf2b9661fe79e030d1f17d1b9b426cb"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0212082d61ef7d62eeffc49c27c52a64f17f0fb"},"cell_type":"markdown","source":"#### 2. Use medians for numeric variables and the most frequent values for non-numeric variables to replace NA"},{"metadata":{"trusted":true,"_uuid":"769d284c5186a4ecd0b049c513669fb372ce0aae"},"cell_type":"code","source":"from sklearn.base import TransformerMixin\nclass MissingDataImputer(TransformerMixin):\n    def fit(self, X, y=None):\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n            index=X.columns)\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59c213e4cad0ad81bd1516cf1f1b4661c3e84feb"},"cell_type":"code","source":"train_nmissing = MissingDataImputer().fit_transform(train.iloc[:,1:-1])\ntest_nmissing = MissingDataImputer().fit_transform(test.iloc[:,1:])\ntrain_nmissing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bf96304c203cd45c66a27d47b44ec68bba1542a"},"cell_type":"code","source":"print(train_nmissing.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a146416ed9c062f8d1e6e32e622b0380202a6a9"},"cell_type":"markdown","source":"### **Transform the Data**"},{"metadata":{"_uuid":"eb3d947ad1829e9d469d84d8a880358b18809c04"},"cell_type":"markdown","source":"#### 1. For categorical variables, apply onehot encoding"},{"metadata":{"_uuid":"93eedcb87c4c8763e1d9ea9b8872ce9589de30b9"},"cell_type":"markdown","source":"1)  Check if training data and test data have the same categorical variables"},{"metadata":{"trusted":true,"_uuid":"89523dd1373218ce19eef405a7ab0193422932a3"},"cell_type":"code","source":"train_cat = train_nmissing.select_dtypes(include=['object']).columns\ntest_cat = test_nmissing.select_dtypes(include=['object']).columns\ntrain_cat.difference(test_cat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76f7ef18a46bcbd1eeaf243d20495aca3894a390"},"cell_type":"markdown","source":"2) Create dummy variables"},{"metadata":{},"cell_type":"markdown","source":"I am not using LabelEncoder because for most categorical variables their values are not in order. So I would prefer one hot encoding. To ensure that the output matrix are in the same order in training and test data, I decide to create the one-hot arrays with get_dummies instead of sklearn OneHotEncoder."},{"metadata":{"trusted":true,"_uuid":"b466d2d4725674bbac7eed368d4153103c627281"},"cell_type":"code","source":"train_w_dummy = pd.get_dummies(train_nmissing, prefix_sep='_', drop_first=True, columns=train_cat)\ntest_w_dummy = pd.get_dummies(test_nmissing, prefix_sep='_', drop_first=True, columns=test_cat)\n\ncat_dummies = [col for col in train_w_dummy \n               if '_' in col \n               and col.split('_')[0] in train_cat]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcada3a2ec4760702b4cf6d82964f1955b2b5c82"},"cell_type":"markdown","source":"3)  Remove additional variales and add missing variables in test data"},{"metadata":{"trusted":true,"_uuid":"4a395612ca5df8552be9181b53728cb2f2d5dc92"},"cell_type":"code","source":"for col in test_w_dummy.columns:\n    if (\"_\" in col) and (col.split(\"_\")[0] in train_cat) and col not in cat_dummies:\n        test_w_dummy.drop(col, axis=1, inplace=True)\n        \nfor col in cat_dummies:\n    if col not in test_w_dummy.columns:\n        test_w_dummy[col] = 0        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"790e6f1f0fa2dc652b740cd5956dd057408b4932"},"cell_type":"markdown","source":"4) Make sure that variables in test data have the same order as in training data"},{"metadata":{"trusted":true,"_uuid":"b3bc33e82ad4e1a33fb89c94f6a4213a5b96ccc3"},"cell_type":"code","source":"train_cols = list(train_w_dummy.columns[:])\ntest_w_dummy = test_w_dummy[train_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remember that we have 76 for training and 75 for test before..."},{"metadata":{"trusted":true,"_uuid":"e243fa3860fea66b737c243c881c23e682602fb9"},"cell_type":"code","source":"train_w_dummy.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7ac9ce170b7a8eb146013e1d7d76c4598124c7b"},"cell_type":"code","source":"test_w_dummy.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"378f4994945577f18c613e44b2e1c079dbaef27d"},"cell_type":"markdown","source":"#### 2. For numeric variables, normalize the data"},{"metadata":{"_uuid":"d1a4f47f6fc0b481a3acbeafe7f9dc37a0389719"},"cell_type":"markdown","source":"1) Check if training data and test data have the same numeric variables"},{"metadata":{"trusted":true,"_uuid":"c9bc60e9046ccdd7c35b099c7da32a9c052b1710"},"cell_type":"code","source":"train_num = train_nmissing.select_dtypes(exclude=['object']).columns\ntest_num = test_nmissing.select_dtypes(exclude=['object']).columns\ntest_num.difference(train_num)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"439648c9ac9143aaec692d21045583ed67b9db80"},"cell_type":"markdown","source":"2) Normalize test data: standardization or Yeo-Johnson transformation"},{"metadata":{},"cell_type":"markdown","source":"As we have experimented before, standardization does not change the shape of the distribution in nature but log transformation do. Log transformation requires data to be positive while Yeo-Johnson transformation supports both positive or negative data. "},{"metadata":{},"cell_type":"markdown","source":"I simply choose skewness as the criterion to select between standardization and Yeo-Johnson transformation because I am not training my data on linear regression models. There are of course more rigorous criteria."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_num_std = [col for col in train_num if abs(train_w_dummy[col].skew()) <= 1]\ntrain_num_yjt = [col for col in train_num if abs(train_w_dummy[col].skew()) > 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_num_yjt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ad8665759ac3dc3ad5ba3a2e406c7bdc2fb9ee4"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\n\n# standardization\nscaler = StandardScaler().fit(train_w_dummy[train_num_std].values)\ntrain_w_dummy[train_num_std] = scaler.transform(train_w_dummy[train_num_std].values)\ntest_w_dummy[train_num_std] = scaler.transform(test_w_dummy[train_num_std].values)\n\n# power transform\npt = PowerTransformer().fit(train_w_dummy[train_num_yjt].values)\ntrain_w_dummy[train_num_yjt] = pt.transform(train_w_dummy[train_num_yjt].values)\ntest_w_dummy[train_num_yjt] = pt.transform(test_w_dummy[train_num_yjt].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82deabe8ffda6c9018db7039f362ee5cf579171f"},"cell_type":"code","source":"test_w_dummy.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Apply Principal Component Analysis"},{"metadata":{},"cell_type":"markdown","source":"To address the multicolliearity problem, I apply PCA to decrease the number of variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA().fit(train_w_dummy)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components', fontsize = 12)\nplt.ylabel('cumulative explained variance', fontsize = 12)\nplt.grid(True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear to notice that the first 100 variables contain nearly 100% of the variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components = 100)\nx_train = pca.fit_transform(train_w_dummy) \nx_test = pca.transform(test_w_dummy) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a95ee93c9cbd1ef697c60b3ac7cc803cbe785b1"},"cell_type":"markdown","source":"## **Start Training**"},{"metadata":{},"cell_type":"markdown","source":"I am not going to use regression models for now because of the assumptions they make on the data. And my transformation is just to speed up convergence during training. Instead, I will try with random forests, boosting, and neural networks."},{"metadata":{"trusted":true,"_uuid":"e66ace7ba9707a39bb23101e53fd0e41e8fb3ba7"},"cell_type":"code","source":"# do not forget to log transform our response variable\ny_train = np.log(train['SalePrice']).values\n\ny_test_data = pd.read_csv('../input/sample_submission.csv')\ny_test = y_test_data['SalePrice'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f489209d9589147b4d3fb55a2411e0bc43456ea5"},"cell_type":"markdown","source":"#### 1. Random Forest\n"},{"metadata":{"trusted":true,"_uuid":"db8a5dab86e95d91b0e8280e0c3072b5cf2cce47"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom pprint import pprint\nrf_base = RandomForestRegressor(n_estimators=400)\n\n# Look at parameters used by our current forest\nprint('Parameters of base model:')\npprint(rf_base.get_params())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Hyperparameter tuning"},{"metadata":{},"cell_type":"markdown","source":"1) Randomized search"},{"metadata":{},"cell_type":"markdown","source":"Compared to grid search, randomized search is less time-consuming so we start from a wider range of parameters with randomized search. In this case, 100 sets of parameters are sampled from all combinations to train."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nmax_depth = [int(x) for x in np.linspace(40, 100, num = 4)]\nmax_depth.append(None)\nrandom_params = {'n_estimators': [int(x) for x in np.linspace(start = 400, stop = 1000, num = 4)],\n                'max_depth': max_depth,\n                'min_samples_split': [2, 5, 10],\n                'min_samples_leaf': [1, 2, 4]}\n\nrf = RandomForestRegressor()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_params, n_iter = 100, cv = 3, n_jobs = -1, verbose = 2, random_state = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random.fit(x_train, y_train)\nrf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# base model result\nfrom sklearn import metrics\n\nrf_base.fit(x_train, y_train)\ny_pred_rf_base = np.exp(rf_base.predict(x_test))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_rf_base))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_rf_base))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_rf_base)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random search with best performance parameters\nrf_random_best = rf_random.best_estimator_\ny_pred_rf_random = np.exp(rf_random_best.predict(x_test))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_rf_random))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_rf_random))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_rf_random)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Through randomized search, we can see that the performance of the random forest is improved."},{"metadata":{},"cell_type":"markdown","source":"2) Grid search"},{"metadata":{},"cell_type":"markdown","source":"More refined tuning through grid search."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\ngrid_params = {'max_depth': [40, 50, 60],\n               'min_samples_leaf': [3, 4, 5],\n               'min_samples_split': [4, 5, 6],\n               'n_estimators': [600, 800]}\n\nrf_grid = GridSearchCV(estimator = rf, param_grid = grid_params, cv = 3, n_jobs = -1, verbose = 2)\nrf_grid.fit(x_train, y_train)\nrf_grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grid search with best performance parameters\nrf_grid_best = rf_grid.best_estimator_\ny_pred_rf_grd = np.exp(rf_grid_best.predict(x_test))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_rf_grd))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_rf_grd))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_rf_grd)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction = pd.DataFrame(y_pred_rf_grd, columns = ['SalePrice'])\n#result = pd.concat([y_test_data['Id'], prediction], axis = 1)\n#result.to_csv('./Predictions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also improved a little bit compared to the model specified using randomized search."},{"metadata":{"_uuid":"0beb6cb9fedb98e4a1523469413d901f38d06c6b"},"cell_type":"markdown","source":"#### 2. XGBoost"},{"metadata":{"trusted":true,"_uuid":"fa409cccbce248e13bef2f1f23cf57ab1301e3a2"},"cell_type":"code","source":"from xgboost import XGBRegressor\n\nxgb_base = XGBRegressor()\nxgb_base.fit(x_train, y_train)\ny_pred_xgb_base = np.exp(xgb_base.predict(x_test))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_xgb_base))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_xgb_base))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_xgb_base)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Waiting for parameter optimization..."},{"metadata":{"trusted":true},"cell_type":"code","source":"#random_params = {}\n\n#xgb_random = RandomizedSearchCV(estimator = xgb_base, param_distributions = random_params, n_iter = 100, cv = 3, n_jobs = -1, verbose = 2, random_state = 12)\n#xgb_random.fit(x_train, y_train)\n#xgb_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xgb_random_best = xgb_random.best_estimator_\n#y_pred_xgb_random = np.exp(xgb_random_best.predict(x_test))\n#print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_xgb_random))  \n#print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_xgb_random))  \n#print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_xgb_random)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Neural Networks"},{"metadata":{},"cell_type":"markdown","source":"Waiting for update..."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
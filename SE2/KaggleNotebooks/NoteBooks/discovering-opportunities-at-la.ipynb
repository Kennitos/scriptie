{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Data Science for Good: City of Los Angeles"},{"metadata":{},"cell_type":"markdown","source":"![](https://media.giphy.com/media/1d7E5ogkVOdY4yiMPY/giphy.gif)"},{"metadata":{},"cell_type":"markdown","source":"### Problem statement"},{"metadata":{},"cell_type":"markdown","source":"The goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to:\n1.  identify language that can negatively bias the pool of applicants; \n2.  improve the diversity and quality of the applicant pool; and/or \n3.  make it easier to determine which promotions are available to employees in each job class."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from wand.image import Image as Img\nImg(filename='../input/cityofla/CityofLA/Additional data/PDFs/2017/july 2017/July 21/ARTS ASSOCIATE 2454 072117 REV 072817.pdf', resolution=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What is in this kernel?"},{"metadata":{},"cell_type":"markdown","source":"1. As my first step toward solving the problem,i will focus on reading the job bulletin files and extracting \n    the required data from it.\n2. Exploratory data analysis\n- NOTE : kernel under construction,more to come !"},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport os\nimport numpy as np\nfrom datetime import datetime\nfrom collections  import Counter\nfrom nltk import word_tokenize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport calendar\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nprint(os.listdir(\"../input\"))\nfrom gensim.models import word2vec\nfrom sklearn.manifold import TSNE\nfrom nltk import pos_tag\nfrom nltk.help import upenn_tagset\nimport gensim\nimport matplotlib.colors as mcolors\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### checking all subdirectories"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"files=[dir for dir in os.walk('../input/cityofla')]\nfor file in files:\n    print(os.listdir(file[0]))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- job bulletins :This directory contains the job bulletins in text format.\n- additional data :This directory contains additional data in pdf and csv format. "},{"metadata":{"trusted":true},"cell_type":"code","source":"bulletins=os.listdir(\"../input/cityofla/CityofLA/Job Bulletins/\")\nadditional=os.listdir(\"../input/cityofla/CityofLA/Additional data/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will find and print all the files inside **Additional data** that has csv format."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"csvfiles=[]\nfor file in additional:\n    if file.endswith('.csv'):\n        print(file)\n        csvfiles.append(\"../input/cityofla/CityofLA/Additional data/\"+file)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 3 comma seperated files inside the folder,\n1. job titles : contains the title given to different jobs available.\n2. sample job class export template.csv : contains sample job bulletin to csv export details.\n3. kaggle_data_dictionary : contains name and description of each column that is in sample job class export template."},{"metadata":{},"cell_type":"markdown","source":"## Reading the required csv files"},{"metadata":{"trusted":true},"cell_type":"code","source":"job_title=pd.read_csv(csvfiles[0])\nsample_job=pd.read_csv(csvfiles[1])\nkaggle_data=pd.read_csv(csvfiles[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting  basic ideas"},{"metadata":{},"cell_type":"markdown","source":"In the below section we will take a look at the three csv files which was just loaded to get basic understanding of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"job_title.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The are %d rows and %d cols in job_title file\" %(job_title.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_job.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The are %d rows and %d cols in sample_job file\" %(sample_job.shape))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kaggle_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The are %d rows and %d cols in kaggle_data file\" %(kaggle_data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are %d text files in bulletin directory\" %len(bulletins))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Extracting the headings from job bulletins"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_headings(bulletin):       \n    \n    \"\"\"\"function to get the headings from text file\n        takes a single argument\n        1.takes single argument list of bulletin files\"\"\"\n    \n    with open(\"../input/cityofla/CityofLA/Job Bulletins/\"+bulletins[bulletin]) as f:    ##reading text files \n        data=f.read().replace('\\t','').split('\\n')\n        data=[head for head in data if head.isupper()]\n        return data\n        \ndef clean_text(bulletin):      \n    \n    \n    \"\"\"function to do basic data cleaning\n        takes a single argument\n        1.takes single argument list of bulletin files\"\"\"\n                                            \n    \n    with open(\"../input/cityofla/CityofLA/Job Bulletins/\"+bulletins[bulletin]) as f:\n        data=f.read().replace('\\t','').replace('\\n','')\n        return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Headings"},{"metadata":{},"cell_type":"markdown","source":"We will print headings from first two job bulletins file."},{"metadata":{"trusted":true},"cell_type":"code","source":"get_headings(1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_headings(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- we can observe that there is some specefic pattern or format which is kept while writing job bulletins.\n- The order of the headings almost coincides with each other,which will be beneficial for our task."},{"metadata":{},"cell_type":"markdown","source":"## Extracting features"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\ndef to_dataframe(num,df):\n    \"\"\"\"function to extract features from job bulletin text files and convert to\n    pandas dataframe.\n    function take two arguments \n                        1.the number of files to be read\n                        2.dataframe object                                      \"\"\"\n    \n\n    \n    opendate=re.compile(r'(Open [D,d]ate:)(\\s+)(\\d\\d-\\d\\d-\\d\\d)')       #match open date\n    \n    salary=re.compile(r'\\$(\\d+,\\d+)((\\s(to|and)\\s)(\\$\\d+,\\d+))?')       #match salary\n    \n    requirements=re.compile(r'(REQUIREMENTS?/\\s?MINIMUM QUALIFICATIONS?)(.*)(PROCESS NOTE)')      #match requirements\n    \n    for no in range(0,num):\n        with open(\"../input/cityofla/CityofLA/Job Bulletins/\"+bulletins[no],encoding=\"ISO-8859-1\") as f:         #reading files \n                try:\n                    file=f.read().replace('\\t','')\n                    data=file.replace('\\n','')\n                    headings=[heading for heading in file.split('\\n') if heading.isupper()]             ##getting heading from job bulletin\n\n                    sal=re.search(salary,data)\n                    date=datetime.strptime(re.search(opendate,data).group(3),'%m-%d-%y')\n                    try:\n                        req=re.search(requirements,data).group(2)\n                    except Exception as e:\n                        req=re.search('(.*)NOTES?',re.findall(r'(REQUIREMENTS?)(.*)(NOTES?)',\n                                                              data)[0][1][:1200]).group(1)\n                    \n                    duties=re.search(r'(DUTIES)(.*)(REQ[A-Z])',data).group(2)\n                    try:\n                        enddate=re.search(\n                                r'(JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER)\\s(\\d{1,2},\\s\\d{4})'\n                                ,data).group()\n                    except Exception as e:\n                        enddate=np.nan\n                    \n                    selection= [z[0] for z in re.findall('([A-Z][a-z]+)((\\s\\.\\s)+)',data)]     ##match selection criteria\n                    \n                    df=df.append({'File Name':bulletins[no],'Position':headings[0].lower(),'salary_start':sal.group(1),\n                               'salary_end':sal.group(5),\"opendate\":date,\"requirements\":req,'duties':duties,\n                                'deadline':enddate,'selection':selection},ignore_index=True)\n                    \n                except Exception as e:\n                    print('umatched sequence')\n                    \n                    \n                \n                \n        \n           \n    return df\n\n            \n            \n            \n            ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"df=pd.DataFrame(columns=['File Name','Position','salary_start','salary_end','opendate','requirements','duties','deadline'])\ndf=to_dataframe(len(bulletins),df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Now we have a proper comma seperataed file containing most of the information we need.we will now start exploring it. "},{"metadata":{},"cell_type":"markdown","source":"## which are the common job sectors in LA?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are %d different jobs available' %df['Position'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\ntext=''.join(job for job in df['Position'])                                ##joining  data to form text\ntext=word_tokenize(text)\njobs=Counter(text)                                                         ##counting number of occurences\njobs_class=[job for job in jobs.most_common(12) if len(job[0])>3]          ##selecting most common words\n#offers=[job[1] for job in jobs.most_common(12) if len(job[0]>3)]\na,b=map(list, zip(*jobs_class))\nsns.barplot(b,a,palette='rocket')                                           ##creating barplot\nplt.title('Job sectors')\nplt.xlabel(\"count\")\nplt.ylabel('sector')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that service sector dominates in creating opputunities."},{"metadata":{},"cell_type":"markdown","source":"## Which are the best paid jobs in LA?"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\"\n    convert salary to proper  form \n    by removing '$' and ',' symbols.\n                                    \"\"\"\n\ndf['salary_start']=[int(sal.split(',')[0]+sal.split(',')[1] ) for sal in df['salary_start']]   \ndf['salary_end']=[sal.replace('$','')  if sal!= None else 0 for sal in df['salary_end']  ]\ndf['salary_end']=[int(sal.split(',')[0]+sal.split(',')[1] ) if type(sal)!=int else 0 for sal in df['salary_end']]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"'''finding the most paid 10 jobs at LA'''\n\nmost_paid=df[['Position','salary_start']].sort_values(by='salary_start',ascending=False)[:10]\nplt.figure(figsize=(7,5))\nsns.barplot(y=most_paid['Position'],x=most_paid['salary_start'],palette='rocket')\nplt.title('Best paid jobs in LA')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Which the jobs with highest salary deviation?"},{"metadata":{"trusted":true},"cell_type":"code","source":"''''calculating salary start - salary end '''\n\ndf['salary_diff']=abs(df['salary_start']-df['salary_end'])\n\nranges=df[['Position','salary_diff']].sort_values(by='salary_diff',ascending=False)[:10]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,5))\nsns.barplot(y=ranges['Position'],x=ranges['salary_diff'],palette='RdBu')   ##plotting\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"ranges","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Has job opportunities really increased recently?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"'''Extracting year out of opendate timestamp object and counting\n    the number of each occurence of each year using count_values() '''\n\ndf['year_of_open']=[date.year for date in df['opendate']]\n\ncount=df['year_of_open'].value_counts(ascending=True)\nyears=['2018', '2017', '2016', '2015', '2014', '2019', '2012', '2020', '2013', '2008', '2006',\n           '2005', '2002', '1999']\nplt.figure(figsize=(7,5))\nplt.plot([z for z in reversed(years)],count.values,color='blue')\n\nplt.title('Oppurtunities over years')\nplt.xlabel('years')\nplt.ylabel('count')\nplt.gca().set_xticklabels([z for z in reversed(years)],rotation='45')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It is evident from the above graph that job oppurtunities is constantly increasing after 2012 or so. \n- job oppurtunities has never decreased."},{"metadata":{},"cell_type":"markdown","source":"## Which month of the year offers most opportunities?"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Extracting month out of opendate timestamp object and counting\n    the number of each occurence of each months using count_values() '''\n\n\nplt.figure(figsize=(7,5))\ndf['open_month']=[z.month for z in df['opendate']]\ncount=df['open_month'].value_counts(sort=False)\nsns.barplot(y=count.values,x=count.index,palette='rocket')\nplt.gca().set_xticklabels([calendar.month_name[x] for x in count.index],rotation='45')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is more job opportunities created in the months of **March,October and December**"},{"metadata":{},"cell_type":"markdown","source":"## Which day of the week ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Extracting weekday out of opendate timestamp object and counting\n    the number of each occurence of each weekday using count_values() '''\n\n\nplt.figure(figsize=(7,5))\n\ndf['open_day']=[z.weekday() for z in df['opendate']]\ncount=df['open_day'].value_counts(sort=False)\nsns.barplot(y=count.values,x=count.index,palette='rocket')\nplt.gca().set_xticklabels([calendar.day_name[x] for x in count.index],rotation='45')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow ! All the postings are open from **friday** ! This is pretty interesting.\n- Is there any specefic reason behind this?\nWe will try to find out."},{"metadata":{},"cell_type":"markdown","source":"### what about deadlines ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('%d job applications may close without prior notice' %df['deadline'].isna().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df['dealine']=df['deadline'].fillna(method='backfill',inplace=True)\n#df['deadline']=[datetime.strptime(x,'%B %d, %Y')  for x in df['deadline'] ]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"req=' '.join(text for text in df['requirements'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Cloud of Requirements"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef show_wordcloud(data, title = None):\n    \n    \n    '''funtion to produce and display wordcloud\n        taken 2 arguments\n        1.data to produce wordcloud\n        2.title of wordcloud'''\n    \n    \n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=set(STOPWORDS),\n        max_words=250,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\nshow_wordcloud(text,'REQUIREMENTS')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Most influential words in requirements"},{"metadata":{"trusted":true},"cell_type":"code","source":"lem=WordNetLemmatizer()\ntext=[lem.lemmatize(w) for w in word_tokenize(req)]\nvect=TfidfVectorizer(ngram_range=(1,3),max_features=100)\nvectorized_data=vect.fit_transform(text)\n#id_map=dict((v,k) for k,v in vect.vocabulary_.items())\nvect.vocabulary_.keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word 2 Vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_corpus(df,col):\n    \n    '''function to build corpus from dataframe'''\n    lem=WordNetLemmatizer()\n    corpus= []\n    for x in df[col]:\n        \n        \n        words=word_tokenize(x)\n        corpus.append([lem.lemmatize(w) for w in words])\n    return corpus\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=build_corpus(df,'requirements')\nmodel = word2vec.Word2Vec(corpus, size=100, window=20, min_count=30, workers=4)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TSNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tsne_plot(model,title='None'):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=80, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(12, 12)) \n    plt.title(title)\n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_plot(model,'Requirements')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What are the common requirements for any post?"},{"metadata":{"trusted":true},"cell_type":"code","source":"token=word_tokenize(req)\ncounter=Counter(token)\ncount=[x[0] for x in counter.most_common(40) if len(x[0])>3]\nprint(\"Most common words in Requirement\")\nprint(count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It can be observed that companies prefer  \n- **experienced** \n- **educated professionals**  having **degree from an accredicted university**\n- also willing to work **full-time**"},{"metadata":{},"cell_type":"markdown","source":"## Duties"},{"metadata":{"trusted":true},"cell_type":"code","source":"duties= ' '.join(d for d in df['duties'])\nshow_wordcloud(duties,'Duties')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Most influential and common words in duties"},{"metadata":{"trusted":true},"cell_type":"code","source":"lem=WordNetLemmatizer()\ntext=[lem.lemmatize(w) for w in word_tokenize(duties)]\nvect=TfidfVectorizer(ngram_range=(1,3),max_features=200)\nvectorized_data=vect.fit_transform(text)\n#id_map=dict((v,k) for k,v in vect.vocabulary_.items())\nvect.vocabulary_.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token=word_tokenize(duties)\ncounter=Counter(token)\ncount=[x[0] for x in counter.most_common(40) if len(x[0])>3]\nprint(\"Most common words in Requirement\")\nprint(count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word 2 Vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=build_corpus(df,'duties')\nmodel = word2vec.Word2Vec(corpus, size=100, window=20, min_count=40, workers=4)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TSNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_plot(model,'Duties')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Latent Dirichlet Allocation (LDA)"},{"metadata":{},"cell_type":"markdown","source":"Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions."},{"metadata":{"trusted":true},"cell_type":"code","source":"lem=WordNetLemmatizer()\ntext=[lem.lemmatize(w) for w in word_tokenize(duties)]\nvect=TfidfVectorizer(ngram_range=(1,3),max_features=200)\nvectorized_data=vect.fit_transform(text)\nid2word=dict((v,k) for k,v in vect.vocabulary_.items())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=gensim.matutils.Sparse2Corpus(vectorized_data,documents_columns=False)\nldamodel = gensim.models.ldamodel.LdaModel(corpus,id2word=id2word,num_topics=8,random_state=34,passes=25,per_word_topics=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ldamodel.show_topic(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  What is the Dominant topic and its percentage contribution in each document"},{"metadata":{},"cell_type":"markdown","source":"In LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The below code extracts this dominant topic for each sentence and shows the weight of the topic and the keywords in a nicely formatted output."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def format_topics_sentences(ldamodel, corpus, texts):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=ldamodel, corpus=corpus, texts=build_corpus(df,'duties'))\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\ndf_dominant_topic.dropna(inplace=True)\ndf_dominant_topic.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Topic modeling visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=build_corpus(df,'duties')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word Counts of Topic Keywords"},{"metadata":{},"cell_type":"markdown","source":"When it comes to the keywords in the topics, the importance (weights) of the keywords matters. Along with that, how frequently the words have appeared in the documents is also interesting to look.\n\nLet’s plot the word counts and the weights of each keyword in the same chart.\n\nYou want to keep an eye out on the words that occur in multiple topics and the ones whose relative frequency is more than the weight. Often such words turn out to be less important. The chart I’ve drawn below is a result of adding several such words to the stop words list in the beginning and re-running the training process."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"topics = ldamodel.show_topics(formatted=False)\ndata_flat = [w for w_list in build_corpus(df,'duties') for w in w_list]\ncounter = Counter(data_flat)\n\nout = []\nfor i, topic in topics:\n    for word, weight in topic:\n        out.append([word, i , weight, counter[word]])\n\ndf_plot= pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n\n# Plot Word Count and Weights of Topic Keywords\nfig, axes = plt.subplots(4, 2, figsize=(10,12), sharey=True, dpi=160)\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=df_plot.loc[df_plot.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=df_plot.loc[df_plot.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    #ax_twin.set_ylim(0, 0.040); ax.set_ylim(0, 4000)\n    ax.set_title('Topic: ' + str(i), color=cols[i])\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(df_plot.loc[df_plot.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)    \nfig.suptitle('Word Count and Importance of Topic Keywords',y=1)    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What are the most common selection criterias?"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(7,7))\ncount=df['selection'].astype(str).value_counts()[:10]\nsns.barplot(y=count.index,x=count,palette='rocket')\nplt.gca().set_yticklabels(count.index,rotation='45')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It is evident that  **interview** ,**Essay** and **Questionnaire** are the most common selection criterias."},{"metadata":{},"cell_type":"markdown","source":"### Is there any Gender bias in job bulletins?"},{"metadata":{},"cell_type":"markdown","source":"In the follow section i am trying investigate if there is any gender biased terms used in **Requirement** and **Duties** section of the job bulletin.   \nFor that i will pos tag all the text data in the requirement field and then,\n- Extract the words having pronoun tag.\n- check if any gender biased terms like he/she is used in the field.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pronoun(data):\n    \n    '''function to tokenize data and perform pos_tagging.Returns tokens having \"PRP\" tag'''\n    \n    prn=[]\n    vrb=[]\n    token=word_tokenize(data)\n    pos=pos_tag(token)\n   \n    vrb=Counter([x[0] for x in pos if x[1]=='PRP'])\n    \n    return vrb\n    \n\n\nreq_prn=pronoun(req)\nduties_prn=pronoun(duties)\nprint('pronouns used in requirement section are')\nprint(req_prn.keys())\nprint('\\npronouns used in duties section are')\nprint(duties_prn.keys())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Surprisingly, i couldn't find any gender biased or racist pronouns in **Requirement ** or **Duties section**\n2. you can see all the pronouns used are neutral."},{"metadata":{},"cell_type":"markdown","source":"##          More to come,stay tuned !"},{"metadata":{},"cell_type":"markdown","source":"### If you like my kernel please consider upvoting.Thank you :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}
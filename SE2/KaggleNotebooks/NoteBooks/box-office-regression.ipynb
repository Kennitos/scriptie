{"cells":[{"metadata":{},"cell_type":"markdown","source":"1.  [EDA](#idEDA)\n2. [Feature Engineering](#idFeature_Engineering)\n3.  [Prediction](#idPrediction)\n4.  [Analysis of Prediction](#idAnalysis_of_Prediction)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntrain_orig = pd.read_csv('../input/train.csv')\ntest_orig = pd.read_csv('../input/test.csv')\nsubm = pd.DataFrame()\nsubm['id'] = test_orig.id.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"idEDA\">EDA</div>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_orig['bool_belongs_to_collection'] = (train_orig['belongs_to_collection'].notnull()).astype(int)\ntest_orig['bool_belongs_to_collection'] = (test_orig['belongs_to_collection'].notnull()).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_orig['split'] = 'train'\ntest_orig['split'] = 'test'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_test = pd.concat([train_orig[['popularity','budget','split','bool_belongs_to_collection']], test_orig[['popularity','budget','split','bool_belongs_to_collection']]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nsns.scatterplot(x=\"popularity\", y=\"budget\", hue=\"split\", data=train_test,ax=ax)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(18.5, 10.5)\nsns.scatterplot(x=\"popularity\", y=\"budget\", hue=\"split\",style='bool_belongs_to_collection', data=train_test,ax=ax, alpha=0.4)\nax.set_xlim([0,100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/jlove5/avocados-usa-prices\n\ng = sns.catplot(x='split',y='budget',data=train_test, kind='box' )\ng.set_axis_labels(\"Split\", \"budget\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from bokeh.plotting import figure, output_file, show, output_notebook\nfrom bokeh.models import ColumnDataSource\noutput_notebook()\n\nx = train_orig.popularity\ny = train_orig.revenue\n\nsource = ColumnDataSource(data=dict(\n    popularity=train_orig.popularity,\n    revenue=train_orig.revenue,\n    original_language=train_orig.original_language,\n))\n\n\noutput_file(\"popularity_revenue.html\", title=\"Popularity, Revenue\", mode=\"cdn\")\nTOOLTIPS = [\n    (\"Popularity\", \"@popularity\"),\n    (\"Revenue\", \"@revenue\"),\n    (\"Original Language\", \"@original_language\"),\n    \n]\n\np = figure(tooltips=TOOLTIPS,y_axis_type=\"log\")\n\np.circle('popularity', 'revenue',fill_alpha=0.6, line_color=None, source = source)\np.xaxis.axis_label = \"popularity\"\np.yaxis.axis_label = \"revenue\"\nshow(p)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from bokeh.plotting import figure, output_file, show, output_notebook\nfrom bokeh.models import ColumnDataSource\noutput_notebook()\n\nx = train_orig.budget\ny = train_orig.revenue\n\nsource = ColumnDataSource(data=dict(\n    budget=train_orig.budget,\n    revenue=train_orig.revenue,\n    original_language=train_orig.original_language,\n))\n\n\n# output to static HTML file (with CDN resources)\noutput_file(\"budget_revenue.html\", title=\"Budget, Revenue\", mode=\"cdn\")\nTOOLTIPS = [\n    (\"Budget\", \"@budget\"),\n    (\"Revenue\", \"@revenue\"),\n    (\"Original Language\", \"@original_language\"),\n    \n]\n\np = figure(tooltips=TOOLTIPS,y_axis_type=\"log\")\n\np.circle('budget', 'revenue',fill_alpha=0.6, line_color=None, source = source)\np.xaxis.axis_label = \"budget\"\np.yaxis.axis_label = \"revenue\"\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_orig.columns))\nprint(len(test_orig.columns))\nolang = train_orig.original_language.value_counts()[train_orig.original_language.value_counts()>5].index.tolist()\nprint(olang)\nprint(len(olang))\ntrain_orig_sample = train_orig[train_orig.original_language.isin(olang)].copy()\nprint(train_orig_sample.original_language.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_orig_sample.loc[:,'revenue'] = np.log(train_orig_sample['revenue'].fillna(0)+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/jlove5/avocados-usa-prices\n\ng = sns.catplot(x='original_language',y='revenue',data=train_orig_sample, kind='box', aspect=2 )\ng.set_axis_labels(\"Original language\", \"Log of revenue\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"train_orig.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3292cf5d1826682749e5ceb0519659bbaf4a3d36"},"cell_type":"code","source":"train_olang = pd.get_dummies(train_orig.original_language)[olang]\ntrain_orig = pd.concat([train_orig,train_olang], axis=1)\n#train_orig.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_id(cell):\n    return yaml.load(cell)[0]['id']\n#train['belongs_to_collection'].dropna().apply(extract_id).value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a890de393fb7a108940d7b25b71fa8e35bf1a789"},"cell_type":"code","source":"train_orig.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"idFeature_Engineering\">Feature Engineering</div>"},{"metadata":{"trusted":true,"_uuid":"bc8f7335b9ad7e5e626f796d7ddef35a15faac99"},"cell_type":"code","source":"test_olang = pd.get_dummies(test_orig.original_language)[olang]\ntest_orig = pd.concat([test_orig,test_olang], axis=1)\n#test_orig.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://stackoverflow.com/questions/51643427/how-to-make-tfidfvectorizer-only-learn-alphabetical-characters-as-part-of-the-vo\ntrain_orig['cast_crew'] = train_orig.cast + ' ' + train_orig.crew \ntest_orig['cast_crew'] = test_orig.cast + ' ' + test_orig.crew\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#https://stackoverflow.com/questions/51643427/how-to-make-tfidfvectorizer-only-learn-alphabetical-characters-as-part-of-the-vo\n#vec = TfidfVectorizer(stop_words='english',analyzer='word',max_features=50,max_df=0.5,token_pattern=r'(?u)\\b[A-Za-z]{3,}\\b')\n#vec = TfidfVectorizer(stop_words='english',analyzer='word',max_features=50,max_df=0.5,token_pattern=r\"(?u)\\b[A-Za-z]{3,}\\b|\\'name': \\'\\.(.*?)\\', \\'\")\nvec = TfidfVectorizer(analyzer='word',max_features=450,token_pattern=r\"'name': '(.*?)'\")\nvec.fit(train_orig.cast_crew.fillna(''))\nvocab = vec.get_feature_names()\nvec = TfidfVectorizer(analyzer='word',vocabulary=vocab)\ntrain_crew_w = vec.fit_transform(train_orig.cast_crew.fillna(''))\ntest_crew_w = vec.transform(test_orig.cast_crew.fillna(''))\ntrain_crew_w_cols = vec.get_feature_names()\ntrain_crew_w_cols = ['crew_'+a for a in train_crew_w_cols]\nprint(train_crew_w.shape)\nprint(test_crew_w.shape)\nprint(train_crew_w_cols)\ntrain_crew_w = pd.DataFrame(train_crew_w.toarray(),columns=train_crew_w_cols)\ntest_crew_w = pd.DataFrame(test_crew_w.toarray(),columns=train_crew_w_cols)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#https://stackoverflow.com/questions/51643427/how-to-make-tfidfvectorizer-only-learn-alphabetical-characters-as-part-of-the-vo\n#vec = TfidfVectorizer(stop_words='english',analyzer='word',max_features=50,max_df=0.5,token_pattern=r'(?u)\\b[A-Za-z]{3,}\\b')\n#vec = TfidfVectorizer(stop_words='english',analyzer='word',max_features=50,max_df=0.5,token_pattern=r\"(?u)\\b[A-Za-z]{3,}\\b|\\'name': \\'\\.(.*?)\\', \\'\")\nvec = TfidfVectorizer(analyzer='word',max_features=100,token_pattern=r\"'name': '(.*?)'\")\nvec.fit(train_orig.production_companies.fillna(''))\nvocab = vec.get_feature_names()\nvec = TfidfVectorizer(analyzer='word',vocabulary=vocab)\ntrain_production_companies_w = vec.fit_transform(train_orig.production_companies.fillna(''))\ntest_production_companies_w = vec.transform(test_orig.production_companies.fillna(''))\ntrain_production_companies_w_cols = vec.get_feature_names()\ntrain_production_companies_w_cols = ['prod_comp_'+a for a in train_production_companies_w_cols]\nprint(train_production_companies_w.shape)\nprint(test_production_companies_w.shape)\nprint(train_production_companies_w_cols)\ntrain_production_companies_w = pd.DataFrame(train_production_companies_w.toarray(),columns=train_production_companies_w_cols)\ntest_production_companies_w = pd.DataFrame(test_production_companies_w.toarray(),columns=train_production_companies_w_cols)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://stackoverflow.com/questions/51643427/how-to-make-tfidfvectorizer-only-learn-alphabetical-characters-as-part-of-the-vo\n\nvec = TfidfVectorizer(analyzer='word',max_features=20,token_pattern=r\"'name': '(.*?)'\")\nvec.fit(train_orig.production_countries.fillna(''))\nvocab = vec.get_feature_names()\nvec = TfidfVectorizer(analyzer='word',vocabulary=vocab)\ntrain_production_countries_w = vec.fit_transform(train_orig.production_countries.fillna(''))\ntest_production_countries_w = vec.transform(test_orig.production_countries.fillna(''))\ntrain_production_countries_w_cols = vec.get_feature_names()\ntrain_production_countries_w_cols = ['prod_country_'+a for a in train_production_countries_w_cols]\nprint(train_production_countries_w.shape)\nprint(test_production_countries_w.shape)\nprint(train_production_countries_w_cols)\ntrain_production_countries_w = pd.DataFrame(train_production_countries_w.toarray(),columns=train_production_countries_w_cols)\ntest_production_countries_w = pd.DataFrame(test_production_countries_w.toarray(),columns=train_production_countries_w_cols)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#https://stackoverflow.com/questions/51643427/how-to-make-tfidfvectorizer-only-learn-alphabetical-characters-as-part-of-the-vo\n#vec = TfidfVectorizer(stop_words='english',analyzer='word',max_features=50,max_df=0.5,token_pattern=r'(?u)\\b[A-Za-z]{3,}\\b')\n#vec = TfidfVectorizer(stop_words='english',analyzer='word',max_features=50,max_df=0.5,token_pattern=r\"(?u)\\b[A-Za-z]{3,}\\b|\\'name': \\'\\.(.*?)\\', \\'\")\nvec = TfidfVectorizer(analyzer='word',max_features=50,token_pattern=r\"'name': '(.*?)'\")\n\ntrain_belongs_to_collection_w = vec.fit_transform(train_orig.belongs_to_collection.fillna(''))\ntest_belongs_to_collection_w = vec.transform(test_orig.belongs_to_collection.fillna(''))\ntrain_belongs_to_collection_w_cols = vec.get_feature_names()\ntrain_belongs_to_collection_w_cols = ['collection_'+a for a in train_belongs_to_collection_w_cols]\nprint(train_belongs_to_collection_w.shape)\nprint(test_belongs_to_collection_w.shape)\nprint(train_belongs_to_collection_w_cols)\ntrain_belongs_to_collection_w = pd.DataFrame(train_belongs_to_collection_w.toarray(),columns=train_belongs_to_collection_w_cols)\ntest_belongs_to_collection_w = pd.DataFrame(test_belongs_to_collection_w.toarray(),columns=train_belongs_to_collection_w_cols)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://stackoverflow.com/questions/51643427/how-to-make-tfidfvectorizer-only-learn-alphabetical-characters-as-part-of-the-vo\nvec = TfidfVectorizer(stop_words='english',analyzer='word',max_features=50,token_pattern=r'(?u)\\b[A-Za-z]{3,}\\b')\ntrain_genres_w = vec.fit_transform(train_orig.genres.fillna(''))\ntest_genres_w = vec.transform(test_orig.genres.fillna(''))\ntrain_genres_w_cols = vec.get_feature_names()\ntrain_genres_w_cols = ['genre_'+a for a in train_genres_w_cols]\nprint(train_genres_w.shape)\nprint(test_genres_w.shape)\nprint(train_genres_w_cols)\ntrain_genres_w = pd.DataFrame(train_genres_w.toarray(),columns=train_genres_w_cols)\ntest_genres_w = pd.DataFrame(test_genres_w.toarray(),columns=train_genres_w_cols)\nprint(train_genres_w.shape)\nprint(test_genres_w.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f193b6bff4621f7fdb4568899133b77058c39d77"},"cell_type":"code","source":"#https://stackoverflow.com/questions/51643427/how-to-make-tfidfvectorizer-only-learn-alphabetical-characters-as-part-of-the-vo\ntrain_orig['Keywords_tagline_overview'] = train_orig.title + ' ' + train_orig.Keywords +' ' + train_orig.tagline + ' ' + train_orig.overview\ntest_orig['Keywords_tagline_overview'] = test_orig.title + ' ' + test_orig.Keywords + ' ' + test_orig.tagline + ' ' + test_orig.overview\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvec = TfidfVectorizer(stop_words='english',analyzer='word',max_features=60,token_pattern=r'(?u)\\b[A-Za-z]{3,}\\b')\ntrain_tagline_keyword_w = vec.fit_transform(train_orig.Keywords_tagline_overview.fillna(''))\ntrain_tagline_keyword_w_cols = vec.get_feature_names()\nprint(train_tagline_keyword_w.shape)\ntest_tagline_w = vec.transform(test_orig.Keywords_tagline_overview.fillna(''))\nprint(test_tagline_w.shape)\ntrain_tagline_keyword_w_cols = ['kw_tg_ow_' + a for a in train_tagline_keyword_w_cols]\ntrain_tagline_keyword_w = pd.DataFrame(train_tagline_keyword_w.toarray(),columns=train_tagline_keyword_w_cols)\ntest_tagline_keyword_w = pd.DataFrame(test_tagline_w.toarray(),columns=train_tagline_keyword_w_cols)\ntrain = pd.concat([train_orig,train_tagline_keyword_w,train_genres_w,train_belongs_to_collection_w,\n                   train_production_companies_w,train_crew_w,train_production_countries_w], axis=1)\ntest = pd.concat([test_orig,test_tagline_keyword_w,test_genres_w,test_belongs_to_collection_w,\n                  test_production_companies_w,test_crew_w,test_production_countries_w], axis=1)\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['bool_belongs_to_collection'] = (train['belongs_to_collection'].notnull()).astype(int)\ntest['bool_belongs_to_collection'] = (test['belongs_to_collection'].notnull()).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba29ab93504a59c70bb4bca2c0379b0b3d431b5c"},"cell_type":"code","source":"len(train_tagline_keyword_w_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41e08be17e4b732106880afcee9cdc9d50c63ff8"},"cell_type":"code","source":"train['release_date'] = pd.to_datetime(train['release_date'] )\ntest['release_date'] = pd.to_datetime(test['release_date'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e739e707acd7df2d39a4294dc98eb5549b33c8ac"},"cell_type":"code","source":"train['release_month'] = train['release_date'].dt.month\n#print(train['release_month'].value_counts())\ntest['release_month'] = test['release_date'].dt.month\n#print(test['release_month'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afbfdbba2fc2720494ef36daf47c3b9beaa4c642"},"cell_type":"code","source":"train['release_year'] = train['release_date'].dt.year\n#print(train['release_year'].value_counts())\ntest['release_year'] = test['release_date'].dt.year\n#print(test['release_year'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['release_dayofyear'] = train['release_date'].dt.dayofyear\ntest['release_dayofyear'] = test['release_date'].dt.dayofyear\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a026cfad0b2674acc05277bbafd259848e79e326"},"cell_type":"code","source":"train['release_day_of_week'] = train['release_date'].dt.dayofweek\n#print(train['release_day_of_week'].value_counts())\ntest['release_day_of_week'] = test['release_date'].dt.dayofweek\n#print(test['release_day_of_week'].value_counts())\ntrain['release_week'] = train['release_date'].dt.week\ntest['release_week'] = test['release_date'].dt.week","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3f84816305a1c112170048af6b1ff3e21ecceb7"},"cell_type":"code","source":"test['release_month'].mode()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3383418f83a9c9a6ff01d0e7847863260879cdc4"},"cell_type":"code","source":"test['release_year'].mode()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['release_week'].mode()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['release_year'].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"721d77143090dd99d4fc3e53adbc8674eafe4f7c"},"cell_type":"code","source":"test['release_month'] = test['release_month'].fillna(9.0)\ntest['release_year'] = test['release_year'].fillna(2014.0)\ntest['release_week'] = test['release_week'].fillna(36.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3642f7ccec288081bb367d4bc7ce531a9edf854a"},"cell_type":"code","source":"\ntrain['bool_homepage'] = (train['homepage'].notnull()).astype(int)\ntest['bool_homepage'] = (test['homepage'].notnull()).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"901b92cd7d9af4ee3151a5a5aef026945278dd7a"},"cell_type":"code","source":"train['production_companies_len'] = train['production_companies'].str.len()\ntest['production_companies_len'] = test['production_companies'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b8af800dff672b1122e57c1fe6a1b147c0207f1"},"cell_type":"code","source":"train['production_countries_len'] = train['production_countries'].str.len()\ntest['production_countries_len'] = test['production_countries'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91aeeaee034ec608ce834e08a380165d20c85ee3"},"cell_type":"code","source":"train['Keywords_len'] = train['Keywords'].str.len()\ntest['Keywords_len'] = test['Keywords'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"859b4a99e3c379c9d3f083f616c30a522a95e830"},"cell_type":"code","source":"train['title_len'] = train['title'].str.len()\ntest['title_len'] = test['title'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a71b25b6f398dfe26b0ff4df229a92eed54d9250"},"cell_type":"code","source":"train['genres_len'] = train['genres'].str.len() \ntest['genres_len'] = test['genres'].str.len() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d5a10d0e9eeb97ac799aa8fe036abee4d598147"},"cell_type":"code","source":"train['cast_crew_len'] = train['cast'].str.len() + train['crew'].str.len()\ntest['cast_crew_len'] = test['cast'].str.len() + test['crew'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb22901ebecded3a8e80f7b7cf5dfca395896e0d"},"cell_type":"code","source":"train['cast_crew_len'].fillna(train['cast_crew_len'].median(),inplace=True)\ntrain['runtime'].fillna(train['runtime'].median(),inplace=True)\ntrain['genres_len'].fillna(train['genres_len'].median(),inplace=True)\ntrain['production_companies_len'].fillna(train['production_companies_len'].median(),inplace=True)\ntrain['production_countries_len'].fillna(train['production_countries_len'].median(),inplace=True)\ntrain['Keywords_len'].fillna(train['Keywords_len'].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b46d33c393a8a2bdd89bbbab6c2ba9ea674fa0a"},"cell_type":"code","source":"(train['release_year']>2019).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1cc9675c3ba3468fbf73a51e50c88831d8f4cde"},"cell_type":"code","source":"train.loc[(train['release_year']>2019),'release_year']=train['release_year'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['month_into_year'] = train['release_month']*train['release_year']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05662fab8177f21ac44e09ca4ac0dc07f63d5984"},"cell_type":"code","source":"(test['release_year']>2019).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca4ea90193729ccf3e75815a25c3db10e72525c5"},"cell_type":"code","source":"test.loc[(test['release_year']>2019),'release_year']=test['release_year'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a2d22309ae8c2c721544f5e94f69c49bebd730e"},"cell_type":"code","source":"vcast_crew_len = test['cast_crew_len'].median()\ntest['cast_crew_len'].fillna(vcast_crew_len, inplace=True)\ntest['runtime'].fillna(test['runtime'].median(),inplace=True)\ntest['release_month'].fillna(test['release_month'].median(),inplace=True)\ntest['title_len'].fillna(test['title_len'].median(),inplace=True)\ntest['release_year'].fillna(test['release_year'].median(),inplace=True)\ntest['release_day_of_week'].fillna(test['release_day_of_week'].median(),inplace=True)\ntest['release_dayofyear'].fillna(test['release_dayofyear'].median(),inplace=True)\ntest['genres_len'].fillna(test['genres_len'].median(),inplace=True)\ntest['production_companies_len'].fillna(test['production_companies_len'].median(),inplace=True)\ntest['production_countries_len'].fillna(test['production_countries_len'].median(),inplace=True)\ntest['Keywords_len'].fillna(test['Keywords_len'].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['month_into_year'] = test['release_month']*test['release_year']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e8f5b0f7e34366470531faf9e74e02a603bfd5f"},"cell_type":"code","source":"features = ['bool_homepage', 'release_dayofyear','production_companies_len', 'production_countries_len', 'Keywords_len' , 'cast_crew_len','budget','popularity','runtime','release_month','release_day_of_week','release_week','genres_len','bool_belongs_to_collection', 'title_len','release_year']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['log_revenue'] = np.log(train['revenue'].fillna(0)+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from bokeh.plotting import figure, output_file, show, output_notebook\nfrom bokeh.models import ColumnDataSource\noutput_notebook()\n\n\nsource = ColumnDataSource(data=dict(\n    cast_crew_len=train.cast_crew_len,\n    revenue=train.revenue,\n    original_language=train.original_language,\n))\n\n\noutput_file(\"cast_crew_len_revenue.html\", title=\"cast_crew_len, Revenue\", mode=\"cdn\")\nTOOLTIPS = [\n    (\"cast_crew_len\", \"@cast_crew_len\"),\n    (\"Revenue\", \"@revenue\"),\n    (\"Original Language\", \"@original_language\"),\n    \n]\n\np = figure(tooltips=TOOLTIPS,y_axis_type=\"log\")\n\np.circle('cast_crew_len', 'revenue',fill_alpha=0.6, line_color=None, source = source)\np.xaxis.axis_label = \"cast_crew_len\"\np.yaxis.axis_label = \"revenue\"\nshow(p)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/jlove5/avocados-usa-prices\n\ng = sns.catplot(x='bool_homepage',y='log_revenue',data=train, kind='box', aspect=1 )\ng.set_axis_labels(\"Is there a homepage\", \"Log of Revenue\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/jlove5/avocados-usa-prices\n\ng = sns.catplot(x='release_month',y='log_revenue',data=train, kind='box', aspect=2 )\ng.set_axis_labels(\"Release month\", \"Log of Revenue\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/jlove5/avocados-usa-prices\n\ng = sns.catplot(x='release_day_of_week',y='log_revenue',data=train, kind='box', aspect=2 )\ng.set_axis_labels(\"Release day of week\", \"Log of Revenue\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/jlove5/avocados-usa-prices\n\ng = sns.catplot(x='release_week',y='log_revenue',data=train, kind='box', aspect=3 )\ng.set_axis_labels(\"Release week\", \"Log of Revenue\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/jlove5/avocados-usa-prices\n\ntrain['is_genre_drama'] = (train['genre_drama']>0).astype(int)\ng = sns.catplot(x='is_genre_drama',y='log_revenue', data=train,kind='box' )\ng.set_axis_labels(\"is_genre_drama\", \"Log of Revenue\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/jlove5/avocados-usa-prices\n\ntrain['is_kw_tg_ow_death'] = (train['kw_tg_ow_death']>0).astype(int)\ng = sns.catplot(x='is_kw_tg_ow_death',y='log_revenue', data=train,kind='box' )\ng.set_axis_labels(\"is_kw_tg_ow_death\", \"Log of Revenue\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/jlove5/avocados-usa-prices\n\ntrain['is_genre_thriller'] = (train['genre_thriller']>0).astype(int)\ng = sns.catplot(x='is_genre_thriller',y='log_revenue', data=train,kind='box' )\ng.set_axis_labels(\"is_genre_thriller\", \"Log of Revenue\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/jlove5/avocados-usa-prices\n\ng = sns.catplot(x='release_year',y='log_revenue',data=train, kind='box', aspect=3 )\ng.set_axis_labels(\"Release year\", \"Log of Revenue\")\ng.set_xticklabels(rotation=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83f4c614243eb744860935030e84da5b887e7750"},"cell_type":"code","source":"len(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"272a4a3338f8bab11d0ec85f8b38729db6c4bfb8"},"cell_type":"code","source":"features = features+olang+train_tagline_keyword_w_cols+train_genres_w_cols+train_belongs_to_collection_w_cols \\\n+train_production_companies_w_cols + train_crew_w_cols + train_production_countries_w_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05d1a6f624f23fe443ed52f65a5dfe82841b7974"},"cell_type":"code","source":"len(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.release_year.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d68d0f3c0426fca15368f18bdc295a397b2ba528"},"cell_type":"code","source":"train[features].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bd6f22ca7695560a8a3d48d098e54e3089c35c0"},"cell_type":"code","source":"test[features].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ac080597f45335d0d0e7b0478b90c9fca723b9c"},"cell_type":"code","source":"target_column = 'revenue'\ncolumns_for_prediction=features\nX = train[columns_for_prediction].copy()\nimport sklearn.preprocessing as preprocessing\ny_scale = preprocessing.MinMaxScaler()\n#y = np.log(train[target_column])\n#https://stackoverflow.com/questions/26584971/how-to-not-standarize-target-data-in-scikit-learn-regression\ny = y_scale.fit_transform(train[target_column].values.reshape(-1, 1) )\n\nX_unseen = test[columns_for_prediction].copy()\n# print(X.budget.max())\n# budget_q = X['budget'].quantile(0.8)\n# print(budget_q)\n# print(X.popularity.max())\n# popularity_q = X['popularity'].quantile(0.8)\n# print(popularity_q)\n# print(X.runtime.max())\n# runtime_q = X['runtime'].quantile(0.8)\n# print(runtime_q)\n# print(y.max())\n# y_q = y.quantile(0.8)\n# print(y_q)\n\n# #Outliers\n# X.loc[X.budget > budget_q, 'budget'] = budget_q\n# X.loc[X.popularity > popularity_q, 'popularity'] = popularity_q\n# X.loc[X.runtime > runtime_q, 'runtime'] = runtime_q\n# y[y>y_q] = y_q\n\n\n# X_unseen.loc[X_unseen.budget > budget_q, 'budget'] = budget_q\n# X_unseen.loc[X_unseen.popularity > popularity_q, 'popularity'] = popularity_q\n# X_unseen.loc[X_unseen.runtime > runtime_q, 'runtime'] = runtime_q\n\nscale = preprocessing.Normalizer()\nX = pd.DataFrame(scale.fit_transform(X),columns=columns_for_prediction)\nX_unseen = pd.DataFrame(scale.transform(test[columns_for_prediction]),columns=columns_for_prediction)\n\nbudget_min = X['budget'].quantile(0.28)\nX['budget'] = X['budget'].replace(0,budget_min)\n\nX_unseen['budget'] = X_unseen['budget'].replace(0,budget_min)\n\n# X['budget'] = np.log(X['budget'])\n# X_unseen['budget'] = np.log(X_unseen['budget'])\n# X['runtime'] = np.log(X['runtime']+1)\n# X_unseen['runtime'] = np.log(X_unseen['runtime']+1)\n# X['popularity'] = np.log(X['popularity']+1)\n# X_unseen['popularity'] = np.log(X_unseen['popularity']+1)\n\n\n\n# from sklearn.preprocessing import StandardScaler\n# scale = StandardScaler()\n# X['budget'] = scale.fit(X[['budget']])\n# X_unseen = test[columns_for_prediction].copy()\n# X_unseen['budget'] = scale.fit(X_unseen[['budget']])\n\n\n# X[['budget']] = pd.DataFrame(scale.fit_transform(X[['budget']].values))\n# X_unseen[['budget']] = pd.DataFrame(scale.fit_transform(X_unseen[['budget']].values))\n#X[['budget']].head()\n\n\n# print(X.release_year.min())\n# print(X_unseen.release_year.min())\n# release_year_min = X.release_year.min()\n# X['release_year'] = X['release_year'] - release_year_min\n# X_unseen['release_year'] = X_unseen['release_year'] - release_year_min\n\n#X['month_into_year'] = np.log(X['month_into_year']+1)\n#X_unseen['month_into_year'] = np.log(X_unseen['month_into_year']+1)\n#X.budget = np.log(X.budget)\n#X_unseen.budget","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"378fded0290c5d84cf744dfd2dcd6e3d95787a77","_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"columns_for_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d03a870023701a3ecf49e5b3f44fbbb104aa5b3b","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#features = ['cast_crew_len','budget','popularity','runtime','release_month','release_day_of_week','genres_len','bool_belongs_to_collection', 'title_len','release_year']\n#2.67146\n# from sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=2019)\n# from sklearn.linear_model import BayesianRidge\n# from sklearn.metrics import mean_squared_error\n\n# reg = BayesianRidge(normalize=False,verbose=True).fit(X_train, y_train)\n# score = reg.score(X_test, y_test)\n# print('Test score %d'%score)\n# preds = reg.predict(X_test)\n# err = mean_squared_error(y_test, preds) ** 0.5\n# print('Test rmse %d'%score)\n# reg = BayesianRidge(normalize=False).fit(X, y)\n# score = reg.score(X, y)\n# print('Train score %d'%score)\n# preds = reg.predict(test[columns_for_prediction])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d4dd6bbe995848fd984243cace06e27d339ee91","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=2019)\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nparams = {'n_estimators': 700, 'max_depth': 4, 'min_samples_split': 2,\n          'learning_rate': 0.01, 'loss': 'ls'}\n\nreg = GradientBoostingRegressor(**params).fit(X_train, y_train)\nscore = reg.score(X_test, y_test)\nprint('Test score %d'%score)\npreds = reg.predict(X_test)\nerr = mean_squared_error(y_test, preds)\nprint('Test mse %d'%err)\nreg = GradientBoostingRegressor(n_estimators=700).fit(X, y)\nscore = reg.score(X, y)\nprint('Train score %d'%score)\npreds_first = reg.predict(X_unseen)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bayes search to find parameters"},{"metadata":{"trusted":true,"_uuid":"8f874744708f83f789a34d7b61d73b04bbaadc8d","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#https://www.kaggle.com/srserves85/boosting-stacking-and-bayes-searching\n# uses baysian optimization to find model parameters\n# from skopt import BayesSearchCV\n# from skopt.space import Real, Categorical, Integer\n\n# model = GradientBoostingRegressor(\n#     loss='ls',\n#     learning_rate = 0.0035,\n#     max_depth=23,\n#     n_estimators=30275,\n#     #max_features=9,\n#     min_samples_leaf=22,\n#     min_samples_split=15,\n#     min_weight_fraction_leaf=0.0102470171519909,\n#     random_state = 0\n# )\n\n# search_params = {\n#     \"n_estimators\": Integer(1650, 4000),\n#     'max_depth': Integer(3, 5),\n#     'min_samples_split': Integer(15, 30),\n#     'min_samples_leaf': Integer(15, 50),\n#     'learning_rate': Real(0.009,0.2),\n#     'subsample': Real(0.9,1),\n#     'max_leaf_nodes': Integer(70, 110),\n#     'random_state': Integer(2019,2020),\n#     #'min_weight_fraction_leaf': Real(0., .5),\n#     #'max_features': Integer(8, 88)\n# }\n\n# opt = BayesSearchCV(model, search_params, n_iter=50, n_jobs=8, cv=5,random_state=2019)\n# opt.fit(X, y)\n# opt_best_params = opt.best_params_\n# opt_best_params\n\n# {'learning_rate': 0.009445613868676355,\n#  'max_depth': 5,\n#  'max_leaf_nodes': 72,\n#  'min_samples_leaf': 20,\n#  'min_samples_split': 5,\n#  'n_estimators': 1621,\n#  'random_state': 2020,\n#  'subsample': 0.9011747167211098}\n\n# params =  {'max_depth': 7,\n#  'min_samples_leaf': 33,\n#  'min_samples_split': 2,\n#  'min_weight_fraction_leaf': 0.0,\n#  'n_estimators': 2921}\n\n# {'learning_rate': 0.005205030202575363,\n#  'max_depth': 4,\n#  'min_samples_leaf': 2,\n#  'min_samples_split': 2,\n#  'n_estimators': 1617,\n#  'subsample': 0.9}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"idPrediction\">Prediction</div>"},{"metadata":{},"cell_type":"markdown","source":"My first Tf regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.tensorflow.org/tutorials/keras/basic_regression\n#https://www.pyimagesearch.com/2019/01/21/regression-with-keras/\n#https://www.kaggle.com/hendraherviawan/regression-with-kerasregressor\n#https://www.kaggle.com/yusufsatilmis/house-prices-prediction-with-keras\n#https://www.kaggle.com/aharless/keras-nn-with-q4-validation\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef norm(x):\n  return (x - train_stats.loc['mean']) / train_stats.loc['std']\ntrain_dataset = X\n#train_labels = y.values\ntrain_labels = y\ntest_dataset = X_unseen\ntrain_stats = train_dataset.describe()\nnormed_train_data = train_dataset\nnormed_test_data = test_dataset\n\n\ndef build_model():\n  model = keras.Sequential([\n    layers.Dense(20, activation='relu', \n                 kernel_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None),\n                 input_shape=[len(train_dataset.keys())]),\n    layers.Dropout(.5),   \n    \n#     layers.Dense(250, activation='relu', kernel_initializer='normal'),\n#     layers.Dropout(.5),  \n#     layers.Dense(100, activation='relu', kernel_initializer='normal'),\n#     layers.Dropout(.4), \n#     layers.Dense(50, activation='relu', kernel_initializer='normal'),\n#     layers.Dropout(.4),   \n#     layers.Dense(20, activation='relu', kernel_initializer='normal'),\n#     layers.Dropout(.3),   \n    layers.Dense(1, activation='linear', kernel_initializer='normal'),\n    layers.Dropout(.2), \n  ])\n\n  optimizer = tf.keras.optimizers.RMSprop(0.01)\n  #optimizer = tf.keras.optimizers.Adam(0.001)\n  model.compile(loss='mean_squared_error',\n                optimizer=optimizer,\n                metrics=['mean_absolute_error', 'mean_squared_error'])\n  return model\n\nmodel = build_model()\nmodel.summary()\nEPOCHS = 100\n\nhistory = model.fit(\n  normed_train_data, train_labels,batch_size = 100,\n  epochs=EPOCHS, validation_split = 0.01, verbose=1)\n\ndef plot_history(history):\n  hist = pd.DataFrame(history.history)\n  hist['epoch'] = history.epoch\n  \n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Abs Error ')\n  plt.plot(hist['epoch'], hist['mean_absolute_error'],\n           label='Train Error')\n  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n           label = 'Val Error')\n  \n  plt.legend()\n  \n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Square Error ')\n  plt.plot(hist['epoch'], hist['mean_squared_error'],\n           label='Train Error')\n  plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n           label = 'Val Error')\n  \n  plt.legend()\n  plt.show()\n\n\nplot_history(history)\n\nmodel = build_model()\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\nhistory = model.fit(normed_train_data, train_labels, epochs=EPOCHS,\n                    validation_split = 0.2, verbose=0, callbacks=[early_stop])\nplot_history(history)\n\npred_using_train = model.predict(normed_train_data).flatten()\npreds_estop = model.predict(normed_test_data).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" ax = sns.scatterplot(x=train_labels.flatten(), y=pred_using_train.flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79063cae1dd9309ec1ef18bf1a39c8a536b3f4ec","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=2019)\n# from sklearn.ensemble import GradientBoostingRegressor\n# from sklearn.metrics import mean_squared_error\n# # params = {'learning_rate': 0.009445613868676355,\n# #  'max_depth': 5,\n# #  'max_leaf_nodes': 72,\n# #  'min_samples_leaf': 20,\n# #  'min_samples_split': 5,\n# #  'n_estimators': 1621,\n# #  'random_state': 2020,\n# #  'subsample': 0.9011747167211098}\n# # params = {'n_estimators': 700, 'max_depth': 4, 'min_samples_split': 2,\n# #           'learning_rate': 0.01, 'loss': 'ls'}\n\n\n# params = opt_best_params\n\n# # params = {'learning_rate': 0.009,\n# #  'max_depth': 3,\n# #  'max_leaf_nodes': 73,\n# #  'min_samples_leaf': 15,\n# #  'min_samples_split': 13,\n# #  'n_estimators': 1600,\n# #  'random_state': 2019,\n# #  'subsample': 0.9732580382105793}\n\n# # params = {'learning_rate': 0.009,\n# #  'max_depth': 3,\n# #  'max_leaf_nodes': 110,\n# #  'min_samples_leaf': 15,\n# #  'min_samples_split': 17,\n# #  'n_estimators': 1650,\n# #  'random_state': 2019,\n# #  'subsample': 0.9}\n\n\n# # params =  {'max_depth': 7,\n# #  'min_samples_leaf': 33,\n# #  'min_samples_split': 2,\n# #  'min_weight_fraction_leaf': 0.0,\n# #  'n_estimators': 2921}\n# # params =  {'learning_rate': 0.005205030202575363,\n# #  'max_depth': 4,\n# #  'min_samples_leaf': 2,\n# #  'min_samples_split': 2,\n# #  'n_estimators': 1617,\n# #  'subsample': 0.9}\n\n# # params = {'learning_rate': 0.01,\n# #  'max_depth': 7,\n# #  'max_leaf_nodes': 25,\n# #  'min_samples_leaf': 50,\n# #  'min_samples_split': 15,\n# #  'n_estimators': 700,\n# #  'subsample': 0.9,\n# #  'random_state': 2019}\n\n# reg = GradientBoostingRegressor(**params).fit(X_train, y_train)\n# score = reg.score(X_test, y_test)\n# print('Test score %d'%score)\n# preds = reg.predict(X_test)\n# err = mean_squared_error(y_test, preds)\n# print('Test mse %d'%err)\n# #reg = GradientBoostingRegressor(n_estimators=700).fit(X, y)\n# score = reg.score(X, y)\n# print('Train score %d'%score)\n# preds = reg.predict(X_unseen)\n# reg.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27f8a22562a69a9b656f03ea7282c4e4c13bb8b0","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# feature_importance = reg.feature_importances_\n# # make importances relative to max importance\n# feature_importance = 100.0 * (feature_importance / feature_importance.max())\n# sorted_idx = np.argsort(feature_importance)\n# [features[i] for i in sorted_idx][::-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8cc836707c791927f3825f871b1f54079de1918"},"cell_type":"code","source":"# from sklearn.linear_model import LassoLarsCV\n# reg = LassoLarsCV(cv=10).fit(X, y)\n# print(reg.score(X, y)) \n# print(reg.alpha_)\n# preds = reg.predict(test[columns_for_prediction])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5458c711ef6044828395965917cd8d7b27dd728"},"cell_type":"code","source":"#median_revenue = y.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56f2e0d23b14e7cb15e28e62728b577df86f889b"},"cell_type":"code","source":"preds[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#https://www.kaggle.com/tejasrinivas/xgb-baseline-comments-classification\n# import xgboost as xgb\n\n# #data_with_imputed_values_wo_Fare = data_with_imputed_values.drop('Fare', axis=1)\n# #X_t, X_v, y_t, y_v = train_test_split(data_with_imputed_values,y_train, stratify=y_train, test_size=0.2, random_state=2019)\n# #https://stackoverflow.com/questions/48645846/pythons-xgoost-valueerrorfeature-names-may-not-contain-or\n# import re\n# regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n# X_copy = X.copy()\n# X_copy.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_copy.columns.values]\n# X_unseen_copy = X_unseen.copy()\n# X_unseen_copy.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_unseen_copy.columns.values]\n\n# X_t, X_v, y_t, y_v = train_test_split(X_copy, y, test_size=.2, random_state=2019)\n# #X_t, X_v, y_t, y_v = train_test_split(data_with_imputed_values_wo_Fare,y_train, test_size=0.2, random_state=2019)\n\n\n# def runXGB(X_t, X_v, y_t, y_v, feature_names=None, seed_val=2017, num_rounds=200):\n#     param = {}\n#     param['objective'] = 'reg:linear'\n#     param['eta'] = 0.2\n#     param['max_depth'] = 7\n#     param['silent'] = 1\n#     param['eval_metric'] = 'logloss'\n#     param['min_child_weight'] = 1 #3\n#     param['subsample'] = 0.9\n#    # param['colsample_bytree'] = 0.5\n#     param['seed'] = seed_val\n#    # param['max_delta_step'] = 8\n# #     param['objective'] = 'binary:logistic'\n# #     param['eta'] = 0.1\n# #     param['max_depth'] = 6\n# #     param['silent'] = 1\n# #     param['eval_metric'] = 'auc'\n# #     param['min_child_weight'] = 1\n# #     param['subsample'] = 0.5\n# #     param['colsample_bytree'] = 0.5\n# #     param['seed'] = seed_val\n    \n    \n#     num_rounds = num_rounds\n\n#     plst = list(param.items())\n#     xgtrain = xgb.DMatrix(X_t, label=y_t)\n\n\n#     xgtest = xgb.DMatrix(X_v, label=y_v)\n#     watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n#     model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n#     #model = xgb.train(plst, xgtrain, num_rounds, watchlist)\n#     return model    \n\n# model = runXGB(X_t, X_v, y_t, y_v)\n# preds = model.predict(xgb.DMatrix(X_unseen_copy))\n# preds[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2c1a7ceaaf182c20113f2e218b200ac006c445b"},"cell_type":"code","source":"\n#preds[preds < 0] = median_revenue\n#preds=np.exp(preds)\nmedian_revenue = train[target_column].median()\n#preds = y_scale.inverse_transform(preds)\npreds = preds_estop\npreds = y_scale.inverse_transform(preds.reshape(-1, 1))\npreds[preds < 0] = median_revenue\n#preds=(preds)**2\nsubm['revenue'] = preds\n#median_revenue = y.median()\n# preds_first[preds_first < 0] = median_revenue\n# preds_first=np.exp(preds_first)\n# subm['revenue'] = preds_first\nsubm.to_csv('submission.csv', index=False)\nprint(subm.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <div id=\"idAnalysis_of_Prediction\">Analysis of Prediction</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n#sns.distplot(train['revenue'] )\ntrain['revenue'].hist(log=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(subm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.distplot(subm['revenue'] )\nsubm['revenue'].hist(log=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"popularity\", y=\"revenue\",\n                     hue=\"release_year\", \n                     data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=test.popularity, y=subm.revenue,\n                     hue=test.release_year)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"budget\", y=\"revenue\",\n                     hue=\"release_year\", \n                     data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=test.budget, y=subm.revenue,\n                     hue=test.release_year)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[ ['release_date', 'revenue']].set_index('release_date').resample('A').mean()[:'2019'].plot(style='--')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['revenue'] = subm['revenue']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[ ['release_date', 'revenue']].set_index('release_date').resample('A').mean()[:'2019'].plot(style='--')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
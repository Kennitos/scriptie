{"cells":[{"metadata":{"_uuid":"276fd621928344c9f315ebc8f92f26b296598c20"},"cell_type":"markdown","source":"## [INTRODUCTION](#introduction)\n* ### [Purpose](#purpose) \n* ### [Dataset](#dataset)\n\n## [A FIRST LOOK AT THE DATASET](#firstLook)\n* ### [Importing Libraries and Reading the Dataset](#importAndRead) \n* ### [Basic Information about Dataset](#basicInfo) \n* ### [Correlation between All Columns](#correlation) \n\n## [DATA VISUALIZATION TO UNDERSTAND THE DATASET](#visualization )\n\n## [REGRESSION ALGORITHMS (SUPERVISED MACHINE LEARNING ALGORITHMS)](#regression)\n* ### [Preparing Data for Regression](#prepareForRegression) \n* ### [Linear Regression](#linearRegression) \n* ### [Random Forest Regression](#rfRegression) \n* ### [Decision Tree Regression](#dtRegression) \n* ### [Comparison of Regression Algorithms](#comparisonOfRegression) \n\n## [CLASSIFICATION ALGORITHMS (SUPERVISED MACHINE LEARNING ALGORITHMS)](#classification)\n* ### [Preparing Data for Classification](#prepareForClassification) \n* ### [Logistic Regression](#lr) \n* ### [Support Vector Machine](#svm) \n* ### [Gaussian Naive Bayes](#gnb)\n* ### [Decision Tree Classification](#dtc) \n* ### [Random Forest Classification](#rfc) \n* ### [K Nearest Neighbors Classification](#knnc)  \n* ### [Comparison of Classification Algorithms](#comparisonOfClassification) \n\n## [CLUSTERING ALGORITHMS (UNSUPERVISED MACHINE LEARNING ALGORITHMS)](#clustering)\n* ### [Preparing Data for Clustering](#prepareForClustering) \n* ### [Principal Component Analysis](#pca) \n* ### [K-means Clustering](#kmeans) \n* ### [Hierarchical Clustering](#hierarchical ) \n* ### [Comparison of Clustering Algorithms](#comparisonOfClustering) \n\n## [THE THREE IMPORTANT FEATURES](#feature)\n* ### [Correlation between All Columns](#correlationForFeature) \n* ### [The Three Features for Linear Regression](#ThreeLinearRegression) "},{"metadata":{"_uuid":"0db364f68cf816491ff8eff1dd4f5d87c81d24a0"},"cell_type":"markdown","source":"## <a id='introduction'>INTRODUCTION</a>"},{"metadata":{"_uuid":"91395e4cf5c04a57a76f99d83943efd67bca5cf5"},"cell_type":"markdown","source":"### <a id='purpose'>Purpose</a>\n\nTo apply for a master's degree is a very expensive and intensive work. With this kernel, students will guess their capacities and they will decide whether to apply for a master's degree or not."},{"metadata":{"_uuid":"d0451e4cf0ab7fe290627f6ba2264543ec81ac01"},"cell_type":"markdown","source":"### <a id='dataset'>Dataset</a>\n\nThis dataset is created for prediction of graduate admissions and the dataset link is below: \n* https://www.kaggle.com/mohansacharya/graduate-admissions\n\nFeatures in the dataset:\n* GRE Scores (290 to 340) \n* TOEFL Scores (92 to 120) \n* University Rating (1 to 5) \n* Statement of Purpose (1 to 5) \n* Letter of Recommendation Strength (1 to 5) \n* Undergraduate CGPA (6.8 to 9.92) \n* Research Experience (0 or 1) \n* Chance of Admit (0.34 to 0.97)"},{"metadata":{"_uuid":"1355b42c18cc0c88bd7dc469634570c635d57b8f"},"cell_type":"markdown","source":"## <a id='firstLook'>A FIRST LOOK AT THE DATASET</a>"},{"metadata":{"_uuid":"593e38899bbdc7e49f9d5ff0e7057df1aaccc5ca"},"cell_type":"markdown","source":"### <a id='importAndRead'>Importing Libraries and Reading the Dataset</a>"},{"metadata":{"trusted":true,"_uuid":"dfb262bc120b71ddbadf137e79c026b99e7079df"},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport sys\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc6548ef53b397c82cecf38ba62d62d5ae356848"},"cell_type":"code","source":"df = pd.read_csv(\"../input/Admission_Predict.csv\",sep = \",\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84c505c4838bb79b68d60ec49a0ba329451f0be7"},"cell_type":"markdown","source":"### <a id='basicInfo'>Basic Information about Dataset</a>\n\n"},{"metadata":{"_uuid":"c63b7890588fdaae3f45ff37441694f07500a3a6"},"cell_type":"markdown","source":"Some important information:\n* There are 9 columns: Serial No., GRE Score, TOEFL Score, University Rating, SOP, LOR , CGPA, Research, Chance of Admit\n* There are no null records. It's good.\n* There are 400 samples in total. That's enough."},{"metadata":{"trusted":true,"_uuid":"0ca58a2764180732aa6abb95358fcfcfa6588c23"},"cell_type":"code","source":"print(\"There are\",len(df.columns),\"columns:\")\nfor x in df.columns:\n    sys.stdout.write(str(x)+\", \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f612351f70e8e02eba7f6c1833b7b3a7e198bdb"},"cell_type":"code","source":"df=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c41a5bcb33eb60e192a8ec1aeb441df28a86f09"},"cell_type":"markdown","source":"df.info ():\n* It was used to find the number of samples and the number of features. \n* There is no string or null data.\n* Data types are int64 and float64.\n* Memory usage: 28.2 KB"},{"metadata":{"trusted":true,"_uuid":"5f63ccf8de126fcac98589a47e96ce876d7fb065"},"cell_type":"code","source":"print(df.info())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3a95fcc9f25162edb3cb12d6e3be6a8501b7333"},"cell_type":"markdown","source":"df.head() and df.tail():\n* They are good to test if a feature is useful for future works.\n* Serial No. may be deleted because it stores index values. In the future this column does not benefit."},{"metadata":{"trusted":true,"_uuid":"ead235bdcca2ad390ddb8e5fa589d393748859e2"},"cell_type":"code","source":"print(df.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3adff14da3908959c142eb5e6c6ded8ebc71d3c"},"cell_type":"markdown","source":"### <a id='correlation'>Correlation between All Columns</a>"},{"metadata":{"_uuid":"41c9f2fa43dccdf9cf04f6c22528ae9d51e766fb"},"cell_type":"markdown","source":"* The 3 most important features for admission to the Master: CGPA, GRE SCORE, and TOEFL SCORE\n* The 3 least important features for admission to the Master: Research, LOR, and SOP"},{"metadata":{"trusted":true,"_uuid":"3f5e9f6fc6df12faa6e16d5c877674244d762b4e"},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(df.corr(), ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43ba5a7fdd9490fed8bd1a4aa4dd4057ae17c7dc"},"cell_type":"markdown","source":"## <a id='visualization'>DATA VISUALIZATION TO UNDERSTAND THE DATASET</a>"},{"metadata":{"_uuid":"e6870beb8f53d244c7ec21e158758aa11631f01f"},"cell_type":"markdown","source":"Having Research or not:\n* The majority of the candidates in the dataset have research experience.\n* Therefore, the Research will be a unimportant feature for the Chance of Admit. The correlation between Chance of Admit and Research was already lower than other correlation values."},{"metadata":{"trusted":true,"_uuid":"6124900faa4968bdb1770a89a308e24455ec3d7c"},"cell_type":"code","source":"print(\"Not Having Research:\",len(df[df.Research == 0]))\nprint(\"Having Research:\",len(df[df.Research == 1]))\ny = np.array([len(df[df.Research == 0]),len(df[df.Research == 1])])\nx = [\"Not Having Research\",\"Having Research\"]\nplt.bar(x,y)\nplt.title(\"Research Experience\")\nplt.xlabel(\"Canditates\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10e3d5554ec855b7f0792cf305554661efd01577"},"cell_type":"markdown","source":"TOEFL Score:\n* The lowest TOEFL score is 92 and the highest Toefl score is 120. The average is 107.41."},{"metadata":{"trusted":true,"_uuid":"70e9983f662834d5619865acef3a35432aa15c46"},"cell_type":"code","source":"y = np.array([df[\"TOEFL Score\"].min(),df[\"TOEFL Score\"].mean(),df[\"TOEFL Score\"].max()])\nx = [\"Worst\",\"Average\",\"Best\"]\nplt.bar(x,y)\nplt.title(\"TOEFL Scores\")\nplt.xlabel(\"Level\")\nplt.ylabel(\"TOEFL Score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bb4e7c5986a31c5e9f1246cb7edc8a48175d151"},"cell_type":"markdown","source":"GRE Score:\n* This histogram shows the frequency for GRE scores.\n* There is a density between 310 and 330. Being above this range would be a good feature for a candidate to stand out."},{"metadata":{"trusted":true,"_uuid":"15ad80f194a9309cc18926616e186f82d9ebfa84"},"cell_type":"code","source":"df[\"GRE Score\"].plot(kind = 'hist',bins = 200,figsize = (6,6))\nplt.title(\"GRE Scores\")\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60b8b349a1cc4b84ea4133b67d48b842789e87e2"},"cell_type":"markdown","source":"CGPA Scores for University Ratings:\n* As the quality of the university increases, the CGPA score increases."},{"metadata":{"trusted":true,"_uuid":"23913b7ec7a92ba3be2a8d722ba8038658a83415"},"cell_type":"code","source":"plt.scatter(df[\"University Rating\"],df.CGPA)\nplt.title(\"CGPA Scores for University Ratings\")\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"CGPA\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89f807d99c157d3191dd86b5c4c44b8ed19db07d"},"cell_type":"markdown","source":"* Candidates with high GRE scores usually have a high CGPA score."},{"metadata":{"trusted":true,"_uuid":"fbb6d8f7a853cb7b2b540f5dd79d1b066e7bc79a"},"cell_type":"code","source":"plt.scatter(df[\"GRE Score\"],df.CGPA)\nplt.title(\"CGPA for GRE Scores\")\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"CGPA\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8114167ba0c2edbef06526ee429cda512ec1c754"},"cell_type":"code","source":"df[df.CGPA >= 8.5].plot(kind='scatter', x='GRE Score', y='TOEFL Score',color=\"red\")\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"TOEFL SCORE\")\nplt.title(\"CGPA>=8.5\")\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"443c7718a6db198ac6303fa7d086b63736c34172"},"cell_type":"markdown","source":"* Candidates who graduate from good universities are more fortunate to be accepted."},{"metadata":{"trusted":true,"_uuid":"fad3c9b2b94c312683be920fe1b55e5b1c68e615"},"cell_type":"code","source":"s = df[df[\"Chance of Admit\"] >= 0.75][\"University Rating\"].value_counts().head(5)\nplt.title(\"University Ratings of Candidates with an 75% acceptance chance\")\ns.plot(kind='bar',figsize=(20, 10))\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"Candidates\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41f57f817994a57d81ecfc924044efbf81155e3f"},"cell_type":"markdown","source":"* Candidates with high CGPA scores usually have a high SOP score."},{"metadata":{"trusted":true,"_uuid":"6c9a92a58c657c0bec45690ddeccb3b701a49bee"},"cell_type":"code","source":"plt.scatter(df[\"CGPA\"],df.SOP)\nplt.xlabel(\"CGPA\")\nplt.ylabel(\"SOP\")\nplt.title(\"SOP for CGPA\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36fab0017941ebab4f1a547fecd31842b557d3ed"},"cell_type":"markdown","source":"* Candidates with high GRE scores usually have a high SOP score."},{"metadata":{"trusted":true,"_uuid":"a403c486e3bc367e1a20b686997ecba1dbd7afc9"},"cell_type":"code","source":"plt.scatter(df[\"GRE Score\"],df[\"SOP\"])\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"SOP\")\nplt.title(\"SOP for GRE Score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8efd5c67b512bb23504defff41495252147435f"},"cell_type":"markdown","source":"## <a id='regression'>REGRESSION ALGORITHMS (SUPERVISED MACHINE LEARNING ALGORITHMS</a>"},{"metadata":{"_uuid":"cc6287b37a9dbc193b79c651336ac25bb8aba886"},"cell_type":"markdown","source":"### <a id='prepareForRegression'>Preparing Data for Regression</a>"},{"metadata":{"_uuid":"637e8922f07a559a28ee3052405137d25815ca23"},"cell_type":"markdown","source":"* Serial No. is deleted because it stores index values. In the future this column does not benefit for regression algorithms."},{"metadata":{"trusted":true,"_uuid":"debd237df7583dc1cf8f5130009d2192ba8dbadb"},"cell_type":"code","source":"# reading the dataset\ndf = pd.read_csv(\"../input/Admission_Predict.csv\",sep = \",\")\n\n# it may be needed in the future.\nserialNo = df[\"Serial No.\"].values\n\ndf.drop([\"Serial No.\"],axis=1,inplace = True)\n\ndf=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fde1d9a8cbefe26d375bc3e49c10bcc92eb552f6"},"cell_type":"markdown","source":"train_test_split:\n* It splits the data into random train (80%) and test (20%) subsets."},{"metadata":{"trusted":true,"_uuid":"fcaec6e902558d31c4138080f4e70c0b6dba5fa0"},"cell_type":"code","source":"y = df[\"Chance of Admit\"].values\nx = df.drop([\"Chance of Admit\"],axis=1)\n\n# separating train (80%) and test (%20) sets\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.20,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e84d560a11c4b6dba6817873e332c6137c2dc10"},"cell_type":"markdown","source":"Normalization (min-max scaling):\n* It makes values  scaled to a fixed range (0-1)."},{"metadata":{"trusted":true,"_uuid":"48d4b7396123a747a4c7dbf1e468b0d567038ab8"},"cell_type":"code","source":"# normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscalerX = MinMaxScaler(feature_range=(0, 1))\nx_train[x_train.columns] = scalerX.fit_transform(x_train[x_train.columns])\nx_test[x_test.columns] = scalerX.transform(x_test[x_test.columns])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b664471a40f31d6895f614c305ae9b7c8da9b4ec"},"cell_type":"markdown","source":"Note about r2_score:\n* It is the regression score function. \n* The best possible score is 1.0 for r2_score.\n* It may be negative."},{"metadata":{"_uuid":"18c293bf5cb8952e737e126a9f938f42ee4067d3"},"cell_type":"markdown","source":"### <a id='linearRegression'>Linear Regression</a>"},{"metadata":{"trusted":true,"_uuid":"2cf51659e99256f495ef97870cd8d0ea6e0f61ff"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train,y_train)\ny_head_lr = lr.predict(x_test)\n\nprint(\"real value of y_test[1]: \" + str(y_test[1]) + \" -> the predict: \" + str(lr.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test[2]: \" + str(y_test[2]) + \" -> the predict: \" + str(lr.predict(x_test.iloc[[2],:])))\n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test,y_head_lr))\n\ny_head_lr_train = lr.predict(x_train)\nprint(\"r_square score (train dataset): \", r2_score(y_train,y_head_lr_train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"728869abddcbc4cf6dfe5e96f706f0b7d1847d77"},"cell_type":"markdown","source":"### <a id='rfRegression'>Random Forest Regression</a>"},{"metadata":{"trusted":true,"_uuid":"d7482a143f4bc3b1928f94e145fdf00746a0c196"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators = 100, random_state = 42)\nrfr.fit(x_train,y_train)\ny_head_rfr = rfr.predict(x_test) \n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test,y_head_rfr))\nprint(\"real value of y_test[1]: \" + str(y_test[1]) + \" -> the predict: \" + str(rfr.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test[2]: \" + str(y_test[2]) + \" -> the predict: \" + str(rfr.predict(x_test.iloc[[2],:])))\n\n\ny_head_rf_train = rfr.predict(x_train)\nprint(\"r_square score (train dataset): \", r2_score(y_train,y_head_rf_train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8071b1a8b998a34e2083725ae039a89f30cc327"},"cell_type":"markdown","source":"### <a id='dtRegression'>Decision Tree Regression</a>"},{"metadata":{"trusted":true,"_uuid":"8e7f7b3c2d3a51e050c0b0738b0899f2d9a98120"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor(random_state = 42)\ndtr.fit(x_train,y_train)\ny_head_dtr = dtr.predict(x_test) \n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test,y_head_dtr))\nprint(\"real value of y_test[1]: \" + str(y_test[1]) + \" -> the predict: \" + str(dtr.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test[2]: \" + str(y_test[2]) + \" -> the predict: \" + str(dtr.predict(x_test.iloc[[2],:])))\n\ny_head_dtr_train = dtr.predict(x_train)\nprint(\"r_square score (train dataset): \", r2_score(y_train,y_head_dtr_train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25af12c559e39ea1a6823428bccf95957401f7e7"},"cell_type":"markdown","source":"### <a id='comparisonOfRegression'>Comparison of Regression Algorithms</a>"},{"metadata":{"_uuid":"2e7d0922e65f68eecedc8d7a61013c49fea63c58"},"cell_type":"markdown","source":"* Linear regression and random forest regression algorithms were better than decision tree regression algorithm."},{"metadata":{"trusted":true,"_uuid":"1065bf8205e7df052fe41a1ae628be558c42ba4d"},"cell_type":"code","source":"y = np.array([r2_score(y_test,y_head_lr),r2_score(y_test,y_head_rfr),r2_score(y_test,y_head_dtr)])\nx = [\"LinearRegression\",\"RandomForestReg.\",\"DecisionTreeReg.\"]\nplt.bar(x,y)\nplt.title(\"Comparison of Regression Algorithms\")\nplt.xlabel(\"Regressor\")\nplt.ylabel(\"r2_score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e160c570b52aae1e5ca22a65d9ac237a1d0c2923"},"cell_type":"markdown","source":"* These are the regression estimates for samples with 5 and 50 indexes:"},{"metadata":{"trusted":true,"_uuid":"d12981ea7b8ac0787af74d257075a7c5f6b598c9"},"cell_type":"code","source":"print(\"real value of y_test[5]: \" + str(y_test[5]) + \" -> the predict: \" + str(lr.predict(x_test.iloc[[5],:])))\nprint(\"real value of y_test[5]: \" + str(y_test[5]) + \" -> the predict: \" + str(rfr.predict(x_test.iloc[[5],:])))\nprint(\"real value of y_test[5]: \" + str(y_test[5]) + \" -> the predict: \" + str(dtr.predict(x_test.iloc[[5],:])))\n\nprint()\n\nprint(\"real value of y_test[50]: \" + str(y_test[50]) + \" -> the predict: \" + str(lr.predict(x_test.iloc[[50],:])))\nprint(\"real value of y_test[50]: \" + str(y_test[50]) + \" -> the predict: \" + str(rfr.predict(x_test.iloc[[50],:])))\nprint(\"real value of y_test[50]: \" + str(y_test[50]) + \" -> the predict: \" + str(dtr.predict(x_test.iloc[[50],:])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d9ccbb44bb2fa13f7b765e55ef64c531037eac9"},"cell_type":"markdown","source":"This is the estimate and the actual acceptance possibilities made with 3 regression algorithms for test samples with 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75 indexes:"},{"metadata":{"trusted":true,"_uuid":"e2f797c825c9ca5377303c07644b1b89aa98eeaa"},"cell_type":"code","source":"red = plt.scatter(np.arange(0,80,5),y_head_lr[0:80:5],color = \"red\")\ngreen = plt.scatter(np.arange(0,80,5),y_head_rfr[0:80:5],color = \"green\")\nblue = plt.scatter(np.arange(0,80,5),y_head_dtr[0:80:5],color = \"blue\")\nblack = plt.scatter(np.arange(0,80,5),y_test[0:80:5],color = \"black\")\nplt.title(\"Comparison of Regression Algorithms\")\nplt.xlabel(\"Index of Candidate\")\nplt.ylabel(\"Chance of Admit\")\nplt.legend((red,green,blue,black),('LR', 'RFR', 'DTR', 'REAL'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"134e6d45adac8345abaf31a6a66b90759deeed5d"},"cell_type":"markdown","source":"Comment:\n* Because most candidates in the data have over 70% chance, many unsuccessful candidates are not well predicted."},{"metadata":{"trusted":true,"_uuid":"0e028c64eb9aecfec5b3aea7d4f59c99b66f5a42"},"cell_type":"code","source":"df[\"Chance of Admit\"].plot(kind = 'hist',bins = 200,figsize = (6,6))\nplt.title(\"Chance of Admit\")\nplt.xlabel(\"Chance of Admit\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f80bad7f38f9c317d195fb23679efe775949e81d"},"cell_type":"markdown","source":"## <a id='classification'>CLASSIFICATION ALGORITHMS (SUPERVISED MACHINE LEARNING ALGORITHMS)</a>"},{"metadata":{"_uuid":"d7e9e22320a95bdf5da4114ab2a4f3fb31431434"},"cell_type":"markdown","source":"### <a id='prepareForClassification'>Preparing Data for Classification</a>"},{"metadata":{"_uuid":"7e5f1f669e601921df6c9b253a45b78816443d34"},"cell_type":"markdown","source":"* If a candidate's Chance of Admit is greater than 80%, the candidate will receive the 1 label.\n* If a candidate's Chance of Admit is less than or equal to 80%, the candidate will receive the 0 label."},{"metadata":{"trusted":true,"_uuid":"ac3b7c9a589681440d740d4e209bf137f6e7c9e8"},"cell_type":"code","source":"# reading the dataset\ndf = pd.read_csv(\"../input/Admission_Predict.csv\",sep = \",\")\n\n# it may be needed in the future.\nserialNo = df[\"Serial No.\"].values\ndf.drop([\"Serial No.\"],axis=1,inplace = True)\n\ndf=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})\ny = df[\"Chance of Admit\"].values\nx = df.drop([\"Chance of Admit\"],axis=1)\n\n# separating train (80%) and test (%20) sets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.20,random_state = 42)\n\n# normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscalerX = MinMaxScaler(feature_range=(0, 1))\nx_train[x_train.columns] = scalerX.fit_transform(x_train[x_train.columns])\nx_test[x_test.columns] = scalerX.transform(x_test[x_test.columns])\n\ny_train_01 = [1 if each > 0.8 else 0 for each in y_train]\ny_test_01  = [1 if each > 0.8 else 0 for each in y_test]\n\n# list to array\ny_train_01 = np.array(y_train_01)\ny_test_01 = np.array(y_test_01)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29282d378cfc31169cb9fa753e4fd0cdb8296ff8"},"cell_type":"markdown","source":"Note about score:\n* It is the mean accuracy for test data and labels.\n\nNote about precision:\n* precision =  TP / (TP + FP)\n\nNote about recall:\n* recall = TP / (TP + FN)\n\nNote about F!:\n* F1 = 2 / ((1/precision)+(1/recall))"},{"metadata":{"_uuid":"203b2ef2f7bc64cefec19c2e61aad9b3530dc605"},"cell_type":"markdown","source":"### <a id='lr'>Logistic Regression</a>"},{"metadata":{"_uuid":"5d8fac278c0f66b0051812111a75a2767bf306e1"},"cell_type":"markdown","source":"Confusion Matrix:\n\nFor Actual 1: 29\n* Predicted 1: 22\n* Predicted 0: 7\n* According to Confusion Matrix, the model predicted that 23 candidate's Chances of Admit are greater than 80%. In reality, 22 of them have a Chance of Admit greater than 80%. In total, 29 candidate's Chances of Admit are greater than 80%.\n\nFor Actual 0: 51\n* Predicted 1: 1\n* Predicted 0: 50\n* According to Confusion Matrix, the model predicted that 57 candidate's Chances of Admit are less than or equal to 80%. In reality, 50 of them have a Chance of Admit less than or equal to 80%. In total, 51 candidate's Chances of Admit are less than or equal to 80%."},{"metadata":{"trusted":true,"_uuid":"efca46648c50302d36b6c16afb9a5b6b653194c4"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlrc = LogisticRegression()\nlrc.fit(x_train,y_train_01)\nprint(\"score: \", lrc.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]: \" + str(y_test_01[1]) + \" -> the predict: \" + str(lrc.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]: \" + str(y_test_01[2]) + \" -> the predict: \" + str(lrc.predict(x_test.iloc[[2],:])))\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_lrc = confusion_matrix(y_test_01,lrc.predict(x_test))\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\n\n# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_lrc,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01,lrc.predict(x_test)))\nprint(\"recall_score: \", recall_score(y_test_01,lrc.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01,lrc.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9e906b7a72feeaad2803c331572a3b49aab0c6e"},"cell_type":"markdown","source":"Test for Train Dataset:"},{"metadata":{"trusted":true,"_uuid":"95294347967cd1975010c940b38a5ff2cca1e608"},"cell_type":"code","source":"cm_lrc_train = confusion_matrix(y_train_01,lrc.predict(x_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_lrc_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43b12a0d5595b37e0175733a7a13f283fedbb2cb"},"cell_type":"markdown","source":"### <a id='svm'>Support Vector Machine</a>"},{"metadata":{"_uuid":"b29c77a14b971704e5ede8c4309ce55c4286c7a9"},"cell_type":"markdown","source":"Confusion Matrix:\n\nFor Actual 1: 29\n* Predicted 1: 22\n* Predicted 0: 7\n\nFor Actual 0: 51\n* Predicted 1: 1\n* Predicted 0: 50"},{"metadata":{"trusted":true,"_uuid":"9f3c8214de2a2b7e6d4987b492a221fb75d4501f"},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train_01)\nprint(\"score: \", svm.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]: \" + str(y_test_01[1]) + \" -> the predict: \" + str(svm.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]: \" + str(y_test_01[2]) + \" -> the predict: \" + str(svm.predict(x_test.iloc[[2],:])))\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_svm = confusion_matrix(y_test_01,svm.predict(x_test))\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\n\n# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_svm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01,svm.predict(x_test)))\nprint(\"recall_score: \", recall_score(y_test_01,svm.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01,svm.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8499674702ce2563117dab6822be6a0019159cce"},"cell_type":"markdown","source":"Test for Train Dataset:"},{"metadata":{"trusted":true,"_uuid":"129455379e16986f2a753aafeed0579a815c8123"},"cell_type":"code","source":"cm_svm_train = confusion_matrix(y_train_01,svm.predict(x_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_svm_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91cd4e4c3cef39b3dca1c28c9946dca24b15116d"},"cell_type":"markdown","source":"### <a id='gnb'>Gaussian Naive Bayes</a>"},{"metadata":{"_uuid":"2f959ddda4636403a1fb1e755ee666a77fa48a5f"},"cell_type":"markdown","source":"Confusion Matrix:\n\nFor Actual 1: 29\n* Predicted 1: 28\n* Predicted 0: 1\n\nFor Actual 0: 51\n* Predicted 1: 2\n* Predicted 0: 49"},{"metadata":{"trusted":true,"_uuid":"204a83f5094c7c2449c7b6578be76fc0388793a8"},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train_01)\nprint(\"score: \", nb.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]: \" + str(y_test_01[1]) + \" -> the predict: \" + str(nb.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]: \" + str(y_test_01[2]) + \" -> the predict: \" + str(nb.predict(x_test.iloc[[2],:])))\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_nb = confusion_matrix(y_test_01,nb.predict(x_test))\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\n# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_nb,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01,nb.predict(x_test)))\nprint(\"recall_score: \", recall_score(y_test_01,nb.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01,nb.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c8180736c170e9f286d4993c8d6ea7479d9aa5f"},"cell_type":"markdown","source":"Test for Train Dataset:"},{"metadata":{"trusted":true,"_uuid":"e3110f44f8b137762528c8de67f334c22ef4ac2f"},"cell_type":"code","source":"cm_nb_train = confusion_matrix(y_train_01,nb.predict(x_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_nb_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcd7b3102824616abeb3d353bf847ea1a5673f89"},"cell_type":"markdown","source":"### <a id='dtc'>Decision Tree Classification</a>"},{"metadata":{"_uuid":"a33ed71135dd546673fd91c89371ac2c4511e638"},"cell_type":"markdown","source":"Confusion Matrix:\n\nFor Actual 1: 29\n* Predicted 1: 26\n* Predicted 0: 3\n\nFor Actual 0: 51\n* Predicted 1: 1\n* Predicted 0: 50"},{"metadata":{"trusted":true,"_uuid":"b025766a01a696db721f0487b677f16c37725706"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train,y_train_01)\nprint(\"score: \", dtc.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]: \" + str(y_test_01[1]) + \" -> the predict: \" + str(dtc.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]: \" + str(y_test_01[2]) + \" -> the predict: \" + str(dtc.predict(x_test.iloc[[2],:])))\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_dtc = confusion_matrix(y_test_01,dtc.predict(x_test))\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\n\n# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_dtc,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01,dtc.predict(x_test)))\nprint(\"recall_score: \", recall_score(y_test_01,dtc.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01,dtc.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06ad938d5ddb565cd6227c7e0ad12c426cbb1216"},"cell_type":"markdown","source":"Test for Train Dataset:"},{"metadata":{"trusted":true,"_uuid":"ac45a927a83dd2fd037573e3dc045f1e4b1b9e67"},"cell_type":"code","source":"cm_dtc_train = confusion_matrix(y_train_01,dtc.predict(x_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_dtc_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f46b014b10cfb3c9fa8177b8df846fe7f113898a"},"cell_type":"markdown","source":"### <a id='rfc'>Random Forest Classification</a>"},{"metadata":{"_uuid":"b58a50a2be658f255c3a38a8afa8f147767fadfb"},"cell_type":"markdown","source":"Confusion Matrix:\n\nFor Actual 1: 29\n* Predicted 1: 25\n* Predicted 0: 4\n\nFor Actual 0: 51\n* Predicted 1: 1\n* Predicted 0: 50"},{"metadata":{"trusted":true,"_uuid":"c9e6b07252f8a43bfad5348245c18b6872d95487"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrfc.fit(x_train,y_train_01)\nprint(\"score: \", rfc.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]: \" + str(y_test_01[1]) + \" -> the predict: \" + str(rfc.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]: \" + str(y_test_01[2]) + \" -> the predict: \" + str(rfc.predict(x_test.iloc[[2],:])))\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_rfc = confusion_matrix(y_test_01,rfc.predict(x_test))\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\n# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_rfc,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01,rfc.predict(x_test)))\nprint(\"recall_score: \", recall_score(y_test_01,rfc.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01,rfc.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"230a59e569d419662fc2fc8c443ad98a02213ba5"},"cell_type":"markdown","source":"Test for Train Dataset:"},{"metadata":{"trusted":true,"_uuid":"c52c860781e0b25e6503b7c2c93cfbd6cb970e3f"},"cell_type":"code","source":"cm_rfc_train = confusion_matrix(y_train_01,rfc.predict(x_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_rfc_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bca20f22daa50658e34d1cbff5807ac4513b0a3"},"cell_type":"markdown","source":"### <a id='knnc'>K Nearest Neighbors Classification</a>\n"},{"metadata":{"_uuid":"353bf624ce2720f42bb1458b6b06249202512fb9"},"cell_type":"markdown","source":"Finding the k value:\n* As a result of the test, the best k value is 3."},{"metadata":{"_uuid":"26dfa5773de5201a04ea171c8919adf270aff766"},"cell_type":"markdown","source":"Confusion Matrix:\n\nFor Actual 1: 29\n* Predicted 1: 26\n* Predicted 0: 3\n\nFor Actual 0: 51\n* Predicted 1: 2\n* Predicted 0: 49"},{"metadata":{"trusted":true,"_uuid":"d24bd88f98c727a344d2f8f3325e8fab3a22760a"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n# finding k value\nscores = []\nfor each in range(1,50):\n    knn_n = KNeighborsClassifier(n_neighbors = each)\n    knn_n.fit(x_train,y_train_01)\n    scores.append(knn_n.score(x_test,y_test_01))\n    \nplt.plot(range(1,50),scores)\nplt.xlabel(\"k\")\nplt.ylabel(\"accuracy\")\nplt.show()\n\nknn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\nknn.fit(x_train,y_train_01)\nprint(\"score of 3 :\",knn.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]: \" + str(y_test_01[1]) + \" -> the predict: \" + str(knn.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]: \" + str(y_test_01[2]) + \" -> the predict: \" + str(knn.predict(x_test.iloc[[2],:])))\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_knn = confusion_matrix(y_test_01,knn.predict(x_test))\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\n\n# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_knn,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01,knn.predict(x_test)))\nprint(\"recall_score: \", recall_score(y_test_01,knn.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01,knn.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46c71c3d9fb6984bf436425d42767209d5bd8b3c"},"cell_type":"markdown","source":"Test for Train Dataset:"},{"metadata":{"trusted":true,"_uuid":"58ade96426426ada699dacfdbc8a750e2c52a290"},"cell_type":"code","source":"cm_knn_train = confusion_matrix(y_train_01,knn.predict(x_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_knn_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2947eb581b8a12f663c1531d72000a78ccae2f45"},"cell_type":"markdown","source":"### <a id='comparisonOfClassification'>Comparison of Classification Algorithms</a>"},{"metadata":{"_uuid":"7a555bb457ba2492bad1e9b83b53241f2283de30"},"cell_type":"markdown","source":"* All classification algorithms achieved around 90% success. The most successful one is Gaussian Naive Bayes with 96% score."},{"metadata":{"trusted":true,"_uuid":"2c9a2c1e937b6ca002fb5bcea5b85b79072e32ed"},"cell_type":"code","source":"y = np.array([lrc.score(x_test,y_test_01),svm.score(x_test,y_test_01),nb.score(x_test,y_test_01),dtc.score(x_test,y_test_01),rfc.score(x_test,y_test_01),knn.score(x_test,y_test_01)])\n#x = [\"LogisticRegression\",\"SVM\",\"GaussianNB\",\"DecisionTreeClassifier\",\"RandomForestClassifier\",\"KNeighborsClassifier\"]\nx = [\"LogisticReg.\",\"SVM\",\"GNB\",\"Dec.Tree\",\"Ran.Forest\",\"KNN\"]\n\nplt.bar(x,y)\nplt.title(\"Comparison of Classification Algorithms\")\nplt.xlabel(\"Classfication\")\nplt.ylabel(\"Score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c368329cc9dbbbc81fb86fef56efb78b435a4a4"},"cell_type":"markdown","source":"## <a id='introduction'>CLUSTERING ALGORITHMS (UNSUPERVISED MACHINE LEARNING ALGORITHMS)</a>"},{"metadata":{"_uuid":"01fd42b8b2115bd8581c0ee2f695972d85e00491"},"cell_type":"markdown","source":"### <a id='prepareForClustering'>Preparing Data for Clustering</a>"},{"metadata":{"trusted":true,"_uuid":"3b13a689463021fcc70d600f889ec4cfe096c02e"},"cell_type":"code","source":"df = pd.read_csv(\"../input/Admission_Predict.csv\",sep = \",\")\ndf=df.rename(columns = {'Chance of Admit ':'ChanceOfAdmit'})\nserial = df[\"Serial No.\"]\ndf.drop([\"Serial No.\"],axis=1,inplace = True)\ndf = (df- np.min(df))/(np.max(df)-np.min(df))\ny = df.ChanceOfAdmit \nx = df.drop([\"ChanceOfAdmit\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f960c85f03890c0b69753105485251aebaaa02f"},"cell_type":"markdown","source":"### <a id='pca'>Principal Component Analysis</a>"},{"metadata":{"_uuid":"e960d3ad5c299c82bc6f1e5b5b910a626d6733fc"},"cell_type":"markdown","source":"[](http://)* All features (x) were collected in one feature with Principal Component Analysis."},{"metadata":{"trusted":true,"_uuid":"a4273a78fb6eed1f4f62eb12a9ff6ac77874ec89"},"cell_type":"code","source":"# for data visualization\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 1, whiten= True )  # whitten = normalize\npca.fit(x)\nx_pca = pca.transform(x)\nx_pca = x_pca.reshape(400,)\ndictionary = {\"x\":x_pca,\"y\":y}\ndata = pd.DataFrame(dictionary)\nprint(\"data:\")\nprint(data.head())\nprint(\"\\ndf:\")\nprint(df.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4924975d9d56a41103f7c392d75e36f4074e16ee"},"cell_type":"markdown","source":"### <a id='kmeans'>K-means Clustering</a>"},{"metadata":{"_uuid":"156e4beb0cc1cbf13ad434e5d220208396c0a90d"},"cell_type":"markdown","source":"* The elbow method is used to determine the best number of clusters for k-means clustering. The number is 3."},{"metadata":{"trusted":true,"_uuid":"716afbf0052e57b749566bdb091fdec3c6bec6e5"},"cell_type":"code","source":"df[\"Serial No.\"] = serial\nfrom sklearn.cluster import KMeans\nwcss = []\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,15),wcss)\nplt.xlabel(\"k values\")\nplt.ylabel(\"WCSS\")\nplt.show()\n\nkmeans = KMeans(n_clusters=3)\nclusters_knn = kmeans.fit_predict(x)\n\ndf[\"label_kmeans\"] = clusters_knn\n\n\nplt.scatter(df[df.label_kmeans == 0 ][\"Serial No.\"],df[df.label_kmeans == 0].ChanceOfAdmit,color = \"red\")\nplt.scatter(df[df.label_kmeans == 1 ][\"Serial No.\"],df[df.label_kmeans == 1].ChanceOfAdmit,color = \"blue\")\nplt.scatter(df[df.label_kmeans == 2 ][\"Serial No.\"],df[df.label_kmeans == 2].ChanceOfAdmit,color = \"green\")\nplt.title(\"K-means Clustering\")\nplt.xlabel(\"Candidates\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()\n\ndf[\"label_kmeans\"] = clusters_knn\nplt.scatter(data.x[df.label_kmeans == 0 ],data[df.label_kmeans == 0].y,color = \"red\")\nplt.scatter(data.x[df.label_kmeans == 1 ],data[df.label_kmeans == 1].y,color = \"blue\")\nplt.scatter(data.x[df.label_kmeans == 2 ],data[df.label_kmeans == 2].y,color = \"green\")\nplt.title(\"K-means Clustering\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1a6405e5d41466161746f55355de6e4bf9be942"},"cell_type":"markdown","source":"### <a id='hierarchical '>Hierarchical Clustering</a>"},{"metadata":{"_uuid":"1b84326feb3a2b028022048d3500969832fa9e4e"},"cell_type":"markdown","source":"* The dendrogram method is used to determine the best number of clusters for hierarchical clustering. The number is 3 again."},{"metadata":{"trusted":true,"_uuid":"6cc329491687ef3e0eb2a7d08aa6179a2fce55e9"},"cell_type":"code","source":"df[\"Serial No.\"] = serial\n\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nmerg = linkage(x,method=\"ward\")\ndendrogram(merg,leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()\n\nfrom sklearn.cluster import AgglomerativeClustering\nhiyerartical_cluster = AgglomerativeClustering(n_clusters = 3,affinity= \"euclidean\",linkage = \"ward\")\nclusters_hiyerartical = hiyerartical_cluster.fit_predict(x)\n\ndf[\"label_hiyerartical\"] = clusters_hiyerartical\n\nplt.scatter(df[df.label_hiyerartical == 0 ][\"Serial No.\"],df[df.label_hiyerartical == 0].ChanceOfAdmit,color = \"red\")\nplt.scatter(df[df.label_hiyerartical == 1 ][\"Serial No.\"],df[df.label_hiyerartical == 1].ChanceOfAdmit,color = \"blue\")\nplt.scatter(df[df.label_hiyerartical == 2 ][\"Serial No.\"],df[df.label_hiyerartical == 2].ChanceOfAdmit,color = \"green\")\nplt.title(\"Hierarchical Clustering\")\nplt.xlabel(\"Candidates\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()\n\nplt.scatter(data[df.label_hiyerartical == 0 ].x,data.y[df.label_hiyerartical == 0],color = \"red\")\nplt.scatter(data[df.label_hiyerartical == 1 ].x,data.y[df.label_hiyerartical == 1],color = \"blue\")\nplt.scatter(data[df.label_hiyerartical == 2 ].x,data.y[df.label_hiyerartical == 2],color = \"green\")\nplt.title(\"Hierarchical Clustering\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c309ddbfcff130a07a967a47cc78871879ad011f"},"cell_type":"code","source":"print(df.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1cd63580ee3030d6745b5e9c20cbe9aa407a1bc"},"cell_type":"markdown","source":"### <a id='comparisonOfClustering'>Comparison of Clustering Algorithms</a>"},{"metadata":{"_uuid":"9f5b5334adb169056c5f5d50ab7adb1b661bdd9a"},"cell_type":"markdown","source":"* K-means Clustering and Hierarchical Clustering are similarly."},{"metadata":{"_uuid":"bc3a1ef92f4fd84e9f1c234eb9b68518a3543ee9"},"cell_type":"markdown","source":"## <a id='feature'>THE THREE IMPORTANT FEATURES</a>"},{"metadata":{"_uuid":"b2661fd7f0d77b6dea397f4ac13931558deb9774"},"cell_type":"markdown","source":"### <a id='correlationForFeature'>Correlation between All Columns</a>"},{"metadata":{"_uuid":"65d875efa4c4c2aa628a1560dde11ac03621e989"},"cell_type":"markdown","source":"* The 3 most important features for admission to the Master: CGPA, GRE SCORE, and TOEFL SCORE\n* The 3 least important features for admission to the Master: Research, LOR, and SOP"},{"metadata":{"trusted":true,"_uuid":"e9f2f2b70f4191a827c9dc344a680e113f79ff7a"},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(df.corr(), ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8891ca935bad550ccc9fba9b311ff34b15ae9a00"},"cell_type":"markdown","source":"### <a id='ThreeLinearRegression'>The Three Features for Linear Regression</a>"},{"metadata":{"_uuid":"b096599719cbce5613498f46e46967bd428ebad6"},"cell_type":"markdown","source":"* The first results for Linear Regression (7 features):\n<br> r_square score:  0.821208259148699\n\n* The results for Linear Regression now (3 features):                               \nr_square score:  0.8212241793299223\n\n* The two results are very close. If these 3 features (CGPA, GRE SCORE, and TOEFL SCORE) are used instead of all 7 features together, the result is not bad and performance is increased because less calculation is required.                                 \n"},{"metadata":{"trusted":true,"_uuid":"720652ccf338d6c20e5fe7bc3b3d84c00f52c717"},"cell_type":"code","source":"df = pd.read_csv(\"../input/Admission_Predict.csv\",sep = \",\")\ndf=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})\nnewDF = pd.DataFrame()\nnewDF[\"GRE Score\"] = df[\"GRE Score\"]\nnewDF[\"TOEFL Score\"] = df[\"TOEFL Score\"]\nnewDF[\"CGPA\"] = df[\"CGPA\"]\nnewDF[\"Chance of Admit\"] = df[\"Chance of Admit\"]\n\ny_new = df[\"Chance of Admit\"].values\nx_new = df.drop([\"Chance of Admit\"],axis=1)\n\n# separating train (80%) and test (%20) sets\nfrom sklearn.model_selection import train_test_split\nx_train_new, x_test_new,y_train_new, y_test_new = train_test_split(x_new,y_new,test_size = 0.20,random_state = 42)\n\n# normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscalerX = MinMaxScaler(feature_range=(0, 1))\nx_train[x_train.columns] = scalerX.fit_transform(x_train[x_train.columns])\nx_test[x_test.columns] = scalerX.transform(x_test[x_test.columns])\n\nfrom sklearn.linear_model import LinearRegression\nlr_new = LinearRegression()\nlr_new.fit(x_train_new,y_train_new)\ny_head_lr_new = lr_new.predict(x_test_new)\n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test_new,y_head_lr_new))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
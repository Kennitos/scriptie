{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tensorflow, keras, scikit\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras import backend as K\nfrom keras.utils import np_utils\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport sklearn.model_selection as sk\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n\n# helper imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparamter gird space options\noptimizer = ['Adam', 'SGD']\nactivation = ['relu', 'sigmoid']\nbatch_size = [50, 100] \nepochs = [5, 10]\nneurons = [64,128,284]\n\n# hyperparamter gird space\nhyperparamter_space = dict(\n    batch_size=batch_size, \n    epochs=epochs,\n    optimizer=optimizer,\n    activation=activation,\n    neurons=neurons\n)\n\n# f1_score calculation function for model evaluation\ndef f1_score(y_true, y_pred):\n\n    # Count positive samples.\n    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n\n    # If there are no true samples, fix the F1 score at 0.\n    if c3 == 0:\n        return 0\n\n    # How many selected items are relevant?\n    precision = c1 / c2\n\n    # How many relevant items are selected?\n    recall = c1 / c3\n\n    # Calculate f1_score\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    return f1_score\n\n# reading csv    \ndef read_csv(filepath, has_header=None):\n    # data import\n    data = pd.read_csv(filepath)\n    return data\n\n# model creation functoin\ndef create_model(f1_score=f1_score,neurons=128,activation='relu',optimizer='sgd'):\n\n    # model init; sequential MLP \n    model = Sequential()\n\n    # layer1: input layer, with kernel initiazer function same as class distriubtion function        \n    model.add(\n        Dense(256, activation=activation, kernel_initializer = 'normal')\n    )\n\n    # layer2: dropout for preventing overfitting \n    model.add(Dropout(0.5))\n\n    # layer3: activation        \n    model.add(\n        Dense(128, activation=activation)\n    )\n\n    # layer4: dropout\n    model.add(Dropout(0.2))        \n\n    # layer5: output with softmax        \n    model.add(\n        Dense(3, activation=tf.nn.softmax)\n    )\n\n    # print model summary \n    print(model.summary())\n\n    # compile model\n    model.compile(\n        optimizer=optimizer, \n        loss='sparse_categorical_crossentropy',\n        metrics=[f1_score, 'accuracy'] # f1 score\n    )\n\n    return model\n\n# fit model\ndef fit_model(model, x_train, y_train, epochs=10, verbose=1):\n\n    # fit model and retrieve history \n    history = model.fit(x_train, y_train, epochs=epochs,verbose=verbose)\n\n    # Plot training & validation f1 values\n    plt.plot(history.history['f1_score'])\n    plt.title('Model F1')\n    plt.ylabel('F1')\n    plt.xlabel('Epoch')\n    plt.legend(['Train'], loc='upper left')\n    plt.show()\n\n    return history\n\n# evaluate model\ndef evaluate_model(model, x_test, y_test):\n\n    test_loss, test_f1, test_acc = model.evaluate(x_test, y_test)\n\n    print('Test F1 Score:', test_f1)\n\n    return test_loss, test_f1, test_acc\n\n# split for cross validation     \ndef split_by_test_size(x, y, test_size=0.33):\n\n    # split 1: train, test\n    x_train, x_test, y_train, y_test = sk.train_test_split(\n        x,\n        y,\n        test_size=test_size\n    )\n\n    return x_train, x_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = read_csv('../input/train.csv', True)\ndataset.head()","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"      id            ...             toxicity_annotator_count\n0  59848            ...                                    4\n1  59849            ...                                    4\n2  59852            ...                                    4\n3  59855            ...                                    4\n4  59856            ...                                   47\n\n[5 rows x 45 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>asian</th>\n      <th>atheist</th>\n      <th>bisexual</th>\n      <th>black</th>\n      <th>buddhist</th>\n      <th>christian</th>\n      <th>female</th>\n      <th>heterosexual</th>\n      <th>hindu</th>\n      <th>homosexual_gay_or_lesbian</th>\n      <th>intellectual_or_learning_disability</th>\n      <th>jewish</th>\n      <th>latino</th>\n      <th>male</th>\n      <th>muslim</th>\n      <th>other_disability</th>\n      <th>other_gender</th>\n      <th>other_race_or_ethnicity</th>\n      <th>other_religion</th>\n      <th>other_sexual_orientation</th>\n      <th>physical_disability</th>\n      <th>psychiatric_or_mental_illness</th>\n      <th>transgender</th>\n      <th>white</th>\n      <th>created_date</th>\n      <th>publication_id</th>\n      <th>parent_id</th>\n      <th>article_id</th>\n      <th>rating</th>\n      <th>funny</th>\n      <th>wow</th>\n      <th>sad</th>\n      <th>likes</th>\n      <th>disagree</th>\n      <th>sexual_explicit</th>\n      <th>identity_annotator_count</th>\n      <th>toxicity_annotator_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59848</td>\n      <td>0.000000</td>\n      <td>This is so cool. It's like, 'would you want yo...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:41.987077+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>59849</td>\n      <td>0.000000</td>\n      <td>Thank you!! This would make my life a lot less...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:42.870083+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59852</td>\n      <td>0.000000</td>\n      <td>This is such an urgent design problem; kudos t...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:45.222647+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59855</td>\n      <td>0.000000</td>\n      <td>Is this something I'll be able to install on m...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:47.601894+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59856</td>\n      <td>0.893617</td>\n      <td>haha you guys are a bunch of losers.</td>\n      <td>0.021277</td>\n      <td>0.0</td>\n      <td>0.021277</td>\n      <td>0.87234</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015-09-29 10:50:48.488476+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>47</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encode labels\ndef reassign_label_by_group(dataset, new_col, col_list, label, threshold_val=0.3):\n    \n    for col in col_list:\n        dataset['target'] = (\n            dataset[col] > threshold_val\n        )*label \n        \n    return dataset\n\n#\nreligions = [\n    'atheist',\n    'buddhist',\n    'christian',\n    'hindu',\n    'jewish',\n    'muslim',\n    'other_religion'\n]\ndataset = one_hot_encode_by_label(dataset, 'religion', religions, 1)\n\n#\nethnicity = [\n    'asian',\n    'black',\n    'latino',\n    'white',\n    'other_race_or_ethnicity'\n]\ndataset = one_hot_encode_by_label(dataset, 'ethnicity', ethnicity, 2)\n\n# \nsexualOrientation = [\n    'bisexual',\n    'heterosexual',\n    'homosexual_gay_or_lesbian',\n    'other_gender',\n    'transgender',\n    'other_sexual_orientation'    \n]\ndataset = one_hot_encode_by_label(dataset, 'sexualOrientation', sexualOrientation, 3)\n\n# dataset.hist(column=\"religion\")\n# dataset.hist(column=\"ethnicity\")\n# dataset.hist(column=\"sexualOrientation\")\n\n# dataset = (\n#     dataset['religion'] == 0 and \n#     dataset['ethnicity'] == 0 and\n#     dataset['sexualOrientation'] == 0\n# )\n# dataset[\n#     dataset['religion'] == 1\n# ].tail()\n# dataset[\n#     dataset['ethnicity'] == 1\n# ].tail()\n\n# # 1804874\n# # len(dataset)","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model creation functoin\ndef create_model(f1_score=f1_score,neurons=128,activation='relu',optimizer='sgd'):\n\n    # model init; sequential MLP \n    model = Sequential()\n\n    # layer1: input layer, with kernel initiazer function same as class distriubtion function        \n    model.add(\n        Dense(256, activation=activation, input_shape=(1,), kernel_initializer = 'normal')\n    )\n\n    # layer2: dropout for preventing overfitting \n    model.add(Dropout(0.5))\n\n    # layer3: activation        \n    model.add(\n        Dense(128, activation=activation)\n    )\n\n    # layer4: dropout\n    model.add(Dropout(0.2))        \n\n    # layer5: output with softmax        \n    model.add(\n        Dense(5, activation=tf.nn.softmax)\n    )\n\n    # print model summary \n    print(model.summary())\n\n    # compile model\n    model.compile(\n        optimizer=optimizer, \n        loss='sparse_categorical_crossentropy',\n        metrics=[f1_score, 'accuracy'] # f1 score\n    )\n\n    return model\n\n# fit model\ndef fit_model(model, x_train, y_train, epochs=10, verbose=1):\n\n    # fit model and retrieve history \n    history = model.fit(x_train, y_train, epochs=epochs,verbose=verbose)\n\n    # Plot training & validation f1 values\n    plt.plot(history.history['f1_score'])\n    plt.title('Model F1')\n    plt.ylabel('F1')\n    plt.xlabel('Epoch')\n    plt.legend(['Train'], loc='upper left')\n    plt.show()\n\n    return history\n\n# evaluate model\ndef evaluate_model(model, x_test, y_test):\n\n    test_loss, test_f1, test_acc = model.evaluate(x_test, y_test)\n\n    print('Test F1 Score:', test_f1)\n\n    return test_loss, test_f1, test_acc\n","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()\n\nfeatures = dataset['comment_text']\nlabels = dataset['target']\n\nx_train, x_test, y_train, y_test = split_by_test_size(\n    features, \n    labels\n)\n\nmodel = create_model()\nmodel = fit_model(model,x_train,y_train)","execution_count":52,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_13 (Dense)             (None, 256)               512       \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_14 (Dense)             (None, 128)               32896     \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 128)               0         \n_________________________________________________________________\ndense_15 (Dense)             (None, 5)                 645       \n=================================================================\nTotal params: 34,053\nTrainable params: 34,053\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/10\n","name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"could not convert string to float: \"Time to initiate the death penalty and remove some of the evil permanently from society. You can't rehabilitate pure evil. Some of the garbage are not in any way, shape, or form fit to live amongst law abiding citizens (even in a cage). And we shouldn't have to wait 30 years for the appeal process. What a joke.\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-b1a6d443017b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-51-0db91637f361>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, x_train, y_train, epochs, verbose)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# fit model and retrieve history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Plot training & validation f1 values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2653\u001b[0m                 array_vals.append(\n\u001b[1;32m   2654\u001b[0m                     np.asarray(value,\n\u001b[0;32m-> 2655\u001b[0;31m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[1;32m   2656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: could not convert string to float: \"Time to initiate the death penalty and remove some of the evil permanently from society. You can't rehabilitate pure evil. Some of the garbage are not in any way, shape, or form fit to live amongst law abiding citizens (even in a cage). And we shouldn't have to wait 30 years for the appeal process. What a joke.\""]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
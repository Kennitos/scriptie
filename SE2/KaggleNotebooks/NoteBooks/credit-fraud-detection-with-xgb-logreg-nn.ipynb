{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Imported Libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\n\n# Other Libraries\nfrom imblearn.datasets import fetch_datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndf = pd.read_csv('../input/creditcard.csv')\ndf.head()","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"   Time        V1        V2        V3  ...         V27       V28  Amount  Class\n0   0.0 -1.359807 -0.072781  2.536347  ...    0.133558 -0.021053  149.62      0\n1   0.0  1.191857  0.266151  0.166480  ...   -0.008983  0.014724    2.69      0\n2   1.0 -1.358354 -1.340163  1.773209  ...   -0.055353 -0.059752  378.66      0\n3   1.0 -0.966272 -0.185226  1.792993  ...    0.062723  0.061458  123.50      0\n4   2.0 -1.158233  0.877737  1.548718  ...    0.219422  0.215153   69.99      0\n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>V11</th>\n      <th>V12</th>\n      <th>V13</th>\n      <th>V14</th>\n      <th>V15</th>\n      <th>V16</th>\n      <th>V17</th>\n      <th>V18</th>\n      <th>V19</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>0.090794</td>\n      <td>-0.551600</td>\n      <td>-0.617801</td>\n      <td>-0.991390</td>\n      <td>-0.311169</td>\n      <td>1.468177</td>\n      <td>-0.470401</td>\n      <td>0.207971</td>\n      <td>0.025791</td>\n      <td>0.403993</td>\n      <td>0.251412</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>-0.166974</td>\n      <td>1.612727</td>\n      <td>1.065235</td>\n      <td>0.489095</td>\n      <td>-0.143772</td>\n      <td>0.635558</td>\n      <td>0.463917</td>\n      <td>-0.114805</td>\n      <td>-0.183361</td>\n      <td>-0.145783</td>\n      <td>-0.069083</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>0.207643</td>\n      <td>0.624501</td>\n      <td>0.066084</td>\n      <td>0.717293</td>\n      <td>-0.165946</td>\n      <td>2.345865</td>\n      <td>-2.890083</td>\n      <td>1.109969</td>\n      <td>-0.121359</td>\n      <td>-2.261857</td>\n      <td>0.524980</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>-0.054952</td>\n      <td>-0.226487</td>\n      <td>0.178228</td>\n      <td>0.507757</td>\n      <td>-0.287924</td>\n      <td>-0.631418</td>\n      <td>-1.059647</td>\n      <td>-0.684093</td>\n      <td>1.965775</td>\n      <td>-1.232622</td>\n      <td>-0.208038</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>0.753074</td>\n      <td>-0.822843</td>\n      <td>0.538196</td>\n      <td>1.345852</td>\n      <td>-1.119670</td>\n      <td>0.175121</td>\n      <td>-0.451449</td>\n      <td>-0.237033</td>\n      <td>-0.038195</td>\n      <td>0.803487</td>\n      <td>0.408542</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_summary(df_fa):\n    print('DataFrame shape')\n    print('rows:',df_fa.shape[0])\n    print('cols:',df_fa.shape[1])\n    col_list=['Null','Unique_Count','Data_type','Max/Min','Mean','Std','Skewness','Sample_values']\n    df=pd.DataFrame(index=df_fa.columns,columns=col_list)\n    df['Null']=list([len(df_fa[col][df_fa[col].isnull()]) for i,col in enumerate(df_fa.columns)])\n    #df['%_Null']=list([len(df_fa[col][df_fa[col].isnull()])/df_fa.shape[0]*100 for i,col in enumerate(df_fa.columns)])\n    df['Unique_Count']=list([len(df_fa[col].unique()) for i,col in enumerate(df_fa.columns)])\n    df['Data_type']=list([df_fa[col].dtype for i,col in enumerate(df_fa.columns)])\n    for i,col in enumerate(df_fa.columns):\n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.at[col,'Max/Min']=str(round(df_fa[col].max(),2))+'/'+str(round(df_fa[col].min(),2))\n            df.at[col,'Mean']=df_fa[col].mean()\n            df.at[col,'Std']=df_fa[col].std()\n            df.at[col,'Skewness']=df_fa[col].skew()\n        df.at[col,'Sample_values']=list(df_fa[col].unique())\n           \n    return(df.fillna('-'))","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### data"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_summary(df)","execution_count":3,"outputs":[{"output_type":"stream","text":"DataFrame shape\nrows: 284807\ncols: 31\n","name":"stdout"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"        Null                        ...                                                              Sample_values\nTime       0                        ...                          [0.0, 1.0, 2.0, 4.0, 7.0, 9.0, 10.0, 11.0, 12....\nV1         0                        ...                          [-1.3598071336738, 1.1918571113148602, -1.3583...\nV2         0                        ...                          [-0.0727811733098497, 0.26615071205963, -1.340...\nV3         0                        ...                          [2.53634673796914, 0.16648011335321, 1.7732093...\nV4         0                        ...                          [1.37815522427443, 0.448154078460911, 0.379779...\nV5         0                        ...                          [-0.33832076994251803, 0.0600176492822243, -0....\nV6         0                        ...                          [0.462387777762292, -0.0823608088155687, 1.800...\nV7         0                        ...                          [0.239598554061257, -0.0788029833323113, 0.791...\nV8         0                        ...                          [0.0986979012610507, 0.0851016549148104, 0.247...\nV9         0                        ...                          [0.363786969611213, -0.255425128109186, -1.514...\nV10        0                        ...                          [0.0907941719789316, -0.16697441400461402, 0.2...\nV11        0                        ...                          [-0.551599533260813, 1.6127266610547901, 0.624...\nV12        0                        ...                          [-0.617800855762348, 1.06523531137287, 0.06608...\nV13        0                        ...                          [-0.991389847235408, 0.48909501589608, 0.71729...\nV14        0                        ...                          [-0.31116935369987897, -0.143772296441519, -0....\nV15        0                        ...                          [1.46817697209427, 0.635558093258208, 2.345864...\nV16        0                        ...                          [-0.47040052525947795, 0.463917041022171, -2.8...\nV17        0                        ...                          [0.20797124192924202, -0.114804663102346, 1.10...\nV18        0                        ...                          [0.0257905801985591, -0.18336127012399397, -0....\nV19        0                        ...                          [0.403992960255733, -0.14578304132525902, -2.2...\nV20        0                        ...                          [0.251412098239705, -0.0690831352230203, 0.524...\nV21        0                        ...                          [-0.018306777944153, -0.225775248033138, 0.247...\nV22        0                        ...                          [0.277837575558899, -0.6386719527718511, 0.771...\nV23        0                        ...                          [-0.110473910188767, 0.10128802125323402, 0.90...\nV24        0                        ...                          [0.0669280749146731, -0.33984647552912706, -0....\nV25        0                        ...                          [0.12853935827352803, 0.167170404418143, -0.32...\nV26        0                        ...                          [-0.189114843888824, 0.125894532368176, -0.139...\nV27        0                        ...                          [0.13355837674038698, -0.00898309914322813, -0...\nV28        0                        ...                          [-0.0210530534538215, 0.0147241691924927, -0.0...\nAmount     0                        ...                          [149.62, 2.69, 378.66, 123.5, 69.99, 3.67, 4.9...\nClass      0                        ...                                                                     [0, 1]\n\n[31 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Null</th>\n      <th>Unique_Count</th>\n      <th>Data_type</th>\n      <th>Max/Min</th>\n      <th>Mean</th>\n      <th>Std</th>\n      <th>Skewness</th>\n      <th>Sample_values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Time</th>\n      <td>0</td>\n      <td>124592</td>\n      <td>float64</td>\n      <td>172792.0/0.0</td>\n      <td>9.481386e+04</td>\n      <td>47488.145955</td>\n      <td>-0.035568</td>\n      <td>[0.0, 1.0, 2.0, 4.0, 7.0, 9.0, 10.0, 11.0, 12....</td>\n    </tr>\n    <tr>\n      <th>V1</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>2.45/-56.41</td>\n      <td>3.919560e-15</td>\n      <td>1.958696</td>\n      <td>-3.280667</td>\n      <td>[-1.3598071336738, 1.1918571113148602, -1.3583...</td>\n    </tr>\n    <tr>\n      <th>V2</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>22.06/-72.72</td>\n      <td>5.688174e-16</td>\n      <td>1.651309</td>\n      <td>-4.624866</td>\n      <td>[-0.0727811733098497, 0.26615071205963, -1.340...</td>\n    </tr>\n    <tr>\n      <th>V3</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>9.38/-48.33</td>\n      <td>-8.769071e-15</td>\n      <td>1.516255</td>\n      <td>-2.240155</td>\n      <td>[2.53634673796914, 0.16648011335321, 1.7732093...</td>\n    </tr>\n    <tr>\n      <th>V4</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>16.88/-5.68</td>\n      <td>2.782312e-15</td>\n      <td>1.415869</td>\n      <td>0.676292</td>\n      <td>[1.37815522427443, 0.448154078460911, 0.379779...</td>\n    </tr>\n    <tr>\n      <th>V5</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>34.8/-113.74</td>\n      <td>-1.552563e-15</td>\n      <td>1.380247</td>\n      <td>-2.425901</td>\n      <td>[-0.33832076994251803, 0.0600176492822243, -0....</td>\n    </tr>\n    <tr>\n      <th>V6</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>73.3/-26.16</td>\n      <td>2.010663e-15</td>\n      <td>1.332271</td>\n      <td>1.826581</td>\n      <td>[0.462387777762292, -0.0823608088155687, 1.800...</td>\n    </tr>\n    <tr>\n      <th>V7</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>120.59/-43.56</td>\n      <td>-1.694249e-15</td>\n      <td>1.237094</td>\n      <td>2.553907</td>\n      <td>[0.239598554061257, -0.0788029833323113, 0.791...</td>\n    </tr>\n    <tr>\n      <th>V8</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>20.01/-73.22</td>\n      <td>-1.927028e-16</td>\n      <td>1.194353</td>\n      <td>-8.521944</td>\n      <td>[0.0986979012610507, 0.0851016549148104, 0.247...</td>\n    </tr>\n    <tr>\n      <th>V9</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>15.59/-13.43</td>\n      <td>-3.137024e-15</td>\n      <td>1.098632</td>\n      <td>0.554680</td>\n      <td>[0.363786969611213, -0.255425128109186, -1.514...</td>\n    </tr>\n    <tr>\n      <th>V10</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>23.75/-24.59</td>\n      <td>1.768627e-15</td>\n      <td>1.088850</td>\n      <td>1.187141</td>\n      <td>[0.0907941719789316, -0.16697441400461402, 0.2...</td>\n    </tr>\n    <tr>\n      <th>V11</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>12.02/-4.8</td>\n      <td>9.170318e-16</td>\n      <td>1.020713</td>\n      <td>0.356506</td>\n      <td>[-0.551599533260813, 1.6127266610547901, 0.624...</td>\n    </tr>\n    <tr>\n      <th>V12</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>7.85/-18.68</td>\n      <td>-1.810658e-15</td>\n      <td>0.999201</td>\n      <td>-2.278401</td>\n      <td>[-0.617800855762348, 1.06523531137287, 0.06608...</td>\n    </tr>\n    <tr>\n      <th>V13</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>7.13/-5.79</td>\n      <td>1.693438e-15</td>\n      <td>0.995274</td>\n      <td>0.065233</td>\n      <td>[-0.991389847235408, 0.48909501589608, 0.71729...</td>\n    </tr>\n    <tr>\n      <th>V14</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>10.53/-19.21</td>\n      <td>1.479045e-15</td>\n      <td>0.958596</td>\n      <td>-1.995176</td>\n      <td>[-0.31116935369987897, -0.143772296441519, -0....</td>\n    </tr>\n    <tr>\n      <th>V15</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>8.88/-4.5</td>\n      <td>3.482336e-15</td>\n      <td>0.915316</td>\n      <td>-0.308423</td>\n      <td>[1.46817697209427, 0.635558093258208, 2.345864...</td>\n    </tr>\n    <tr>\n      <th>V16</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>17.32/-14.13</td>\n      <td>1.392007e-15</td>\n      <td>0.876253</td>\n      <td>-1.100966</td>\n      <td>[-0.47040052525947795, 0.463917041022171, -2.8...</td>\n    </tr>\n    <tr>\n      <th>V17</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>9.25/-25.16</td>\n      <td>-7.528491e-16</td>\n      <td>0.849337</td>\n      <td>-3.844914</td>\n      <td>[0.20797124192924202, -0.114804663102346, 1.10...</td>\n    </tr>\n    <tr>\n      <th>V18</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>5.04/-9.5</td>\n      <td>4.328772e-16</td>\n      <td>0.838176</td>\n      <td>-0.259880</td>\n      <td>[0.0257905801985591, -0.18336127012399397, -0....</td>\n    </tr>\n    <tr>\n      <th>V19</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>5.59/-7.21</td>\n      <td>9.049732e-16</td>\n      <td>0.814041</td>\n      <td>0.109192</td>\n      <td>[0.403992960255733, -0.14578304132525902, -2.2...</td>\n    </tr>\n    <tr>\n      <th>V20</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>39.42/-54.5</td>\n      <td>5.085503e-16</td>\n      <td>0.770925</td>\n      <td>-2.037155</td>\n      <td>[0.251412098239705, -0.0690831352230203, 0.524...</td>\n    </tr>\n    <tr>\n      <th>V21</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>27.2/-34.83</td>\n      <td>1.537294e-16</td>\n      <td>0.734524</td>\n      <td>3.592991</td>\n      <td>[-0.018306777944153, -0.225775248033138, 0.247...</td>\n    </tr>\n    <tr>\n      <th>V22</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>10.5/-10.93</td>\n      <td>7.959909e-16</td>\n      <td>0.725702</td>\n      <td>-0.213258</td>\n      <td>[0.277837575558899, -0.6386719527718511, 0.771...</td>\n    </tr>\n    <tr>\n      <th>V23</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>22.53/-44.81</td>\n      <td>5.367590e-16</td>\n      <td>0.624460</td>\n      <td>-5.875140</td>\n      <td>[-0.110473910188767, 0.10128802125323402, 0.90...</td>\n    </tr>\n    <tr>\n      <th>V24</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>4.58/-2.84</td>\n      <td>4.458112e-15</td>\n      <td>0.605647</td>\n      <td>-0.552499</td>\n      <td>[0.0669280749146731, -0.33984647552912706, -0....</td>\n    </tr>\n    <tr>\n      <th>V25</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>7.52/-10.3</td>\n      <td>1.453003e-15</td>\n      <td>0.521278</td>\n      <td>-0.415793</td>\n      <td>[0.12853935827352803, 0.167170404418143, -0.32...</td>\n    </tr>\n    <tr>\n      <th>V26</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>3.52/-2.6</td>\n      <td>1.699104e-15</td>\n      <td>0.482227</td>\n      <td>0.576693</td>\n      <td>[-0.189114843888824, 0.125894532368176, -0.139...</td>\n    </tr>\n    <tr>\n      <th>V27</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>31.61/-22.57</td>\n      <td>-3.660161e-16</td>\n      <td>0.403632</td>\n      <td>-1.170209</td>\n      <td>[0.13355837674038698, -0.00898309914322813, -0...</td>\n    </tr>\n    <tr>\n      <th>V28</th>\n      <td>0</td>\n      <td>275663</td>\n      <td>float64</td>\n      <td>33.85/-15.43</td>\n      <td>-1.206049e-16</td>\n      <td>0.330083</td>\n      <td>11.192091</td>\n      <td>[-0.0210530534538215, 0.0147241691924927, -0.0...</td>\n    </tr>\n    <tr>\n      <th>Amount</th>\n      <td>0</td>\n      <td>32767</td>\n      <td>float64</td>\n      <td>25691.16/0.0</td>\n      <td>8.834962e+01</td>\n      <td>250.120109</td>\n      <td>16.977724</td>\n      <td>[149.62, 2.69, 378.66, 123.5, 69.99, 3.67, 4.9...</td>\n    </tr>\n    <tr>\n      <th>Class</th>\n      <td>0</td>\n      <td>2</td>\n      <td>int64</td>\n      <td>1/0</td>\n      <td>1.727486e-03</td>\n      <td>0.041527</td>\n      <td>23.997579</td>\n      <td>[0, 1]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n# RobustScaler is less prone to outliers.\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import StratifiedKFold\n\nprint('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Check the Distribution of the labels","execution_count":5,"outputs":[{"output_type":"stream","text":"No Frauds 99.83 % of the dataset\nFrauds 0.17 % of the dataset\nTrain: [ 30473  30496  31002 ... 284804 284805 284806] Test: [    0     1     2 ... 57017 57018 57019]\nTrain: [     0      1      2 ... 284804 284805 284806] Test: [ 30473  30496  31002 ... 113964 113965 113966]\nTrain: [     0      1      2 ... 284804 284805 284806] Test: [ 81609  82400  83053 ... 170946 170947 170948]\nTrain: [     0      1      2 ... 284804 284805 284806] Test: [150654 150660 150661 ... 227866 227867 227868]\nTrain: [     0      1      2 ... 227866 227867 227868] Test: [212516 212644 213092 ... 284804 284805 284806]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turn into an array\noriginal_Xtrain = X_train.values\noriginal_Xtest = X_test.values\noriginal_ytrain = y_train.values\noriginal_ytest = y_test.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label/ len(original_ytrain))\nprint(test_counts_label/ len(original_ytest))","execution_count":6,"outputs":[{"output_type":"stream","text":"----------------------------------------------------------------------------------------------------\nLabel Distributions: \n\n[0.99827076 0.00172924]\n[0.99827952 0.00172048]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n\nprint('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n\n# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# Classifier with optimal parameters\n# log_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm = LogisticRegression()\n\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)","execution_count":7,"outputs":[{"output_type":"stream","text":"Length of X (train): 227846 | Length of y (train): 227846\nLength of X (test): 56961 | Length of y (test): 56961\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### EDA and other feature engineering skipped in this notebook"},{"metadata":{},"cell_type":"markdown","source":"### SMOTE oversampling"},{"metadata":{},"cell_type":"markdown","source":"#### SMOTE:\n\n1. Solving the Class Imbalance: SMOTE creates synthetic points from the minority class in order to reach an equal balance between the minority and majority class.\n\n2. Location of the synthetic points: SMOTE picks the distance between the closest neighbors of the minority class, in between these distances it creates synthetic points.\n\n3. Final Effect: More information is retained since we didn't have to delete any rows unlike in random undersampling.\n\n4. Accuracy || Time Tradeoff: Although it is likely that SMOTE will be more accurate than random under-sampling, it will take more time to train since no rows are eliminated as previously stated."},{"metadata":{},"cell_type":"markdown","source":"#### Note: we should do the cross validation before SMOTE oversampling"},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression with SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n\nprint('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n\n# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# Classifier with optimal parameters\n# log_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm = LogisticRegression()\n\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)","execution_count":8,"outputs":[{"output_type":"stream","text":"Length of X (train): 227846 | Length of y (train): 227846\nLength of X (test): 56961 | Length of y (test): 56961\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Implementing SMOTE Technique \n# Cross Validating the right way\n\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) \n    # SMOTE happens during Cross Validation not before..\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('---' * 35)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('AUC: {}'.format(np.mean(auc_lst)))\nprint('---' * 35)","execution_count":9,"outputs":[{"output_type":"stream","text":"---------------------------------------------------------------------------------------------------------\n\naccuracy: 0.9617771496324765\nprecision: 0.06319737177574562\nrecall: 0.9136319376825707\nf1: 0.11670986563084436\nAUC: 0.9377461336251234\n---------------------------------------------------------------------------------------------------------\nCPU times: user 10min 36s, sys: 15.8 s, total: 10min 51s\nWall time: 10min 43s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['No Fraud', 'Fraud']\nsmote_prediction = best_est.predict(original_Xtest)\nprint(classification_report(original_ytest, smote_prediction, target_names=labels))","execution_count":10,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n    No Fraud       1.00      0.99      0.99     56863\n       Fraud       0.11      0.86      0.20        98\n\n   micro avg       0.99      0.99      0.99     56961\n   macro avg       0.56      0.92      0.60     56961\nweighted avg       1.00      0.99      0.99     56961\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### XGB with SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBClassifier","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### without SMOTE to see the optimal paramaters"},{"metadata":{},"cell_type":"markdown","source":"#### GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# GridSearchCV\n# from sklearn.model_selection import GridSearchCV\n\n# xgb_params = {\"eta\": [0.02,0.05,0.1], \n#              'min_child_weight': [50,100,200,300,360],\n#              'subsample':[0.5,0.7,0.8,0.9],\n#              \"colsample_bytree\":[0.3,0.5,0.7,0.9],\n#              \"max_depth\":[8,10,12,15],\n#              'objective':['binary:logistic',\"reg:linear\"]\n#             }\n\n#xgb_sm = XGBClassifier()\n#grid_xgb = GridSearchCV(xgb_sm, xgb_params)\n#grid_xgb.fit(X_train, y_train)\n#get the best estimator\n#best_est_grid = grid_xgb.best_estimator_","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RandomizedSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# RandomizedSearchCV\n# fit model no training data\n\nxgb_params = {\"eta\": [0.01,0.02,0.03,0.05,0.8,0.1], \n              'min_child_weight': [50,100,200,300,360,500],\n              'subsample':[0.5,0.7,0.8,0.9,0.95,0.99],\n              \"colsample_bytree\":[0.3,0.5,0.7,0.9,0.99],\n              \"max_depth\":[6,8,10,12]\n             }\n\nxgb_sm = XGBClassifier()\nrand_xgb = RandomizedSearchCV(xgb_sm, xgb_params, n_iter=6)\nrand_xgb.fit(X_train,y_train)\nbest_est_ran = rand_xgb.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using SMOTE with optimal parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=0.5, eta=0.8, gamma=0, learning_rate=0.1,\n       max_delta_step=0, max_depth=12, min_child_weight=50, missing=None,\n       n_estimators=100, n_jobs=1, nthread=None,\n       objective='binary:logistic', random_state=0, reg_alpha=0,\n       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n       subsample=0.7)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Implementing SMOTE Technique \n# Cross Validating the right way\n\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), best_xgb) \n    # SMOTE happens during Cross Validation not before..\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    prediction = best_xgb.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('---' * 35)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('AUC: {}'.format(np.mean(auc_lst)))\nprint('---' * 35)","execution_count":15,"outputs":[{"output_type":"stream","text":"---------------------------------------------------------------------------------------------------------\n\naccuracy: 0.9732519130855734\nprecision: 0.1977601946427154\nrecall: 0.8919993508601104\nf1: 0.25611878938844557\nAUC: 0.9326958526031068\n---------------------------------------------------------------------------------------------------------\nCPU times: user 10min 38s, sys: 1.78 s, total: 10min 39s\nWall time: 10min 39s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['No Fraud', 'Fraud']\nsmote_prediction = best_xgb.predict(original_Xtest)\nprint(classification_report(original_ytest, smote_prediction, target_names=labels))","execution_count":17,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n    No Fraud       1.00      1.00      1.00     56863\n       Fraud       0.90      0.73      0.81        98\n\n   micro avg       1.00      1.00      1.00     56961\n   macro avg       0.95      0.87      0.90     56961\nweighted avg       1.00      1.00      1.00     56961\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Keras with one hidden layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\n\nn_inputs = X_train.shape[1]\n\noversample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')])\n\n# SMOTE Technique (OverSampling) After splitting and Cross Validating\nsm = SMOTE(ratio='minority', random_state=42)\n# Xsm_train, ysm_train = sm.fit_sample(X_train, y_train)\n\n\n# This will be the data were we are going to \nXsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)","execution_count":16,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"oversample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oversample_model.fit(Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=2)","execution_count":19,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 363923 samples, validate on 90981 samples\nEpoch 1/20\n - 5s - loss: 0.0682 - acc: 0.9759 - val_loss: 0.0270 - val_acc: 0.9928\nEpoch 2/20\n - 3s - loss: 0.0146 - acc: 0.9963 - val_loss: 0.0079 - val_acc: 0.9996\nEpoch 3/20\n - 3s - loss: 0.0075 - acc: 0.9985 - val_loss: 0.0069 - val_acc: 0.9997\nEpoch 4/20\n - 3s - loss: 0.0052 - acc: 0.9990 - val_loss: 0.0015 - val_acc: 1.0000\nEpoch 5/20\n - 3s - loss: 0.0041 - acc: 0.9992 - val_loss: 0.0045 - val_acc: 1.0000\nEpoch 6/20\n - 3s - loss: 0.0033 - acc: 0.9994 - val_loss: 0.0024 - val_acc: 1.0000\nEpoch 7/20\n - 3s - loss: 0.0028 - acc: 0.9995 - val_loss: 8.0757e-04 - val_acc: 1.0000\nEpoch 8/20\n - 3s - loss: 0.0025 - acc: 0.9995 - val_loss: 8.4267e-04 - val_acc: 0.9999\nEpoch 9/20\n - 3s - loss: 0.0021 - acc: 0.9996 - val_loss: 3.2615e-04 - val_acc: 1.0000\nEpoch 10/20\n - 3s - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0022 - val_acc: 1.0000\nEpoch 11/20\n - 3s - loss: 0.0019 - acc: 0.9996 - val_loss: 5.9087e-04 - val_acc: 1.0000\nEpoch 12/20\n - 4s - loss: 0.0014 - acc: 0.9997 - val_loss: 2.0993e-04 - val_acc: 1.0000\nEpoch 13/20\n - 4s - loss: 0.0016 - acc: 0.9997 - val_loss: 4.9885e-04 - val_acc: 1.0000\nEpoch 14/20\n - 4s - loss: 0.0016 - acc: 0.9997 - val_loss: 6.0029e-04 - val_acc: 1.0000\nEpoch 15/20\n - 3s - loss: 0.0011 - acc: 0.9998 - val_loss: 4.7593e-04 - val_acc: 1.0000\nEpoch 16/20\n - 3s - loss: 0.0011 - acc: 0.9998 - val_loss: 2.6700e-04 - val_acc: 1.0000\nEpoch 17/20\n - 3s - loss: 0.0014 - acc: 0.9997 - val_loss: 4.3405e-04 - val_acc: 1.0000\nEpoch 18/20\n - 3s - loss: 7.8169e-04 - acc: 0.9998 - val_loss: 7.4084e-04 - val_acc: 1.0000\nEpoch 19/20\n - 3s - loss: 0.0011 - acc: 0.9997 - val_loss: 3.2975e-04 - val_acc: 1.0000\nEpoch 20/20\n - 3s - loss: 8.8323e-04 - acc: 0.9998 - val_loss: 1.7259e-04 - val_acc: 1.0000\n","name":"stdout"},{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"<keras.callbacks.History at 0x7fd335a1b780>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"oversample_predictions = oversample_model.predict(original_Xtest, batch_size=200, verbose=0)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oversample_fraud_predictions = oversample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=14)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oversample_smote = confusion_matrix(original_ytest, oversample_fraud_predictions)\nactual_cm = confusion_matrix(original_ytest, original_ytest)\n\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(16,8))\n\nfig.add_subplot(221)\nplot_confusion_matrix(oversample_smote, labels, title=\"OverSample (SMOTE) \\n Confusion Matrix\", cmap=plt.cm.Oranges)\n\nfig.add_subplot(222)\nplot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)","execution_count":28,"outputs":[{"output_type":"stream","text":"Confusion matrix, without normalization\n[[56851    12]\n [   34    64]]\nConfusion matrix, without normalization\n[[56863     0]\n [    0    98]]\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1152x576 with 4 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
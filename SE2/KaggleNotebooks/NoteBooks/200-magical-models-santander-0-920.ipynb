{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The \"Magic\" of Santander\n",
    "In this kernel, we will display pictures of the Santander magic! Previously [here][1], in \"Modified Naive Bayes\", we saw that we can model each variable separately and then combine the 200 models to score LB 0.899. We will do the same here after adding a \"magic\" feature to each variable. We will then ensemble the 200 models with logistic regression and score LB 0.920  \n",
    "  \n",
    "![image](http://playagricola.com/Kaggle/magic41019.jpg)\n",
    "  \n",
    "[1]: https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The \"Magic\" Feature\n",
    "When LGBM \"looks\" at the histogram for `Var_198`, it \"sees\" that when `var_198<13` the probability of having `target=1` is high. And when `var_198>13` the probability is low. This can be displayed by showing the predictions made by LGBM (when building a model from only the variable `var_198`). LGBM basically predicts `target=0.18` for `var_198<13` and `target=0.10` otherwise. \n",
    "  \n",
    "![image](http://playagricola.com/Kaggle/198without.png)  \n",
    "  \n",
    "LGBM \"divides\" the histogram with **vertical lines** because LGBM does not see **horizontal** differences. A histogram places multiple values into a single bin and produces a smooth picture. If you place every value in its own bin, you will have a jagged picture, where bars change heights from value to value. Some values are unique, some values occur dozens of times (and in the case of `var_108`, some values occur over 300 times!!) Below is a histogram with one value per bin and we zoom in on `11.0000<x<11.1000`. We see that value `11.0712` occurs 5 times and its close neighbor `11.0720` occurs only once.  \n",
    "    \n",
    "![image](http://playagricola.com/Kaggle/198zoom3.png)   \n",
    "  \n",
    "These counts are the \"magic\" feature. For each variable, we make a new feature (column) whose value is the number of counts of the corresponding variable. An example of this new column is displayed above next to the histogram. When LGBM has this new feature, it can now \"divide\" the histogram with **horizontal lines** in addition to vertical.  \n",
    "  \n",
    "![image](http://playagricola.com/Kaggle/198with.png)  \n",
    "  \n",
    "Notice now that LGBM predicts `target=0.1` when `var_198<13` AND `count=1`. When  `var_198<13` AND `count>1`, it predicts `target=0.36`. This improvement (using the magic) causes validation AUC to become 0.551 as opposed to 0.547 when using `var_198` alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is the Magic difficult to find?\n",
    "The \"magic\" is difficult to find because the new feature `Var_198_FE` interacts with `Var_198`. Therefore if you add the new feature to an LGBM with `feature_fraction=0.05`, you will not increase your CV or LB. You must set `feature_fraction=1.0`. Then you will gain the benefit from the new feature but you also have the determental effect of modeling spurious original variable interactions. None-the-less, adding the new feature and using `feature_fraction=1.0` achieves CV 0.910. To reach 0.920, we must remove the spurious effects from the original variable interactions.  \n",
    "  \n",
    "UPDATE: I just discovered another reason why magic is hidden. When calculating frequency counts for the test data, you must remove the **fake** test data before counting. (Fake test described [here][1]) If you don't, then your test predictions will score LB 0.900 instead of LB 0.910 and you may disregard frequency counts as useless.\n",
    "\n",
    "# Maximizing Magic Feature\n",
    "To maximize the gain of the \"magic\" feature (and climb from LB 0.910 to 0.920), we must allow the new feature to interact with the original variables while preventing the original variables from interacting with each other. Here are 3 ways to do that:\n",
    "* Use Data Augmentation (as shown in Jiwei's awesome kernel [here][2]). You must keep original and new feature in same row.\n",
    "* Use 200 separate models as shown in this kernel below.\n",
    "* Merge new feature and original feature into one feature. In original data, simply add 200 to each unique value. (And don't add new columns)\n",
    "\n",
    "# Let's Begin\n",
    "When counting the occurence of each value, we will merge the training data and **real** test data first, and count everything together. In YaG320's brilliant kernel [here][1], we learned that half the test data is fake.  \n",
    "  \n",
    "[1]: https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split\n",
    "[2]: https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# LOAD LIBRARIES\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd, numpy as np, gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# GET INDICIES OF REAL TEST DATA FOR FE\n",
    "#######################\n",
    "# TAKE FROM YAG320'S KERNEL\n",
    "# https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split\n",
    "\n",
    "test_path = '../input/test.csv'\n",
    "\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_test.drop(['ID_code'], axis=1, inplace=True)\n",
    "df_test = df_test.values\n",
    "\n",
    "unique_samples = []\n",
    "unique_count = np.zeros_like(df_test)\n",
    "for feature in range(df_test.shape[1]):\n",
    "    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n",
    "    unique_count[index_[count_ == 1], feature] += 1\n",
    "\n",
    "# Samples which have unique values are real the others are fake\n",
    "real_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n",
    "synthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n",
    "\n",
    "print('Found',len(real_samples_indexes),'real test')\n",
    "print('Found',len(synthetic_samples_indexes),'fake test')\n",
    "\n",
    "###################\n",
    "\n",
    "d = {}\n",
    "for i in range(200): d['var_'+str(i)] = 'float32'\n",
    "d['target'] = 'uint8'\n",
    "d['ID_code'] = 'object'\n",
    "\n",
    "train = pd.read_csv('../input/train.csv', dtype=d)\n",
    "test = pd.read_csv('../input/test.csv', dtype=d)\n",
    "\n",
    "print('Loaded',len(train),'rows of train')\n",
    "print('Loaded',len(test),'rows of test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUENCY ENCODE\n",
    "def encode_FE(df,col,test):\n",
    "    cv = df[col].value_counts()\n",
    "    nm = col+'_FE'\n",
    "    df[nm] = df[col].map(cv)\n",
    "    test[nm] = test[col].map(cv)\n",
    "    test[nm].fillna(0,inplace=True)\n",
    "    if cv.max()<=255:\n",
    "        df[nm] = df[nm].astype('uint8')\n",
    "        test[nm] = test[nm].astype('uint8')\n",
    "    else:\n",
    "        df[nm] = df[nm].astype('uint16')\n",
    "        test[nm] = test[nm].astype('uint16')        \n",
    "    return\n",
    "\n",
    "test['target'] = -1\n",
    "comb = pd.concat([train,test.loc[real_samples_indexes]],axis=0,sort=True)\n",
    "for i in range(200): encode_FE(comb,'var_'+str(i),test)\n",
    "train = comb[:len(train)]; del comb\n",
    "print('Added 200 new magic features!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "# LGBM PARAMETERS\n",
    "param = {\n",
    "    'learning_rate': 0.04,\n",
    "    'num_leaves': 3,\n",
    "    'metric':'auc',\n",
    "    'boost_from_average':'false',\n",
    "    'feature_fraction': 1.0,\n",
    "    'max_depth': -1,\n",
    "    'objective': 'binary',\n",
    "    'verbosity': -10}\n",
    "train2 = train.sample(frac=1,random_state=42)\n",
    "evals_result = {}\n",
    "num_vars = 200\n",
    "\n",
    "# SAVE OUT-OF-FOLD PREDICTIONS\n",
    "all_oof = np.zeros((len(train2),num_vars+1))\n",
    "all_oof[:,0] = np.ones(len(train2))\n",
    "all_oofB = np.zeros((len(train2),num_vars+1))\n",
    "all_oofB[:,0] = np.ones(len(train2))\n",
    "\n",
    "# SAVE TEST PREDICTIONS\n",
    "all_preds = np.zeros((len(test),num_vars+1))\n",
    "all_preds[:,0] = np.ones(len(test))\n",
    "all_predsB = np.zeros((len(test),num_vars+1))\n",
    "all_predsB[:,0] = np.ones(len(test))\n",
    "\n",
    "for j in range(num_vars):\n",
    "    \n",
    "    # MODEL WITH MAGIC\n",
    "    features = ['var_'+str(j),'var_'+str(j)+'_FE']\n",
    "    oof = np.zeros(len(train2))\n",
    "    preds = np.zeros(len(test))\n",
    "    \n",
    "    # PLOT DENSITIES    \n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.distplot(train2[train2['target']==0]['var_'+str(j)], label = 't=0')\n",
    "    sns.distplot(train2[train2['target']==1]['var_'+str(j)], label = 't=1')\n",
    "    plt.legend()\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('Var_'+str(j))\n",
    "\n",
    "    # MAKE A GRID OF POINTS FOR LGBM TO PREDICT    \n",
    "    mn,mx = plt.xlim()\n",
    "    mnFE = train2['var_'+str(j)+'_FE'].min()\n",
    "    mxFE = train2['var_'+str(j)+'_FE'].max()\n",
    "    step = 50\n",
    "    stepB = train2['var_'+str(j)+'_FE'].nunique()\n",
    "    w = (mx-mn)/step\n",
    "    x = w * (np.arange(0,step)+0.5) + mn\n",
    "    x2 = np.array([])\n",
    "    for i in range(stepB):\n",
    "        x2 = np.concatenate([x,x2])\n",
    "    df = pd.DataFrame({'var_'+str(j):x2})\n",
    "    df['var_'+str(j)+'_FE'] = mnFE + (mxFE-mnFE)/(stepB-1) * (df.index//step)\n",
    "    df['pred'] = 0\n",
    "    \n",
    "    # 5-FOLD WITH MAGIC\n",
    "    for k in range(5):\n",
    "            valid = train2.iloc[k*40000:(k+1)*40000]\n",
    "            train = train2[ ~train2.index.isin(valid.index) ]    \n",
    "            trn_data  = lgb.Dataset(train[features], label=train['target'])\n",
    "            val_data = lgb.Dataset(valid[features], label=valid['target'])     \n",
    "            model = lgb.train(param, trn_data, 750, valid_sets = [trn_data, val_data], \n",
    "                    verbose_eval=False, evals_result=evals_result)      \n",
    "            x = evals_result['valid_1']['auc']\n",
    "            best = x.index(max(x))\n",
    "            #print('i=',i,'k=',k,'best=',best)\n",
    "            oof[k*40000:(k+1)*40000] = model.predict(valid[features], num_iteration=best)\n",
    "            preds += model.predict(test[features], num_iteration=best)/5.0\n",
    "            df['pred'] += model.predict(df[features], num_iteration=best)/5.0\n",
    "            \n",
    "    val_auc = roc_auc_score(train2['target'],oof)\n",
    "    print('VAR_'+str(j)+' with magic val_auc =',round(val_auc,5))\n",
    "    all_oof[:,j+1] = oof\n",
    "    all_preds[:,j+1] = preds\n",
    "    x = df['pred'].values\n",
    "    x = np.reshape(x,(stepB,step))\n",
    "    x = np.flip(x,axis=0)\n",
    "    \n",
    "    # PLOT LGBM PREDICTIONS USING MAGIC    \n",
    "    plt.subplot(1,2,1)\n",
    "    sns.heatmap(x, cmap='RdBu_r', center=0.0) \n",
    "    plt.title('VAR_'+str(j)+' Predictions with Magic',fontsize=16)    \n",
    "    plt.xticks(np.linspace(0,49,5),np.round(np.linspace(mn,mx,5),1))\n",
    "    plt.xlabel('Var_'+str(j))\n",
    "    s = min(mxFE-mnFE+1,20)\n",
    "    plt.yticks(np.linspace(mnFE,mxFE,s)-0.5,np.linspace(mxFE,mnFE,s).astype('int'))\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "    # MODEL WITHOUT MAGIC\n",
    "    features = ['var_'+str(j)]\n",
    "    oof = np.zeros(len(train2))\n",
    "    preds = np.zeros(len(test))\n",
    "    \n",
    "    # PLOT DENSITIES\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.distplot(train2[train2['target']==0]['var_'+str(j)], label = 't=0')\n",
    "    sns.distplot(train2[train2['target']==1]['var_'+str(j)], label = 't=1')\n",
    "    plt.legend()\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('Var_'+str(j))\n",
    "    \n",
    "    # MAKE A GRID OF POINTS FOR LGBM TO PREDICT\n",
    "    mn,mx = plt.xlim()\n",
    "    mnFE = train2['var_'+str(j)+'_FE'].min()\n",
    "    mxFE = train2['var_'+str(j)+'_FE'].max()\n",
    "    step = 50\n",
    "    stepB = train2['var_'+str(j)+'_FE'].nunique()\n",
    "    w = (mx-mn)/step\n",
    "    x = w * (np.arange(0,step)+0.5) + mn\n",
    "    x2 = np.array([])\n",
    "    for i in range(stepB):\n",
    "        x2 = np.concatenate([x,x2])\n",
    "    df = pd.DataFrame({'var_'+str(j):x2})\n",
    "    df['var_'+str(j)+'_FE'] = mnFE + (mxFE-mnFE)/(stepB-1) * (df.index//step)\n",
    "    df['pred'] = 0\n",
    "    \n",
    "    # 5-FOLD WITHOUT MAGIC\n",
    "    for k in range(5):\n",
    "            valid = train2.iloc[k*40000:(k+1)*40000]\n",
    "            train = train2[ ~train2.index.isin(valid.index) ]\n",
    "            trn_data  = lgb.Dataset(train[features], label=train['target'])\n",
    "            val_data = lgb.Dataset(valid[features], label=valid['target'])     \n",
    "            model = lgb.train(param, trn_data, 750, valid_sets = [trn_data, val_data], \n",
    "                    verbose_eval=False, evals_result=evals_result)      \n",
    "            x = evals_result['valid_1']['auc']\n",
    "            best = x.index(max(x))\n",
    "            #print('i=',i,'k=',k,'best=',best)\n",
    "            oof[k*40000:(k+1)*40000] = model.predict(valid[features], num_iteration=best)\n",
    "            preds += model.predict(test[features], num_iteration=best)/5.0\n",
    "            df['pred'] += model.predict(df[features], num_iteration=best)/5.0\n",
    "            \n",
    "    val_auc = roc_auc_score(train2['target'],oof)\n",
    "    print('VAR_'+str(j)+' without magic val_auc =',round(val_auc,5))\n",
    "    all_oofB[:,j+1] = oof\n",
    "    all_predsB[:,j+1] = preds\n",
    "    x = df['pred'].values\n",
    "    x = np.reshape(x,(stepB,step))\n",
    "    x = np.flip(x,axis=0)\n",
    "    \n",
    "    # PLOT LGBM PREDICTIONS WITHOUT USING MAGIC\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.heatmap(x, cmap='RdBu_r', center=0.0) \n",
    "    plt.title('VAR_'+str(j)+' Predictions without Magic',fontsize=16)\n",
    "    plt.xticks(np.linspace(0,49,5),np.round(np.linspace(mn,mx,5),1))\n",
    "    plt.xlabel('Var_'+str(j))\n",
    "    plt.yticks([])\n",
    "    plt.ylabel('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble 200 Models with LR\n",
    "We now have a model for each variable and its predictions on test and its out-of-fold predictions on train. If we just add (or multiply) the predictions together, the AUC is low. Instead we will use logistic regression to ensemble them. Each set of predictions is a vector of length 200000. We have 200 vectors of out-of-fold predictions, call them `x1, x2, x3, ..., x200`. We know the true train target, call it `y`. We will now use logistic regression to find 200 coefficients (model y from x's). Then we will use those coefficients to combine our 200 test predictions to create a submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE MODEL WITHOUT MAGIC\n",
    "logrB = sm.Logit(train2['target'], all_oofB[:,:num_vars+1])\n",
    "logrB = logrB.fit(disp=0)\n",
    "ensemble_predsB = logrB.predict(all_oofB[:,:num_vars+1])\n",
    "ensemble_aucB = roc_auc_score(train2['target'],ensemble_predsB)  \n",
    "print('##################')\n",
    "print('Combined Model without magic Val_AUC=',round(ensemble_aucB,5))\n",
    "print()\n",
    "\n",
    "# ENSEMBLE MODEL WITH MAGIC\n",
    "logr = sm.Logit(train2['target'], all_oof[:,:num_vars+1])\n",
    "logr = logr.fit(disp=0)\n",
    "ensemble_preds = logr.predict(all_oof[:,:num_vars+1])\n",
    "ensemble_auc = roc_auc_score(train2['target'],ensemble_preds)  \n",
    "print('##################')\n",
    "print('Combined Model with magic Val_AUC=',round(ensemble_auc,5))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# SAVE PREDICTIONS TO CSV    \n",
    "print('Test predictions saved as submission.csv')\n",
    "print('OOF predictions saved as oof_submission.csv')\n",
    "print('Histogram of test predictions displayed below:')\n",
    "\n",
    "sub = train2[['ID_code','target']].copy()\n",
    "sub['predict'] = ensemble_preds\n",
    "sub.reset_index(inplace=True)\n",
    "sub.sort_values('index',inplace=True)\n",
    "sub.to_csv('oof_submission.csv',index=False)\n",
    "\n",
    "test_preds = logr.predict(all_preds[:,:num_vars+1])\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub['target'] = test_preds\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "\n",
    "# DISPLAY HISTOGRAM OF PREDICTIONS\n",
    "b = plt.hist(sub['target'], bins=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

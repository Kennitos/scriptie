{"cells":[{"metadata":{"_uuid":"6fdd1fc3e18e261cb936cc8f186e796021b2a218"},"cell_type":"markdown","source":"Hi everyone, here is the starter code for this competition. We will use xgboost and lightgbm and then create their out of fold predictions to use in a logistic regression stack.\nFirst let's load the required libraries"},{"metadata":{"_uuid":"447978a2af25d44f0f9a69c3217efde89b40a22c","_execution_state":"idle","trusted":true},"cell_type":"code","source":"list.files(path = \"../input\")\n\nlibrary(data.table)\nlibrary(caret)\nlibrary(xgboost)\nlibrary(lightgbm)\nlibrary(pROC)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1be7b7791cddba589c6555331c814c55ea2b391"},"cell_type":"markdown","source":"Now we will read the data sets"},{"metadata":{"trusted":true,"_uuid":"1ca10dd1c79acc3348008a43579b49d7b66746f2"},"cell_type":"code","source":"train = fread(\"../input/train.csv\")\ntest = fread(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35822e1de487c53de0ddd019e1bef628cb88e295"},"cell_type":"markdown","source":"Let's check the dimension of train and test sets. Also check what are the variables that are there in train but not in test. Also let's have a look at the head of the data sets"},{"metadata":{"trusted":true,"_uuid":"49111367e809b1c87aae3d923c2bb06848d467ba"},"cell_type":"code","source":"dim(train) ; dim(test) ; setdiff(colnames(train) , colnames(test)) ; head(train) ; head(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"918ce976d8d9cb66e0bae0cf56450e778a8281e8"},"cell_type":"markdown","source":"It seems like the variables have no names as such and the only variable that is missing in the test set is the target column which we need to predict. Let's check the sample submission file. We will check if the ids in sample sub and the test file are in same order or not as well."},{"metadata":{"trusted":true,"_uuid":"fe4863630fa1f8888b31c2cd85b34dae4d2f4600"},"cell_type":"code","source":"sub = fread(\"../input/sample_submission.csv\") ; head(sub) ; identical(sub$ID_code , test$ID_code)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b18edfcd7313058847202101127eb9086d6784d"},"cell_type":"markdown","source":"Let's remove the ID column"},{"metadata":{"trusted":true,"_uuid":"5d7b4c1cba6e3e8ad695a46846ff8e2932aa0b6a"},"cell_type":"code","source":"train$ID_code = NULL\ntest$ID_code = NULL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5394e88b1062bbbf283db61542a50ae3a5a9bd0"},"cell_type":"code","source":"target = train$target\nsummary(target)\ntable(target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"901eae818512bf53b123cc235dd6c8724d37c8ae"},"cell_type":"markdown","source":"we have 10% of the labels as positive and rest are zeros. \nNow let's create models"},{"metadata":{"trusted":true,"_uuid":"9c010169302decc5f66aa402c5fd3e3676c80984"},"cell_type":"code","source":"train$target = NULL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Feature engineering that might be helpful ####\n\ntrain$n91 = ifelse(train$var_9 < 8 , train$var_9 , NA)\ntrain$n92 = ifelse(train$var_9 > 8 , train$var_9 , NA)\n\ntrain$n131 = ifelse(train$var_13 < 10 , train$var_13 , NA)\ntrain$n132 = ifelse(train$var_13 > 10 , train$var_13 , NA)\n\ntrain$n1081 = ifelse(train$var_108 < 14.2 , train$var_108 , NA)\ntrain$n1082 = ifelse(train$var_108 > 14.2 , train$var_108 , NA)\n\n\n\ntest$n91 = ifelse(test$var_9 < 8 , test$var_9 , NA)\ntest$n92 = ifelse(test$var_9 > 8 , test$var_9 , NA)\n\ntest$n131 = ifelse(test$var_13 < 10 , test$var_13 , NA)\ntest$n132 = ifelse(test$var_13 > 10 , test$var_13 , NA)\n\ntest$n1081 = ifelse(test$var_108 < 14.2 , test$var_108 , NA)\ntest$n1082 = ifelse(test$var_108 > 14.2 , test$var_108 , NA)\n\nhead(train) ; head(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79f39858afc6fb61678f9f92bb46657621ed1d7c"},"cell_type":"code","source":"nrounds = 5\nset.seed(1234)\nfolds = createFolds(factor(target), k = 5, list = FALSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"238dc03e1886f6edcae730998d026a19d1cab5bb"},"cell_type":"code","source":"#train = as.data.frame(train)\n#test = as.data.frame(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"236eba463da8687ac72820e4c524f5e0efbe0e5e"},"cell_type":"code","source":"tefinal = data.matrix(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28ea56d010ef78cc4ef801964c89d6de43f47e00"},"cell_type":"markdown","source":"XGBOOST model\n\nIt is a very simple xgboost model and the parameters are not tuned properly. We can later tune the parameters by checking the cv score"},{"metadata":{"trusted":true,"_uuid":"136c6feb731d8665ca865f574585b7ca9bcfc029"},"cell_type":"code","source":"dev.result <-  rep(0, nrow(train)) \npred_te <- rep(0, nrow(test))\n\nfor (this.round in 1:nrounds){      \n      valid <- c(1:length(target)) [folds == this.round]\n      dev <- c(1:length(target)) [folds != this.round]\n\n      dtrain<- xgb.DMatrix(data= as.matrix(train[dev,]), \n                           label= target[dev])\n                           #weight = w[dev])\n      dvalid <- xgb.DMatrix(data= as.matrix(train[valid,]) , \n                            label= target[valid])\n      valids <- list(val = dvalid)\n      #### parameters are far from being optimal ####  \n      param = list(objective = \"binary:logistic\", \n               eval_metric = \"auc\",\n               max_depth = 4,\n               eta = 0.05,\n               gamma = 5,\n               subsample = 0.7,   \n               colsample_bytree = 0.7,\n               min_child_weight = 50,  \n               colsample_bylevel = 0.7,\n               lambda = 1, \n               alpha = 0,\n               booster = \"gbtree\",\n               silent = 0\n  ) \n        model<- xgb.train(data = dtrain,\n                params= param, \n                nrounds = 5000, \n                verbose = T, \n                list(val1=dtrain , val2 = dvalid) ,       \n                early_stopping_rounds = 50 , \n                print_every_n = 500,\n                maximize = T\n      )\n      pred = predict(model,as.matrix(train[valid,]))\n      dev.result[valid] = pred  \n      pred_test  = predict(model,tefinal)\n  pred_te = pred_te +pred_test\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22b6333c7661bb3d95ecbc409615de9652b9707e"},"cell_type":"markdown","source":"Now, let's check the xgboost CV score"},{"metadata":{"trusted":true,"_uuid":"47e43117523c223765ef52c2ebe66ec009b3314b"},"cell_type":"code","source":"auc(target,dev.result)\npred_test = pred_te/nrounds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0919a34fbe2c42c6a9ca070db73f83a344025560"},"cell_type":"markdown","source":"Let's save the oof prediction for further use"},{"metadata":{"trusted":true,"_uuid":"8be150bbb889b8e7c578abb610259c85020353ea"},"cell_type":"code","source":"pred_test_xgb = pred_test\noof_xgb = dev.result","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b435b51dae6b0ab1a9c3abf7a5224732e2ede28"},"cell_type":"markdown","source":"Now, let's create a simple lightgbm model.\nPlease note that, hyper parameters are not tuned and it can be improved way more. It is also faster than the XGB. "},{"metadata":{"trusted":true,"_uuid":"0ca575c91fc9b9971496af51a629aa5939068971"},"cell_type":"code","source":"dev.result <-  rep(0, nrow(train)) \npred_te <- rep(0, nrow(test))\n\nfor (this.round in 1:nrounds){      \n      valid <- c(1:length(target)) [folds == this.round]\n      dev <- c(1:length(target)) [folds != this.round]\n\n      dtrain<- lgb.Dataset(data= as.matrix(train[dev,]), \n                           label= target[dev])\n                           #weight = w[dev])\n      dvalid <- lgb.Dataset(data= as.matrix(train[valid,]) , \n                            label= target[valid])\n      valids <- list(val = dvalid)\n      #### parameters are far from being optimal ####  \n      p <- list(boosting_type = \"gbdt\", \n            objective = \"binary\",\n            metric = \"auc\",  \n            learning_rate = 0.05, \n            max_depth = 6,\n            num_leaves = 20,\n            sub_feature = 0.7, \n            sub_row = 0.7, \n            bagging_freq = 1,\n            lambda_l1 = 5, \n            lambda_l2 = 5\n               )\n    \n    model<- lgb.train(data = dtrain,\n                params= p, \n                nrounds=5000, \n                valids = list(val1=dtrain , val2 = dvalid), #,valids,\n                metric=\"auc\",\n                obj = \"binary\",\n                eval_freq = 500, \n                early_stopping_rounds=50\n      )\n    \n      pred = predict(model,as.matrix(train[valid,]))\n      dev.result[valid] = pred  \n      pred_test  = predict(model,tefinal)\n  pred_te = pred_te +pred_test\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa37c412104779b35d07e21c876592863481bbcd"},"cell_type":"markdown","source":"Let's check the lgbm cv score"},{"metadata":{"trusted":true,"_uuid":"f5cae9544fa603ac03d5d2a7babd35a02d25724c"},"cell_type":"code","source":"auc(target,dev.result)\npred_test = pred_te/nrounds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"935df1f865355d1f2690876117475cf64b877650"},"cell_type":"markdown","source":"Let's save the oof predictions here as well"},{"metadata":{"trusted":true,"_uuid":"f5bed60e4516626036dfb06924ef3108ecd93783"},"cell_type":"code","source":"pred_test_lgb = pred_test\noof_lgb = dev.result","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"505cd27743f216e21f5bac83e7ddcd8c1c9b7e22"},"cell_type":"markdown","source":"CV score of the basic average"},{"metadata":{"trusted":true,"_uuid":"30c9e7840f2ab4fd7b1b1b21497193d15e5d5920"},"cell_type":"code","source":"auc(target,(oof_xgb + oof_lgb)/2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45790935d1aae1d5396c84ba75e5308c87f0e0ef"},"cell_type":"code","source":"cor(oof_xgb , oof_lgb) ; cor(pred_test_xgb , pred_test_lgb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01663590f96e4f49e3cf99fe7f3aeb161c6ddc46"},"cell_type":"markdown","source":"Now we will use a simple logistic regression on our oof predictions to see how it performs"},{"metadata":{"trusted":true,"_uuid":"db44440327fe4111ad0519a3780c2ec247b296fb"},"cell_type":"code","source":"oof_data = data.frame(target , oof_xgb, oof_lgb)\ncolnames(oof_data)[2] = \"xgb\"\ncolnames(oof_data)[3] = \"lgb\"\n\npred_data = data.frame(pred_test_xgb , pred_test_lgb)\ncolnames(pred_data)[1] = \"xgb\"\ncolnames(pred_data)[2] = \"lgb\"\n\nlr = glm(target~., data=oof_data, family=binomial)\nsummary(lr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52c66953278c0cec031199d35bf6019cdb76b7b3"},"cell_type":"markdown","source":"Let's also check if the CV score improved after the stacking from the single models or not"},{"metadata":{"trusted":true,"_uuid":"726da3a534c301312f7472017a4deecf0242d212"},"cell_type":"code","source":"auc(target , predict(lr, newdata=oof_data, type=\"response\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"772fe463575a978b7827d02e2e0582e363c5383f"},"cell_type":"code","source":"pred_stack = predict(lr, newdata=pred_data, type=\"response\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b4eeb8da4d1708df63f42abe476ed65581d57a2"},"cell_type":"code","source":"cor(pred_stack , pred_test_lgb) ; cor(pred_stack , pred_test_xgb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3fdb38d44299774a6fe2f5a90cedfd278a97f02"},"cell_type":"markdown","source":"Now, for the final prediction, we will use the logistic regression prediction."},{"metadata":{"trusted":true,"_uuid":"fc8a50e43b701c63a0604a3db3bbd7ddb8263e1b"},"cell_type":"code","source":"sub$target = pred_stack #(pred_test_xgb + pred_test_lgb)/2\nhead(sub)\nfwrite(sub , \"toy_sub.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92f15eb123e23aa2d0cc9413bcbd294480528402"},"cell_type":"markdown","source":"Potentially this script has a lot of improvement scopes, we can tune the parameters and also add more models in the stacking algorithm and we may use a different algorithm for stacking as well.\nThanks a lot for stopping by and reading it out. :)"}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}
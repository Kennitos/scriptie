{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Elastic search local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Links:\n",
    "- celltypes for df: https://stackoverflow.com/questions/38231591/splitting-dictionary-list-inside-a-pandas-column-into-separate-columns\n",
    "- url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_output(cell):\n",
    "    \"\"\"\n",
    "    Explanation function\n",
    "    \"\"\"\n",
    "    if cell['outputs']!=[]:\n",
    "        output_type = cell['outputs'][0]['output_type']\n",
    "        if output_type == 'stream':\n",
    "            return str(cell['outputs'][0]['text']),output_type\n",
    "                       \n",
    "        elif output_type == 'error':\n",
    "            return str(cell['outputs'][0]['traceback']),output_type\n",
    "                       \n",
    "        elif output_type == 'display_data':\n",
    "            return 'displayed data',output_type\n",
    "                       \n",
    "        elif 'data' in cell['outputs'][0].keys():\n",
    "            return str(list(cell['outputs'][0]['data'].values())),output_type\n",
    "                       \n",
    "        elif 'text' in cell['outputs'][0].keys():\n",
    "            return str(cell['outputs'][0]['text']),output_type\n",
    "                       \n",
    "        elif 'ename' in cell['outputs'][0].keys():\n",
    "            return str(cell['outputs'][0]['ename']+cell['outputs'][0]['evalue']),output_type\n",
    "    else:\n",
    "#         return '',''\n",
    "        return 'empty','empty'\n",
    "\n",
    "\n",
    "def read_ipynb_cell(file_id,file_dict,file,folder,location,repo):\n",
    "    \"\"\"\n",
    "    Explanation function\n",
    "    \"\"\"\n",
    "#     file_dict = {} # gets created in the function 'create_SE_from_folder'\n",
    "    temp_dict = {}\n",
    "    temp_dict['cell_types'] = {}\n",
    "    temp_dict['output_types'] = {}\n",
    "    # cell_types/output_types used to be list, but changed to dict to use json_normalize in a later stage.\n",
    "    \n",
    "    with open(location,encoding=\"utf8\") as notebook:\n",
    "        data = json.load(notebook)\n",
    "        nbformat = data['nbformat']\n",
    "        file_text = \"\"\n",
    "        file_lines = 0\n",
    "        \n",
    "        if nbformat == 4: # current nbformat\n",
    "            data_cells =  data['cells']\n",
    "        elif nbformat == 3: # old nbformat (older files)\n",
    "            data_cells =  data['worksheets'][0]['cells']\n",
    "        elif nbformat == 2: # even older nbformat, no differences with nbformat 3 at first glance at json level\n",
    "            data_cells =  data['worksheets'][0]['cells']\n",
    "        else:\n",
    "            print(nbformat,file)\n",
    "\n",
    "        for cell in data_cells:\n",
    "            cell_type = cell['cell_type']\n",
    "            if cell_type == 'code' and (nbformat == 3 or nbformat == 2): #cell['source'] doesn't exist within this condition, use cell['input']\n",
    "                text = cell['input']\n",
    "            else:\n",
    "                text = cell['source']\n",
    "            clean_cell = list(map(lambda s: s.strip(), text)) #remove the '\\n' at the end of each string in the list         \n",
    "            lines = len(clean_cell)\n",
    "            single_string = ' '.join(clean_cell)\n",
    "            file_text += single_string\n",
    "            file_lines += lines\n",
    "            \n",
    "            # CELL TYPE\n",
    "            cell_type = cell['cell_type']\n",
    "            if cell_type == 'code':\n",
    "                output, output_type = code_output(cell)\n",
    "                file_text += output\n",
    "                if output_type not in temp_dict['output_types'].keys():\n",
    "                    temp_dict['output_types'][output_type] = 1    \n",
    "            if cell_type not in temp_dict['cell_types'].keys():\n",
    "                temp_dict['cell_types'][cell_type] = 1\n",
    "\n",
    "        temp_dict['file'] = file\n",
    "        temp_dict['nbformat'] = data['nbformat']\n",
    "        temp_dict['folder'] = folder\n",
    "        temp_dict['repo'] =  repo\n",
    "        temp_dict['location'] = location\n",
    "        temp_dict['string'] = file_text\n",
    "        temp_dict['lines'] = file_lines\n",
    "        file_dict[file_id] = temp_dict\n",
    "        \n",
    "    return file_dict\n",
    "\n",
    "def rec_to_actions(df):\n",
    "    for record in df.to_dict(orient=\"records\"):\n",
    "        yield ('{ \"index\" : { \"_index\" : \"%s\", \"_type\" : \"%s\" }}'% (INDEX, TYPE))\n",
    "        yield (json.dumps(record, default=int))\n",
    "      \n",
    "\n",
    "def index_marks(nrows, chunk_size):\n",
    "    return range(1 * chunk_size, (nrows // chunk_size + 1) * chunk_size, chunk_size)\n",
    "\n",
    "\n",
    "def split(dfm, chunk_size):\n",
    "    indices = index_marks(dfm.shape[0], chunk_size)\n",
    "    return np.split(dfm, indices) \n",
    "    \n",
    "\n",
    "def create_SE_from_folder(repo_name,folder):\n",
    "    \"\"\"\n",
    "    Explenation\n",
    "    \"\"\"\n",
    "    total_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    if folder == 'single_files':\n",
    "        path = cwd+\"\\\\\"+folder\n",
    "    elif folder == 'notebooks1':\n",
    "        path = 'E:\\\\Files\\\\Universiteit\\\\thesis\\\\notebooks1' # External harddrive with 90 gb's of ipynb files\n",
    "    elif folder == 'notebooks':\n",
    "        path = cwd+\"\\\\\"+folder\n",
    "    else:\n",
    "        path = cwd+\"\\\\\"+folder+\"\\\\\"+repo_name\n",
    "        print(path)\n",
    "        \n",
    "    # CREATE DICT FOR ALL 'IPYNB' FILES\n",
    "    print('Looping through all files within the path directory...')\n",
    "    file_id = 0\n",
    "    file_dict = {}    \n",
    "    for root, dirs, files in os.walk(path): # tqdm progress bar not possible since os.walk doesnt know\n",
    "        for file in files:                  # how many files it will cross yet\n",
    "            if file.endswith(\".ipynb\"):\n",
    "                folder = root.split('\\\\')[-1]\n",
    "                location = os.path.join(root,file)\n",
    "                repo = repo_name\n",
    "                try:\n",
    "                    file_dict = read_ipynb_cell(file_id,file_dict,file,folder,location,repo)\n",
    "                except Exception as e:\n",
    "#                     print(\"failed for file:\",file)\n",
    "                    fail_count += 1\n",
    "                file_id += 1\n",
    "                total_count += 1\n",
    "                \n",
    "    print('Failed files:',fail_count,'/',total_count) \n",
    "    print('Setting up local elastic search')\n",
    "    \n",
    "    # CREATE DATAFRAME FROM DICT\n",
    "    file_df = pd.DataFrame.from_dict(file_dict,orient='index')\n",
    "#     file_df = file_df.fillna('empty').reset_index()\n",
    "    file_df_extra = pd.concat([file_df.drop(['cell_types','output_types'], axis=1),\n",
    "                            json_normalize(file_df['cell_types']),\n",
    "                            json_normalize(file_df['output_types'])],\n",
    "                           axis=1).fillna(0)#.drop([\"\"],axis=1)\n",
    "    \n",
    "\n",
    "    # PUT DATAFRAME INTO ELASTIC SEARCH\n",
    "    split_size = 500\n",
    "    for chuck in tqdm_notebook(split(file_df_extra, split_size)):\n",
    "        try:\n",
    "            r = es.bulk(rec_to_actions(chuck))\n",
    "        except:\n",
    "            print('mini_split')\n",
    "            try:\n",
    "                for mini_chuck in tqdm_notebook(split(chuck, split_size/10)):\n",
    "                    r = es.bulk(rec_to_actions(mini_chuck))\n",
    "            except Exception as e:\n",
    "                print('failed, skip this df',e)\n",
    "    \n",
    "    return es, file_df_extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell below takes proximately:  \n",
    "- looping through files: 2 minutes 30 seconds\n",
    "- put df in local es: 1 minutes 0 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through all files within the path directory...\n",
      "Failed files: 35 / 6529\n",
      "Setting up local elastic search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b8f9b4c40443788dc43c8154c575de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini_split\n",
      "failed, skip this df 'float' object cannot be interpreted as an integer\n",
      "mini_split\n",
      "failed, skip this df 'float' object cannot be interpreted as an integer\n",
      "\n",
      "Wall time: 3min 31s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>nbformat</th>\n",
       "      <th>folder</th>\n",
       "      <th>repo</th>\n",
       "      <th>location</th>\n",
       "      <th>string</th>\n",
       "      <th>lines</th>\n",
       "      <th>code</th>\n",
       "      <th>markdown</th>\n",
       "      <th>heading</th>\n",
       "      <th>raw</th>\n",
       "      <th>stream</th>\n",
       "      <th>empty</th>\n",
       "      <th>execute_result</th>\n",
       "      <th>display_data</th>\n",
       "      <th>pyout</th>\n",
       "      <th>pyerr</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>nb_1000546.ipynb</td>\n",
       "      <td>4.0</td>\n",
       "      <td>notebooks</td>\n",
       "      <td>sample</td>\n",
       "      <td>C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_1...</td>\n",
       "      <td>from defectlib import load_tensors from defect...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>nb_1000590.ipynb</td>\n",
       "      <td>4.0</td>\n",
       "      <td>notebooks</td>\n",
       "      <td>sample</td>\n",
       "      <td>C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_1...</td>\n",
       "      <td># Up to this point we have not used R, but now...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>nb_1000623.ipynb</td>\n",
       "      <td>4.0</td>\n",
       "      <td>notebooks</td>\n",
       "      <td>sample</td>\n",
       "      <td>C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_1...</td>\n",
       "      <td>import numpy as np import math import matplotl...</td>\n",
       "      <td>329.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>nb_1001147.ipynb</td>\n",
       "      <td>4.0</td>\n",
       "      <td>notebooks</td>\n",
       "      <td>sample</td>\n",
       "      <td>C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_1...</td>\n",
       "      <td>import numpy as np import pandas as pd import ...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>nb_1001280.ipynb</td>\n",
       "      <td>3.0</td>\n",
       "      <td>notebooks</td>\n",
       "      <td>sample</td>\n",
       "      <td>C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_1...</td>\n",
       "      <td>def gcd(m,n): while m%n != 0: oldm = m oldn = ...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6524</td>\n",
       "      <td>nb_999055.ipynb</td>\n",
       "      <td>4.0</td>\n",
       "      <td>notebooks</td>\n",
       "      <td>sample</td>\n",
       "      <td>C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_9...</td>\n",
       "      <td>Derived from sqlite_P1_v10import pandas as pd ...</td>\n",
       "      <td>182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6525</td>\n",
       "      <td>nb_999056.ipynb</td>\n",
       "      <td>4.0</td>\n",
       "      <td>notebooks</td>\n",
       "      <td>sample</td>\n",
       "      <td>C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_9...</td>\n",
       "      <td>import pandas as pd import sqlite3emptypd.set_...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6526</td>\n",
       "      <td>nb_999673.ipynb</td>\n",
       "      <td>4.0</td>\n",
       "      <td>notebooks</td>\n",
       "      <td>sample</td>\n",
       "      <td>C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_9...</td>\n",
       "      <td>import pandas as pd import reempty### Splits s...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6527</td>\n",
       "      <td>nb_999700.ipynb</td>\n",
       "      <td>3.0</td>\n",
       "      <td>notebooks</td>\n",
       "      <td>sample</td>\n",
       "      <td>C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_9...</td>\n",
       "      <td>from sklearn import datasets iris = datasets.l...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6528</td>\n",
       "      <td>nb_999948.ipynb</td>\n",
       "      <td>4.0</td>\n",
       "      <td>notebooks</td>\n",
       "      <td>sample</td>\n",
       "      <td>C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_9...</td>\n",
       "      <td>Performance Tuning ==================  In this...</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6529 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  file  nbformat     folder    repo  \\\n",
       "0     nb_1000546.ipynb       4.0  notebooks  sample   \n",
       "1     nb_1000590.ipynb       4.0  notebooks  sample   \n",
       "2     nb_1000623.ipynb       4.0  notebooks  sample   \n",
       "3     nb_1001147.ipynb       4.0  notebooks  sample   \n",
       "4     nb_1001280.ipynb       3.0  notebooks  sample   \n",
       "...                ...       ...        ...     ...   \n",
       "6524   nb_999055.ipynb       4.0  notebooks  sample   \n",
       "6525   nb_999056.ipynb       4.0  notebooks  sample   \n",
       "6526   nb_999673.ipynb       4.0  notebooks  sample   \n",
       "6527   nb_999700.ipynb       3.0  notebooks  sample   \n",
       "6528   nb_999948.ipynb       4.0  notebooks  sample   \n",
       "\n",
       "                                               location  \\\n",
       "0     C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_1...   \n",
       "1     C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_1...   \n",
       "2     C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_1...   \n",
       "3     C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_1...   \n",
       "4     C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_1...   \n",
       "...                                                 ...   \n",
       "6524  C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_9...   \n",
       "6525  C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_9...   \n",
       "6526  C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_9...   \n",
       "6527  C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_9...   \n",
       "6528  C:\\Users\\kenne\\Documents\\thesis\\notebooks\\nb_9...   \n",
       "\n",
       "                                                 string  lines  code  \\\n",
       "0     from defectlib import load_tensors from defect...   31.0   1.0   \n",
       "1     # Up to this point we have not used R, but now...   59.0   1.0   \n",
       "2     import numpy as np import math import matplotl...  329.0   1.0   \n",
       "3     import numpy as np import pandas as pd import ...   94.0   1.0   \n",
       "4     def gcd(m,n): while m%n != 0: oldm = m oldn = ...   73.0   1.0   \n",
       "...                                                 ...    ...   ...   \n",
       "6524  Derived from sqlite_P1_v10import pandas as pd ...  182.0   0.0   \n",
       "6525  import pandas as pd import sqlite3emptypd.set_...   38.0   0.0   \n",
       "6526  import pandas as pd import reempty### Splits s...   31.0   0.0   \n",
       "6527  from sklearn import datasets iris = datasets.l...   23.0   0.0   \n",
       "6528  Performance Tuning ==================  In this...   67.0   0.0   \n",
       "\n",
       "      markdown  heading  raw  stream  empty  execute_result  display_data  \\\n",
       "0          0.0      0.0  0.0     1.0    1.0             1.0           1.0   \n",
       "1          1.0      0.0  0.0     1.0    1.0             1.0           0.0   \n",
       "2          0.0      0.0  0.0     1.0    1.0             0.0           1.0   \n",
       "3          0.0      0.0  0.0     1.0    1.0             0.0           0.0   \n",
       "4          0.0      0.0  0.0     1.0    0.0             0.0           0.0   \n",
       "...        ...      ...  ...     ...    ...             ...           ...   \n",
       "6524       0.0      0.0  0.0     0.0    0.0             0.0           0.0   \n",
       "6525       0.0      0.0  0.0     0.0    0.0             0.0           0.0   \n",
       "6526       0.0      0.0  0.0     0.0    0.0             0.0           0.0   \n",
       "6527       0.0      0.0  0.0     0.0    0.0             0.0           0.0   \n",
       "6528       0.0      0.0  0.0     0.0    0.0             0.0           0.0   \n",
       "\n",
       "      pyout  pyerr  error  \n",
       "0       0.0    0.0    0.0  \n",
       "1       0.0    0.0    0.0  \n",
       "2       0.0    0.0    0.0  \n",
       "3       0.0    0.0    0.0  \n",
       "4       0.0    0.0    0.0  \n",
       "...     ...    ...    ...  \n",
       "6524    0.0    0.0    0.0  \n",
       "6525    0.0    0.0    0.0  \n",
       "6526    0.0    0.0    0.0  \n",
       "6527    0.0    0.0    0.0  \n",
       "6528    0.0    0.0    0.0  \n",
       "\n",
       "[6529 rows x 18 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time es, file_df = create_SE_from_folder('sample','notebooks')\n",
    "file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6529 entries, 0 to 6528\n",
      "Data columns (total 18 columns):\n",
      "file              6529 non-null object\n",
      "nbformat          6529 non-null float64\n",
      "folder            6529 non-null object\n",
      "repo              6529 non-null object\n",
      "location          6529 non-null object\n",
      "string            6529 non-null object\n",
      "lines             6529 non-null float64\n",
      "code              6529 non-null float64\n",
      "markdown          6529 non-null float64\n",
      "heading           6529 non-null float64\n",
      "raw               6529 non-null float64\n",
      "stream            6529 non-null float64\n",
      "empty             6529 non-null float64\n",
      "execute_result    6529 non-null float64\n",
      "display_data      6529 non-null float64\n",
      "pyout             6529 non-null float64\n",
      "pyerr             6529 non-null float64\n",
      "error             6529 non-null float64\n",
      "dtypes: float64(13), object(5)\n",
      "memory usage: 969.1+ KB\n"
     ]
    }
   ],
   "source": [
    "file_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # update pandas for this function\n",
    "# # !pip install --user pandas==1.0.3\n",
    "# from pandas.io.json import json_normalize\n",
    "\n",
    "# print(pd.__version__)\n",
    "# json_normalize(file_df['cell_types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_normalize(file_df['cell_types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate the normalized columns of 'cell_types' and 'output_types'\n",
    "# # fill the NaN values with 0, meaning negative (while 1 means a positive)\n",
    "# # drop the column '' (empty sttring)\n",
    "\n",
    "# file_df_extra = pd.concat([file_df.drop(['cell_types','output_types'], axis=1),\n",
    "#                             json_normalize(file_df['cell_types']),\n",
    "#                             json_normalize(file_df['output_types'])],\n",
    "#                            axis=1).fillna(0).drop([\"\"],axis=1)\n",
    "# file_df_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # get all files containing a header\n",
    "# file_df[file_df.heading == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(file_df.cell_types[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time print(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create local elastic search variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic Search curl's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health status index             uuid                   pri rep docs.count docs.deleted store.size pri.store.size\n",
      "yellow open   test_df_extra     DCUnYCeMSeSpqyf4guzzhw   1   1       6029            0    514.8mb        514.8mb\n",
      "yellow open   sample_notebooks1 kYo_GDRQTrmTrrxSklQDSQ   1   1     176812            0    915.7mb        915.7mb\n",
      "yellow open   sample_notebooks  64SAHKxJSC6IqaDLYCuJ3A   1   1     176312            0    691.5mb        691.5mb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   452  100   452    0     0  13696      0 --:--:-- --:--:-- --:--:-- 13294\n",
      "100   452  100   452    0     0  13696      0 --:--:-- --:--:-- --:--:-- 13294\n"
     ]
    }
   ],
   "source": [
    "# !curl \"http://localhost:9200/test\"\n",
    "# !curl -XDELETE \"localhost:9200/test_df_extra\"\n",
    "# !curl -XPOST \"http://localhost:9200/_shutdown\"\n",
    "\n",
    "!curl \"http://localhost:9200/_cat/indices?v\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put single_files into elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = 'http://localhost:9200/'\n",
    "es = Elasticsearch(hosts=[HOST]) \n",
    "\n",
    "\n",
    "INDEX = \"test_single\"\n",
    "TYPE = \"record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_SE_from_folder('unknown',es,'single_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health status index             uuid                   pri rep docs.count docs.deleted store.size pri.store.size\n",
      "yellow open   sample_notebooks1 kYo_GDRQTrmTrrxSklQDSQ   1   1     176812            0    915.7mb        915.7mb\n",
      "yellow open   sample_notebooks  64SAHKxJSC6IqaDLYCuJ3A   1   1     176312            0    691.5mb        691.5mb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   339  100   339    0     0   2690      0 --:--:-- --:--:-- --:--:--  2712\n"
     ]
    }
   ],
   "source": [
    "!curl \"http://localhost:9200/_cat/indices?v\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put github repositories into elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -XDELETE \"localhost:9200/test_repos\"\n",
    "\n",
    "HOST = 'http://localhost:9200/'\n",
    "es = Elasticsearch(hosts=[HOST]) \n",
    "\n",
    "\n",
    "INDEX = \"test_df_extra\"\n",
    "TYPE = \"record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'repos'\n",
    "path = path = os.getcwd()+'//'+folder\n",
    "dir_list = os.listdir(path)\n",
    "file_id = 0\n",
    "for repo in tqdm_notebook(dir_list):\n",
    "    try:\n",
    "        a = create_SE_from_folder(repo,es,folder,file_id)\n",
    "        es = a[0]\n",
    "        file_id = a[1]\n",
    "    except Exception as e:\n",
    "        print(e,repo)\n",
    "\n",
    "    \n",
    "# cell_df,es,file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"http://localhost:9200/_cat/indices?v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ipynb(folder):\n",
    "    path = os.getcwd()+'//'+folder\n",
    "    dir_list = os.listdir(path)\n",
    "\n",
    "    file_id = 0\n",
    "    for root, dirs, files in os.walk(path):\n",
    "#         for file in tqdm_notebook(files):\n",
    "        for file in files:\n",
    "            if file.endswith(\".ipynb\"):\n",
    "                file_id += 1\n",
    "    return file_id\n",
    "\n",
    "count_ipynb('single_files'),count_ipynb('repos'),count_ipynb('notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_SE_from_folder('sample',es,'notebooks',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = 'http://localhost:9200/'\n",
    "es = Elasticsearch(hosts=[HOST],timeout=30,max_retries=10, retry_on_timeout=True) \n",
    "\n",
    "\n",
    "INDEX = \"sample_notebooks1\"\n",
    "TYPE = \"record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = create_SE_from_folder('sample',es,'notebooks',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.drop(columns=['char'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -XDELETE \"localhost:9200/sample_notebooks\"\n",
    "!curl \"http://localhost:9200/_cat/indices?v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(sample_df.output_type)\n",
    "\n",
    "sample_df[sample_df.output_type=='display_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chuck in tqdm_notebook(split(sample_df, 500)):\n",
    "    try:\n",
    "        r = es.bulk(rec_to_actions(chuck))\n",
    "    except:\n",
    "        try:\n",
    "            for mini_chuck in tqdm_notebook(split(chuck, 500)):\n",
    "                r = es.bulk(rec_to_actions(mini_chuck))\n",
    "        except:\n",
    "            print('failed, skip this df')\n",
    "    \n",
    "    \n",
    "# for chuck in tqdm_notebook(split(sample_df.drop(columns=['char']), 10000)):\n",
    "#     r = es.bulk(rec_to_actions(chuck))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data = np.random.rand(117000,2)\n",
    "df = pd.DataFrame(data=numpy_data, columns=[\"column1\", \"column2\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in split(df, 50000):\n",
    "    print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(code:1)\n",
      "(markdown:1)\n",
      "(stream:1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'code': 1,\n",
       "  'markdown': 1,\n",
       "  'heading': 0,\n",
       "  'raw': 0,\n",
       "  'stream': 1,\n",
       "  'empty': 0,\n",
       "  'execute_result': 0},\n",
       " '(string:drop column) AND (code:1) AND (markdown:1) AND (stream:1)')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'drop column'\n",
    "attributes_dict = {'code':1,'markdown':1,'heading':0,'raw':0,'stream':1,'empty':0,'execute_result':0}\n",
    "query_str = ''\n",
    "\n",
    "query_str += '(string:'+text+')'\n",
    "for key in q.keys():\n",
    "    if q[key]==1:\n",
    "        to_add = '('+key+':1)'\n",
    "\n",
    "        \n",
    "        if query_str != '':\n",
    "            query_str += ' AND '\n",
    "        query_str += to_add\n",
    "        print(to_add)\n",
    "        \n",
    "    \n",
    "q,query_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'query_string': {'query': '(string:drop column) AND (code:1) AND (markdown:1) AND (stream:1)'}},\n",
       " 'highlight': {'pre_tags': ['<b>'],\n",
       "  'post_tags': ['</b>'],\n",
       "  'order': 'score',\n",
       "  'fields': {'string': {}}}}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_string_query(text,att_dict,highlight):\n",
    "    query_str = ''\n",
    "    fields = []\n",
    "    \n",
    "    query_str += '(string:'+text+')'\n",
    "    for key in att_dict.keys():\n",
    "        if att_dict[key]==1:\n",
    "            fields.append\n",
    "            to_add = '('+key+':1)'\n",
    "            if query_str != '':\n",
    "                query_str += ' AND '\n",
    "            query_str += to_add\n",
    "    \n",
    "        \n",
    "    q = {}\n",
    "    query_dict = {}\n",
    "    query_string = {}\n",
    "    \n",
    "    query_string['query'] = query_str\n",
    "    query_dict['query_string'] = query_string\n",
    "    q['query'] = query_dict\n",
    "    \n",
    "    if highlight == True:\n",
    "        highlight = {\"pre_tags\":[\"<b>\"],\n",
    "                     \"post_tags\":[\"</b>\"],\n",
    "                     \"order\":\"score\",\n",
    "                     \"fields\":{'string':{}}#,'code_str':{}}\n",
    "                    }\n",
    "        q['highlight'] = highlight\n",
    "    return q\n",
    "\n",
    "# test\n",
    "text = 'drop column'\n",
    "attributes_dict = {'code':1,'markdown':1,'heading':0,'raw':0,'stream':1,'empty':0,'execute_result':0}\n",
    "\n",
    "q_test_querystring = query_string_query(text,attributes_dict,True)\n",
    "q_test_querystring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_match_query(code,markdown,modules,highlight):\n",
    "    query_str = ''\n",
    "    fields = []\n",
    "    \n",
    "    if code != None:\n",
    "        fields += ['code_str']\n",
    "        query_str += ' '+code\n",
    "    if markdown != None:\n",
    "        fields += ['markdown_str']\n",
    "        query_str += ' '+markdown\n",
    "    if modules != None:\n",
    "        fields += ['modules']\n",
    "        query_str += ' '+modules   \n",
    "    \n",
    "        \n",
    "    q = {}\n",
    "    query_dict = {}\n",
    "    multi_match = {}\n",
    "    \n",
    "    multi_match['fields'] = fields\n",
    "    multi_match['query'] = query_str\n",
    "    query_dict['multi_match'] = multi_match\n",
    "    q['query'] = query_dict\n",
    "    \n",
    "    if highlight != None:\n",
    "        highlight = {\"pre_tags\":[\"<b>\"],\n",
    "            \"post_tags\":[\"</b>\"],\n",
    "            \"fields\":{'markdown_str':{}}}\n",
    "        q['highlight'] = highlight\n",
    "    return q\n",
    "\n",
    "def query_string_query(code,markdown,modules,highlight):\n",
    "    query_str = ''\n",
    "    fields = []\n",
    "    \n",
    "    if code != None:\n",
    "        fields += ['code_str']\n",
    "        query_str += '(code_str:'+code+')'\n",
    "    if markdown != None:\n",
    "        fields += ['markdown_str']\n",
    "        if query_str != '':\n",
    "            query_str += ' AND '\n",
    "        query_str += '(markdown_str:'+markdown+')'\n",
    "    if modules != None:\n",
    "        fields += ['modules']\n",
    "        if query_str != '':\n",
    "            query_str += ' AND '\n",
    "        query_str += '(modules:'+modules+')'  \n",
    "    \n",
    "        \n",
    "    q = {}\n",
    "    query_dict = {}\n",
    "    query_string = {}\n",
    "    \n",
    "    query_string['query'] = query_str\n",
    "    query_dict['query_string'] = query_string\n",
    "    q['query'] = query_dict\n",
    "    \n",
    "    if highlight == True:\n",
    "        highlight = {\"pre_tags\":[\"<b>\"],\n",
    "                     \"post_tags\":[\"</b>\"],\n",
    "                     \"order\":\"score\",\n",
    "                     \"fields\":{'markdown_str':{},'code_str':{}}}\n",
    "        q['highlight'] = highlight\n",
    "    return q\n",
    "\n",
    "# \"order\":\"score\",\n",
    "# \"fields\":{'_all':{}}} WERKT NIET....\n",
    "\n",
    "\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/6.8/search-request-highlighting.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'multi_match': {'fields': ['code_str', 'modules'],\n",
       "   'query': ' linear numpy'}},\n",
       " 'highlight': {'pre_tags': ['<b>'],\n",
       "  'post_tags': ['</b>'],\n",
       "  'fields': {'markdown_str': {}}}}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "q_test_multimatch = multi_match_query('linear',None,'numpy',True)\n",
    "q_test_multimatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'query_string': {'query': '(markdown_str:drop) AND (modules:pandas)'}},\n",
       " 'highlight': {'pre_tags': ['<b>'],\n",
       "  'post_tags': ['</b>'],\n",
       "  'order': 'score',\n",
       "  'fields': {'markdown_str': {}, 'code_str': {}}}}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "q_test_querystring = query_string_query(None,'drop','pandas',True)\n",
    "q_test_querystring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

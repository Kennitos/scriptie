{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine for Jupyter notebooks\n",
    "## Creating a dictionary from the notebook files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00.00-Preface.ipynb',\n",
       " '01.00-IPython-Beyond-Normal-Python.ipynb',\n",
       " '01.01-Help-And-Documentation.ipynb',\n",
       " '01.02-Shell-Keyboard-Shortcuts.ipynb',\n",
       " '01.03-Magic-Commands.ipynb',\n",
       " '01.04-Input-Output-History.ipynb',\n",
       " '01.05-IPython-And-Shell-Commands.ipynb',\n",
       " '01.06-Errors-and-Debugging.ipynb',\n",
       " '01.07-Timing-and-Profiling.ipynb',\n",
       " '01.08-More-IPython-Resources.ipynb',\n",
       " '02.00-Introduction-to-NumPy.ipynb',\n",
       " '02.01-Understanding-Data-Types.ipynb',\n",
       " '02.02-The-Basics-Of-NumPy-Arrays.ipynb',\n",
       " '02.03-Computation-on-arrays-ufuncs.ipynb',\n",
       " '02.04-Computation-on-arrays-aggregates.ipynb',\n",
       " '02.05-Computation-on-arrays-broadcasting.ipynb',\n",
       " '02.06-Boolean-Arrays-and-Masks.ipynb',\n",
       " '02.07-Fancy-Indexing.ipynb',\n",
       " '02.08-Sorting.ipynb',\n",
       " '02.09-Structured-Data-NumPy.ipynb',\n",
       " '03.00-Introduction-to-Pandas.ipynb',\n",
       " '03.01-Introducing-Pandas-Objects.ipynb',\n",
       " '03.02-Data-Indexing-and-Selection.ipynb',\n",
       " '03.03-Operations-in-Pandas.ipynb',\n",
       " '03.04-Missing-Values.ipynb',\n",
       " '03.05-Hierarchical-Indexing.ipynb',\n",
       " '03.06-Concat-And-Append.ipynb',\n",
       " '03.07-Merge-and-Join.ipynb',\n",
       " '03.08-Aggregation-and-Grouping.ipynb',\n",
       " '03.09-Pivot-Tables.ipynb',\n",
       " '03.10-Working-With-Strings.ipynb',\n",
       " '03.11-Working-with-Time-Series.ipynb',\n",
       " '03.12-Performance-Eval-and-Query.ipynb',\n",
       " '03.13-Further-Resources.ipynb',\n",
       " '04.00-Introduction-To-Matplotlib.ipynb',\n",
       " '04.01-Simple-Line-Plots.ipynb',\n",
       " '04.02-Simple-Scatter-Plots.ipynb',\n",
       " '04.03-Errorbars.ipynb',\n",
       " '04.04-Density-and-Contour-Plots.ipynb',\n",
       " '04.05-Histograms-and-Binnings.ipynb',\n",
       " '04.06-Customizing-Legends.ipynb',\n",
       " '04.07-Customizing-Colorbars.ipynb',\n",
       " '04.08-Multiple-Subplots.ipynb',\n",
       " '04.09-Text-and-Annotation.ipynb',\n",
       " '04.10-Customizing-Ticks.ipynb',\n",
       " '04.11-Settings-and-Stylesheets.ipynb',\n",
       " '04.12-Three-Dimensional-Plotting.ipynb',\n",
       " '04.13-Geographic-Data-With-Basemap.ipynb',\n",
       " '04.14-Visualization-With-Seaborn.ipynb',\n",
       " '04.15-Further-Resources.ipynb',\n",
       " '05.00-Machine-Learning.ipynb',\n",
       " '05.01-What-Is-Machine-Learning.ipynb',\n",
       " '05.02-Introducing-Scikit-Learn.ipynb',\n",
       " '05.03-Hyperparameters-and-Model-Validation.ipynb',\n",
       " '05.04-Feature-Engineering.ipynb',\n",
       " '05.05-Naive-Bayes.ipynb',\n",
       " '05.06-Linear-Regression.ipynb',\n",
       " '05.07-Support-Vector-Machines.ipynb',\n",
       " '05.08-Random-Forests.ipynb',\n",
       " '05.09-Principal-Component-Analysis.ipynb',\n",
       " '05.10-Manifold-Learning.ipynb',\n",
       " '05.11-K-Means.ipynb',\n",
       " '05.12-Gaussian-Mixtures.ipynb',\n",
       " '05.13-Kernel-Density-Estimation.ipynb',\n",
       " '05.14-Image-Features.ipynb',\n",
       " '05.15-Learning-More.ipynb',\n",
       " '06.00-Figure-Code.ipynb',\n",
       " 'Index.ipynb']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = 'PythonDataScienceHandbook-notebooks'\n",
    "files = [file for file in os.listdir(os.getcwd()+'\\\\'+folder) if file[-6:]=='.ipynb']\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_ipynb(file,folder):\n",
    "#     cwd = os.getcwd()\n",
    "#     with open(cwd+'\\\\'+folder+'\\\\'+file,encoding=\"utf8\") as file:\n",
    "#         data = json.load(file)\n",
    "#         code = []\n",
    "#         markdown = []\n",
    "#         for cell in data['cells']:\n",
    "#             clean_cell = list(map(lambda s: s.strip(), cell['source'])) #remove the '\\n' at the end of each string in the list\n",
    "#             if cell['cell_type'] == 'markdown':\n",
    "#                 markdown += clean_cell\n",
    "#             if cell['cell_type'] == 'code':\n",
    "#                 code += clean_cell\n",
    "                \n",
    "#     markdown_str = ' '.join(map(str, markdown))\n",
    "#     code_str = ' '.join(map(str, code))\n",
    "#     return markdown,code,markdown_str,code_str\n",
    "        \n",
    "\n",
    "# # read_ipynb('00.00-Preface.ipynb',folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ipynb(file,folder):\n",
    "    cwd = os.getcwd()\n",
    "    with open(cwd+'\\\\'+folder+'\\\\'+file,encoding=\"utf8\") as file:\n",
    "        data = json.load(file)\n",
    "        code = []\n",
    "        markdown = []\n",
    "        modules = []\n",
    "        heading = []\n",
    "        code_output = []\n",
    "        \n",
    "        output_line = []\n",
    "        for cell in data['cells']:\n",
    "            clean_cell = list(map(lambda s: s.strip(), cell['source'])) #remove the '\\n' at the end of each string in the list         \n",
    "            for line in clean_cell:\n",
    "                if line[:6]=='import' or line[:4]=='from':\n",
    "                    modules += [line]\n",
    "                if line[:1] == '#':\n",
    "                    heading += [line]\n",
    "            \n",
    "            if cell['cell_type'] == 'markdown':\n",
    "                markdown += clean_cell\n",
    "            \n",
    "            if cell['cell_type'] == 'code':\n",
    "                code += clean_cell\n",
    "                if cell['outputs']!=[]:\n",
    "                    output_type = cell['outputs'][0]['output_type']\n",
    "                    if output_type == 'stream':\n",
    "                        output_line = cell['outputs'][0]['text']\n",
    "                    if output_type == 'execute_result':\n",
    "                        output_line = cell['outputs'][0]['data']['text/plain']\n",
    "                    code_output += output_line\n",
    "\n",
    "    markdown_str = ' '.join(map(str, markdown))\n",
    "    code_str = ' '.join(map(str, code))\n",
    "    code_output_str = ' '.join(map(str, code_output))\n",
    "    modules = list(set(modules))\n",
    "\n",
    "    return sorted(modules),heading,markdown_str,code_str,code_output_str #markdown,code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['import numpy as np'],\n",
       " ['# The Basics of NumPy Arrays',\n",
       "  '## NumPy Array Attributes',\n",
       "  '## Array Indexing: Accessing Single Elements',\n",
       "  '## Array Slicing: Accessing Subarrays',\n",
       "  '### One-dimensional subarrays',\n",
       "  '### Multi-dimensional subarrays',\n",
       "  '#### Accessing array rows and columns',\n",
       "  '### Subarrays as no-copy views',\n",
       "  '### Creating copies of arrays',\n",
       "  '## Reshaping of Arrays',\n",
       "  '# row vector via reshape',\n",
       "  '# row vector via newaxis',\n",
       "  '# column vector via reshape',\n",
       "  '# column vector via newaxis',\n",
       "  '## Array Concatenation and Splitting',\n",
       "  '### Concatenation of arrays',\n",
       "  '# concatenate along the first axis',\n",
       "  '# concatenate along the second axis (zero-indexed)',\n",
       "  '# vertically stack the arrays',\n",
       "  '# horizontally stack the arrays',\n",
       "  '### Splitting of arrays'],\n",
       " '<!--BOOK_INFORMATION--> <img align=\"left\" style=\"padding-right:10px;\" src=\"figures/PDSH-cover-small.png\">  *This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*  *The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book](http://shop.oreilly.com/product/0636920034919.do)!* <!--NAVIGATION--> < [Understanding Data Types in Python](02.01-Understanding-Data-Types.ipynb) | [Contents](Index.ipynb) | [Computation on NumPy Arrays: Universal Functions](02.03-Computation-on-arrays-ufuncs.ipynb) >  <a href=\"https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.02-The-Basics-Of-NumPy-Arrays.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a> # The Basics of NumPy Arrays Data manipulation in Python is nearly synonymous with NumPy array manipulation: even newer tools like Pandas ([Chapter 3](03.00-Introduction-to-Pandas.ipynb)) are built around the NumPy array. This section will present several examples of using NumPy array manipulation to access data and subarrays, and to split, reshape, and join the arrays. While the types of operations shown here may seem a bit dry and pedantic, they comprise the building blocks of many other examples used throughout the book. Get to know them well!  We\\'ll cover a few categories of basic array manipulations here:  - *Attributes of arrays*: Determining the size, shape, memory consumption, and data types of arrays - *Indexing of arrays*: Getting and setting the value of individual array elements - *Slicing of arrays*: Getting and setting smaller subarrays within a larger array - *Reshaping of arrays*: Changing the shape of a given array - *Joining and splitting of arrays*: Combining multiple arrays into one, and splitting one array into many ## NumPy Array Attributes First let\\'s discuss some useful array attributes. We\\'ll start by defining three random arrays, a one-dimensional, two-dimensional, and three-dimensional array. We\\'ll use NumPy\\'s random number generator, which we will *seed* with a set value in order to ensure that the same random arrays are generated each time this code is run: Each array has attributes ``ndim`` (the number of dimensions), ``shape`` (the size of each dimension), and ``size`` (the total size of the array): Another useful attribute is the ``dtype``, the data type of the array (which we discussed previously in [Understanding Data Types in Python](02.01-Understanding-Data-Types.ipynb)): Other attributes include ``itemsize``, which lists the size (in bytes) of each array element, and ``nbytes``, which lists the total size (in bytes) of the array: In general, we expect that ``nbytes`` is equal to ``itemsize`` times ``size``. ## Array Indexing: Accessing Single Elements If you are familiar with Python\\'s standard list indexing, indexing in NumPy will feel quite familiar. In a one-dimensional array, the $i^{th}$ value (counting from zero) can be accessed by specifying the desired index in square brackets, just as with Python lists: To index from the end of the array, you can use negative indices: In a multi-dimensional array, items can be accessed using a comma-separated tuple of indices: Values can also be modified using any of the above index notation: Keep in mind that, unlike Python lists, NumPy arrays have a fixed type. This means, for example, that if you attempt to insert a floating-point value to an integer array, the value will be silently truncated. Don\\'t be caught unaware by this behavior! ## Array Slicing: Accessing Subarrays Just as we can use square brackets to access individual array elements, we can also use them to access subarrays with the *slice* notation, marked by the colon (``:``) character. The NumPy slicing syntax follows that of the standard Python list; to access a slice of an array ``x``, use this: ``` python x[start:stop:step] ``` If any of these are unspecified, they default to the values ``start=0``, ``stop=``*``size of dimension``*, ``step=1``. We\\'ll take a look at accessing sub-arrays in one dimension and in multiple dimensions. ### One-dimensional subarrays A potentially confusing case is when the ``step`` value is negative. In this case, the defaults for ``start`` and ``stop`` are swapped. This becomes a convenient way to reverse an array: ### Multi-dimensional subarrays  Multi-dimensional slices work in the same way, with multiple slices separated by commas. For example: Finally, subarray dimensions can even be reversed together: #### Accessing array rows and columns  One commonly needed routine is accessing of single rows or columns of an array. This can be done by combining indexing and slicing, using an empty slice marked by a single colon (``:``): In the case of row access, the empty slice can be omitted for a more compact syntax: ### Subarrays as no-copy views  One important–and extremely useful–thing to know about array slices is that they return *views* rather than *copies* of the array data. This is one area in which NumPy array slicing differs from Python list slicing: in lists, slices will be copies. Consider our two-dimensional array from before: Let\\'s extract a $2 \\\\times 2$ subarray from this: Now if we modify this subarray, we\\'ll see that the original array is changed! Observe: This default behavior is actually quite useful: it means that when we work with large datasets, we can access and process pieces of these datasets without the need to copy the underlying data buffer. ### Creating copies of arrays  Despite the nice features of array views, it is sometimes useful to instead explicitly copy the data within an array or a subarray. This can be most easily done with the ``copy()`` method: If we now modify this subarray, the original array is not touched: ## Reshaping of Arrays  Another useful type of operation is reshaping of arrays. The most flexible way of doing this is with the ``reshape`` method. For example, if you want to put the numbers 1 through 9 in a $3 \\\\times 3$ grid, you can do the following: Note that for this to work, the size of the initial array must match the size of the reshaped array. Where possible, the ``reshape`` method will use a no-copy view of the initial array, but with non-contiguous memory buffers this is not always the case.  Another common reshaping pattern is the conversion of a one-dimensional array into a two-dimensional row or column matrix. This can be done with the ``reshape`` method, or more easily done by making use of the ``newaxis`` keyword within a slice operation: We will see this type of transformation often throughout the remainder of the book. ## Array Concatenation and Splitting  All of the preceding routines worked on single arrays. It\\'s also possible to combine multiple arrays into one, and to conversely split a single array into multiple arrays. We\\'ll take a look at those operations here. ### Concatenation of arrays  Concatenation, or joining of two arrays in NumPy, is primarily accomplished using the routines ``np.concatenate``, ``np.vstack``, and ``np.hstack``. ``np.concatenate`` takes a tuple or list of arrays as its first argument, as we can see here: You can also concatenate more than two arrays at once: It can also be used for two-dimensional arrays: For working with arrays of mixed dimensions, it can be clearer to use the ``np.vstack`` (vertical stack) and ``np.hstack`` (horizontal stack) functions: Similary, ``np.dstack`` will stack arrays along the third axis. ### Splitting of arrays  The opposite of concatenation is splitting, which is implemented by the functions ``np.split``, ``np.hsplit``, and ``np.vsplit``.  For each of these, we can pass a list of indices giving the split points: Notice that *N* split-points, leads to *N + 1* subarrays. The related functions ``np.hsplit`` and ``np.vsplit`` are similar: Similarly, ``np.dsplit`` will split arrays along the third axis. <!--NAVIGATION--> < [Understanding Data Types in Python](02.01-Understanding-Data-Types.ipynb) | [Contents](Index.ipynb) | [Computation on NumPy Arrays: Universal Functions](02.03-Computation-on-arrays-ufuncs.ipynb) >  <a href=\"https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.02-The-Basics-Of-NumPy-Arrays.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>',\n",
       " 'import numpy as np np.random.seed(0)  # seed for reproducibility  x1 = np.random.randint(10, size=6)  # One-dimensional array x2 = np.random.randint(10, size=(3, 4))  # Two-dimensional array x3 = np.random.randint(10, size=(3, 4, 5))  # Three-dimensional array print(\"x3 ndim: \", x3.ndim) print(\"x3 shape:\", x3.shape) print(\"x3 size: \", x3.size) print(\"dtype:\", x3.dtype) print(\"itemsize:\", x3.itemsize, \"bytes\") print(\"nbytes:\", x3.nbytes, \"bytes\") x1 x1[0] x1[4] x1[-1] x1[-2] x2 x2[0, 0] x2[2, 0] x2[2, -1] x2[0, 0] = 12 x2 x1[0] = 3.14159  # this will be truncated! x1 x = np.arange(10) x x[:5]  # first five elements x[5:]  # elements after index 5 x[4:7]  # middle sub-array x[::2]  # every other element x[1::2]  # every other element, starting at index 1 x[::-1]  # all elements, reversed x[5::-2]  # reversed every other from index 5 x2 x2[:2, :3]  # two rows, three columns x2[:3, ::2]  # all rows, every other column x2[::-1, ::-1] print(x2[:, 0])  # first column of x2 print(x2[0, :])  # first row of x2 print(x2[0])  # equivalent to x2[0, :] print(x2) x2_sub = x2[:2, :2] print(x2_sub) x2_sub[0, 0] = 99 print(x2_sub) print(x2) x2_sub_copy = x2[:2, :2].copy() print(x2_sub_copy) x2_sub_copy[0, 0] = 42 print(x2_sub_copy) print(x2) grid = np.arange(1, 10).reshape((3, 3)) print(grid) x = np.array([1, 2, 3])  # row vector via reshape x.reshape((1, 3)) # row vector via newaxis x[np.newaxis, :] # column vector via reshape x.reshape((3, 1)) # column vector via newaxis x[:, np.newaxis] x = np.array([1, 2, 3]) y = np.array([3, 2, 1]) np.concatenate([x, y]) z = [99, 99, 99] print(np.concatenate([x, y, z])) grid = np.array([[1, 2, 3], [4, 5, 6]]) # concatenate along the first axis np.concatenate([grid, grid]) # concatenate along the second axis (zero-indexed) np.concatenate([grid, grid], axis=1) x = np.array([1, 2, 3]) grid = np.array([[9, 8, 7], [6, 5, 4]])  # vertically stack the arrays np.vstack([x, grid]) # horizontally stack the arrays y = np.array([[99], [99]]) np.hstack([grid, y]) x = [1, 2, 3, 99, 99, 3, 2, 1] x1, x2, x3 = np.split(x, [3, 5]) print(x1, x2, x3) grid = np.arange(16).reshape((4, 4)) grid upper, lower = np.vsplit(grid, [2]) print(upper) print(lower) left, right = np.hsplit(grid, [2]) print(left) print(right)',\n",
       " 'x3 ndim:  3\\n x3 shape: (3, 4, 5)\\n x3 size:  60\\n dtype: int64\\n itemsize: 8 bytes\\n nbytes: 480 bytes\\n array([5, 0, 3, 3, 7, 9]) 5 7 9 7 array([[3, 5, 2, 4],\\n        [7, 6, 8, 8],\\n        [1, 6, 7, 7]]) 3 1 7 array([[12,  5,  2,  4],\\n        [ 7,  6,  8,  8],\\n        [ 1,  6,  7,  7]]) array([3, 0, 3, 3, 7, 9]) array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) array([0, 1, 2, 3, 4]) array([5, 6, 7, 8, 9]) array([4, 5, 6]) array([0, 2, 4, 6, 8]) array([1, 3, 5, 7, 9]) array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0]) array([5, 3, 1]) array([[12,  5,  2,  4],\\n        [ 7,  6,  8,  8],\\n        [ 1,  6,  7,  7]]) array([[12,  5,  2],\\n        [ 7,  6,  8]]) array([[12,  2],\\n        [ 7,  8],\\n        [ 1,  7]]) array([[ 7,  7,  6,  1],\\n        [ 8,  8,  6,  7],\\n        [ 4,  2,  5, 12]]) [12  7  1]\\n [12  5  2  4]\\n [12  5  2  4]\\n [[12  5  2  4]\\n  [ 7  6  8  8]\\n  [ 1  6  7  7]]\\n [[12  5]\\n  [ 7  6]]\\n [[99  5]\\n  [ 7  6]]\\n [[99  5  2  4]\\n  [ 7  6  8  8]\\n  [ 1  6  7  7]]\\n [[99  5]\\n  [ 7  6]]\\n [[42  5]\\n  [ 7  6]]\\n [[99  5  2  4]\\n  [ 7  6  8  8]\\n  [ 1  6  7  7]]\\n [[1 2 3]\\n  [4 5 6]\\n  [7 8 9]]\\n array([[1, 2, 3]]) array([[1, 2, 3]]) array([[1],\\n        [2],\\n        [3]]) array([[1],\\n        [2],\\n        [3]]) array([1, 2, 3, 3, 2, 1]) [ 1  2  3  3  2  1 99 99 99]\\n array([[1, 2, 3],\\n        [4, 5, 6],\\n        [1, 2, 3],\\n        [4, 5, 6]]) array([[1, 2, 3, 1, 2, 3],\\n        [4, 5, 6, 4, 5, 6]]) array([[1, 2, 3],\\n        [9, 8, 7],\\n        [6, 5, 4]]) array([[ 9,  8,  7, 99],\\n        [ 6,  5,  4, 99]]) [1 2 3] [99 99] [3 2 1]\\n array([[ 0,  1,  2,  3],\\n        [ 4,  5,  6,  7],\\n        [ 8,  9, 10, 11],\\n        [12, 13, 14, 15]]) [[0 1 2 3]\\n  [4 5 6 7]]\\n [[ 8  9 10 11]\\n  [12 13 14 15]]\\n [[ 0  1]\\n  [ 4  5]\\n  [ 8  9]\\n  [12 13]]\\n [[ 2  3]\\n  [ 6  7]\\n  [10 11]\\n  [14 15]]\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "read_ipynb('02.02-The-Basics-Of-NumPy-Arrays.ipynb',folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 805 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ipynb_dict = defaultdict()\n",
    "for file in files:\n",
    "    temp_dict = {}\n",
    "    values = read_ipynb(file,folder)\n",
    "    temp_dict['file_name'] = file\n",
    "    temp_dict['modules'] = values[0]\n",
    "    temp_dict['heading'] = values[1]\n",
    "    temp_dict['markdown_str'] = values[2]\n",
    "    temp_dict['code_str'] = values[3]\n",
    "    temp_dict['code_output_str'] = values[4]\n",
    "    ipynb_dict[file] = temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import numpy as np']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipynb_dict['02.02-The-Basics-Of-NumPy-Arrays.ipynb']['modules']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the dictionary by creating a index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>modules</th>\n",
       "      <th>heading</th>\n",
       "      <th>markdown_str</th>\n",
       "      <th>code_str</th>\n",
       "      <th>code_output_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00.00-Preface.ipynb</td>\n",
       "      <td>[]</td>\n",
       "      <td>[# Preface, ## What Is Data Science?, ## Who I...</td>\n",
       "      <td>&lt;!--BOOK_INFORMATION--&gt; &lt;img align=\"left\" styl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>01.00-IPython-Beyond-Normal-Python.ipynb</td>\n",
       "      <td>[]</td>\n",
       "      <td>[# IPython: Beyond Normal Python, ## Shell or ...</td>\n",
       "      <td>&lt;!--BOOK_INFORMATION--&gt; &lt;img align=\"left\" styl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>01.01-Help-And-Documentation.ipynb</td>\n",
       "      <td>[]</td>\n",
       "      <td>[# Help and Documentation in IPython, ## Acces...</td>\n",
       "      <td>&lt;!--BOOK_INFORMATION--&gt; &lt;img align=\"left\" styl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>01.02-Shell-Keyboard-Shortcuts.ipynb</td>\n",
       "      <td>[]</td>\n",
       "      <td>[# Keyboard Shortcuts in the IPython Shell, ##...</td>\n",
       "      <td>&lt;!--BOOK_INFORMATION--&gt; &lt;img align=\"left\" styl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>01.03-Magic-Commands.ipynb</td>\n",
       "      <td>[]</td>\n",
       "      <td>[# IPython Magic Commands, ## Pasting Code Blo...</td>\n",
       "      <td>&lt;!--BOOK_INFORMATION--&gt; &lt;img align=\"left\" styl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>05.13-Kernel-Density-Estimation.ipynb</td>\n",
       "      <td>[from mpl_toolkits.basemap import Basemap, fro...</td>\n",
       "      <td>[# In-Depth: Kernel Density Estimation, ## Mot...</td>\n",
       "      <td>&lt;!--BOOK_INFORMATION--&gt; &lt;img align=\"left\" styl...</td>\n",
       "      <td>%matplotlib inline import matplotlib.pyplot as...</td>\n",
       "      <td>1.0 1.0 (-0.2, 8) (-0.2, 8) (-0.2, 8) (-0.02, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>05.14-Image-Features.ipynb</td>\n",
       "      <td>[from itertools import chain, from skimage imp...</td>\n",
       "      <td>[# Application: A Face Detection Pipeline, ## ...</td>\n",
       "      <td>&lt;!--BOOK_INFORMATION--&gt; &lt;img align=\"left\" styl...</td>\n",
       "      <td>%matplotlib inline import matplotlib.pyplot as...</td>\n",
       "      <td>(13233, 62, 47) (30000, 62, 47) (30000, 62, 47...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>05.15-Learning-More.ipynb</td>\n",
       "      <td>[]</td>\n",
       "      <td>[# Further Machine Learning Resources, ## Mach...</td>\n",
       "      <td>&lt;!--BOOK_INFORMATION--&gt; &lt;img align=\"left\" styl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>06.00-Figure-Code.ipynb</td>\n",
       "      <td>[from helpers_05_08 import visualize_tree, fro...</td>\n",
       "      <td>[# Appendix: Figure Code, ## Broadcasting, # A...</td>\n",
       "      <td>&lt;!--BOOK_INFORMATION--&gt; &lt;img align=\"left\" styl...</td>\n",
       "      <td>%matplotlib inline import matplotlib.pyplot as...</td>\n",
       "      <td>Overwriting helpers_05_08.py\\n Overwriting hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>Index.ipynb</td>\n",
       "      <td>[]</td>\n",
       "      <td>[# Python Data Science Handbook, ## Table of C...</td>\n",
       "      <td># Python Data Science Handbook *Jake VanderPla...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   file_name  \\\n",
       "0                        00.00-Preface.ipynb   \n",
       "1   01.00-IPython-Beyond-Normal-Python.ipynb   \n",
       "2         01.01-Help-And-Documentation.ipynb   \n",
       "3       01.02-Shell-Keyboard-Shortcuts.ipynb   \n",
       "4                 01.03-Magic-Commands.ipynb   \n",
       "..                                       ...   \n",
       "63     05.13-Kernel-Density-Estimation.ipynb   \n",
       "64                05.14-Image-Features.ipynb   \n",
       "65                 05.15-Learning-More.ipynb   \n",
       "66                   06.00-Figure-Code.ipynb   \n",
       "67                               Index.ipynb   \n",
       "\n",
       "                                              modules  \\\n",
       "0                                                  []   \n",
       "1                                                  []   \n",
       "2                                                  []   \n",
       "3                                                  []   \n",
       "4                                                  []   \n",
       "..                                                ...   \n",
       "63  [from mpl_toolkits.basemap import Basemap, fro...   \n",
       "64  [from itertools import chain, from skimage imp...   \n",
       "65                                                 []   \n",
       "66  [from helpers_05_08 import visualize_tree, fro...   \n",
       "67                                                 []   \n",
       "\n",
       "                                              heading  \\\n",
       "0   [# Preface, ## What Is Data Science?, ## Who I...   \n",
       "1   [# IPython: Beyond Normal Python, ## Shell or ...   \n",
       "2   [# Help and Documentation in IPython, ## Acces...   \n",
       "3   [# Keyboard Shortcuts in the IPython Shell, ##...   \n",
       "4   [# IPython Magic Commands, ## Pasting Code Blo...   \n",
       "..                                                ...   \n",
       "63  [# In-Depth: Kernel Density Estimation, ## Mot...   \n",
       "64  [# Application: A Face Detection Pipeline, ## ...   \n",
       "65  [# Further Machine Learning Resources, ## Mach...   \n",
       "66  [# Appendix: Figure Code, ## Broadcasting, # A...   \n",
       "67  [# Python Data Science Handbook, ## Table of C...   \n",
       "\n",
       "                                         markdown_str  \\\n",
       "0   <!--BOOK_INFORMATION--> <img align=\"left\" styl...   \n",
       "1   <!--BOOK_INFORMATION--> <img align=\"left\" styl...   \n",
       "2   <!--BOOK_INFORMATION--> <img align=\"left\" styl...   \n",
       "3   <!--BOOK_INFORMATION--> <img align=\"left\" styl...   \n",
       "4   <!--BOOK_INFORMATION--> <img align=\"left\" styl...   \n",
       "..                                                ...   \n",
       "63  <!--BOOK_INFORMATION--> <img align=\"left\" styl...   \n",
       "64  <!--BOOK_INFORMATION--> <img align=\"left\" styl...   \n",
       "65  <!--BOOK_INFORMATION--> <img align=\"left\" styl...   \n",
       "66  <!--BOOK_INFORMATION--> <img align=\"left\" styl...   \n",
       "67  # Python Data Science Handbook *Jake VanderPla...   \n",
       "\n",
       "                                             code_str  \\\n",
       "0                                                       \n",
       "1                                                       \n",
       "2                                                       \n",
       "3                                                       \n",
       "4                                                       \n",
       "..                                                ...   \n",
       "63  %matplotlib inline import matplotlib.pyplot as...   \n",
       "64  %matplotlib inline import matplotlib.pyplot as...   \n",
       "65                                                      \n",
       "66  %matplotlib inline import matplotlib.pyplot as...   \n",
       "67                                                      \n",
       "\n",
       "                                      code_output_str  \n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "..                                                ...  \n",
       "63  1.0 1.0 (-0.2, 8) (-0.2, 8) (-0.2, 8) (-0.02, ...  \n",
       "64  (13233, 62, 47) (30000, 62, 47) (30000, 62, 47...  \n",
       "65                                                     \n",
       "66  Overwriting helpers_05_08.py\\n Overwriting hel...  \n",
       "67                                                     \n",
       "\n",
       "[68 rows x 6 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebooks_df = pd.DataFrame.from_dict(ipynb_dict,orient='index').reset_index(drop=True)\n",
    "notebooks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Search Setup Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from elasticsearch import Elasticsearch # <== May need to pip install this\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = 'http://localhost:9200/'\n",
    "es = Elasticsearch(hosts=[HOST])\n",
    "\n",
    "INDEX=\"handboek\"\n",
    "TYPE= \"record\"\n",
    "\n",
    "def rec_to_actions(df):\n",
    "    for record in df.to_dict(orient=\"records\"):\n",
    "        yield ('{ \"index\" : { \"_index\" : \"%s\", \"_type\" : \"%s\" }}'% (INDEX, TYPE))\n",
    "        yield (json.dumps(record, default=int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the dataframe .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40dbacceac743e4aca22a54d397e65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# to dataframe\n",
    "# df = pd.read_dict(wiki_dict,orient='index')\n",
    "\n",
    "\n",
    "def index_marks(nrows, chunk_size):\n",
    "    return range(1 * chunk_size, (nrows // chunk_size + 1) * chunk_size, chunk_size)\n",
    "\n",
    "\n",
    "def split(dfm, chunk_size):\n",
    "    indices = index_marks(dfm.shape[0], chunk_size)\n",
    "    return np.split(dfm, indices) \n",
    "\n",
    "# split up the dataframe\n",
    "chunks = split(notebooks_df, 9000)\n",
    "\n",
    "# Now bulk index all the chunks\n",
    "c = len(chunks)\n",
    "for c in tqdm_notebook(chunks):\n",
    "    if c.shape[0]>0:\n",
    "        r = es.bulk(rec_to_actions(c)) # return a dict\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size\n",
      "yellow open   handboek 8SoUSayXT_OGv2TXxjdXhQ   1   1         68            0     16.2kb         16.2kb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   208  100   208    0     0  10947      0 --:--:-- --:--:-- --:--:-- 10947\n"
     ]
    }
   ],
   "source": [
    "# !curl \"http://localhost:9200/handboek\"\n",
    "# !curl -XDELETE \"localhost:9200/handboek\"\n",
    "\n",
    "!curl \"http://localhost:9200/_cat/indices?v\"\n",
    "\n",
    "\n",
    "# !curl -XPOST \"http://localhost:9200/_shutdown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_match_query(code,markdown,modules,highlight):\n",
    "    query_str = ''\n",
    "    fields = []\n",
    "    \n",
    "    if code != None:\n",
    "        fields += ['code_str']\n",
    "        query_str += ' '+code\n",
    "    if markdown != None:\n",
    "        fields += ['markdown_str']\n",
    "        query_str += ' '+markdown\n",
    "    if modules != None:\n",
    "        fields += ['modules']\n",
    "        query_str += ' '+modules   \n",
    "    \n",
    "        \n",
    "    q = {}\n",
    "    query_dict = {}\n",
    "    multi_match = {}\n",
    "    \n",
    "    multi_match['fields'] = fields\n",
    "    multi_match['query'] = query_str\n",
    "    query_dict['multi_match'] = multi_match\n",
    "    q['query'] = query_dict\n",
    "    \n",
    "    if highlight != None:\n",
    "        highlight = {\"pre_tags\":[\"<b>\"],\n",
    "            \"post_tags\":[\"</b>\"],\n",
    "            \"fields\":{'markdown_str':{}}}\n",
    "        q['highlight'] = highlight\n",
    "    return q\n",
    "\n",
    "def query_string_query(code,markdown,modules,highlight):\n",
    "    query_str = ''\n",
    "    fields = []\n",
    "    \n",
    "    if code != None:\n",
    "        fields += ['code_str']\n",
    "        query_str += '(code_str:'+code+')'\n",
    "    if markdown != None:\n",
    "        fields += ['markdown_str']\n",
    "        if query_str != '':\n",
    "            query_str += ' AND '\n",
    "        query_str += '(markdown_str:'+markdown+')'\n",
    "    if modules != None:\n",
    "        fields += ['modules']\n",
    "        if query_str != '':\n",
    "            query_str += ' AND '\n",
    "        query_str += '(modules:'+modules+')'  \n",
    "    \n",
    "        \n",
    "    q = {}\n",
    "    query_dict = {}\n",
    "    query_string = {}\n",
    "    \n",
    "    query_string['query'] = query_str\n",
    "    query_dict['query_string'] = query_string\n",
    "    q['query'] = query_dict\n",
    "    \n",
    "    if highlight == True:\n",
    "        highlight = {\"pre_tags\":[\"<b>\"],\n",
    "                     \"post_tags\":[\"</b>\"],\n",
    "                     \"order\":\"score\",\n",
    "                     \"fields\":{'markdown_str':{},'code_str':{}}}\n",
    "        q['highlight'] = highlight\n",
    "    return q\n",
    "\n",
    "# \"order\":\"score\",\n",
    "# \"fields\":{'_all':{}}} WERKT NIET....\n",
    "\n",
    "\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/6.8/search-request-highlighting.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-multi-match-query.html\n",
    "\n",
    "# {\n",
    "#   \"query\": {\n",
    "#     \"multi_match\" : {\n",
    "#       \"query\":    \"this is a test\", \n",
    "#       \"fields\": [ \"subject\", \"message\" ] \n",
    "#     }\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html\n",
    "\n",
    "# {\n",
    "#     \"query\": {\n",
    "#         \"query_string\": {\n",
    "#             \"query\": \"(content:this OR name:this) AND (content:that OR name:that)\"\n",
    "#         }\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'multi_match': {'fields': ['code_str', 'modules'],\n",
       "   'query': ' linear numpy'}},\n",
       " 'highlight': {'pre_tags': ['<b>'],\n",
       "  'post_tags': ['</b>'],\n",
       "  'fields': {'markdown_str': {}}}}"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_test_multimatch = multi_match_query('linear',None,'numpy',True)\n",
    "q_test_multimatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'query_string': {'query': '(markdown_str:drop) AND (modules:pandas)'}},\n",
       " 'highlight': {'pre_tags': ['<b>'],\n",
       "  'post_tags': ['</b>'],\n",
       "  'order': 'score',\n",
       "  'fields': {'_all': {}}}}"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_test_querystring = query_string_query(None,'drop','pandas',True)\n",
    "q_test_querystring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search_results(query,max_res):\n",
    "#     result = es.search(body=query, size=max_res)\n",
    "#     transcripts = \"\"\n",
    "#     for i in range(len(result['hits']['hits'])): # for all hits\n",
    "#         res = result['hits']['hits'][i]\n",
    "#         modules = res['_source']['modules']\n",
    "#         #len(res['_source']['modules'])\n",
    "#         print(res['_score'],'https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/'+res['_source']['file_name'])\n",
    "#         print(res['highlight']['markdown_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_test = multi_match_query(None,'linear regression',None,False)\n",
    "# print(q_test)\n",
    "# # print()\n",
    "\n",
    "# search_results(q_test,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'handboek',\n",
       " '_type': 'record',\n",
       " '_id': 'LTR2YnABgBRbd-7Oab31',\n",
       " '_score': 4.1406717,\n",
       " '_source': {'file_name': '05.07-Support-Vector-Machines.ipynb',\n",
       "  'modules': ['from ipywidgets import interact, fixed',\n",
       "   'from mpl_toolkits import mplot3d',\n",
       "   'from scipy import stats',\n",
       "   'from sklearn.cross_validation import train_test_split',\n",
       "   'from sklearn.datasets import fetch_lfw_people',\n",
       "   'from sklearn.datasets.samples_generator import make_blobs',\n",
       "   'from sklearn.datasets.samples_generator import make_circles',\n",
       "   'from sklearn.decomposition import RandomizedPCA',\n",
       "   'from sklearn.grid_search import GridSearchCV',\n",
       "   'from sklearn.metrics import classification_report',\n",
       "   'from sklearn.metrics import confusion_matrix',\n",
       "   'from sklearn.pipeline import make_pipeline',\n",
       "   'from sklearn.svm import SVC',\n",
       "   'from sklearn.svm import SVC # \"Support vector classifier\"',\n",
       "   'import matplotlib.pyplot as plt',\n",
       "   'import numpy as np',\n",
       "   'import seaborn as sns; sns.set()'],\n",
       "  'heading': ['# In-Depth: Support Vector Machines',\n",
       "   '# use seaborn plotting defaults',\n",
       "   '## Motivating Support Vector Machines',\n",
       "   '## Support Vector Machines: Maximizing the *Margin*',\n",
       "   '### Fitting a support vector machine',\n",
       "   '# create grid to evaluate model',\n",
       "   '# plot decision boundary and margins',\n",
       "   '# plot support vectors',\n",
       "   '### Beyond linear boundaries: Kernel SVM',\n",
       "   '### Tuning the SVM: Softening Margins',\n",
       "   '## Example: Face Recognition',\n",
       "   '## Support Vector Machine Summary'],\n",
       "  'markdown_str': '<!--BOOK_INFORMATION--> <img align=\"left\" style=\"padding-right:10px;\" src=\"figures/PDSH-cover-small.png\">  *This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*  *The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book](http://shop.oreilly.com/product/0636920034919.do)!* <!--NAVIGATION--> < [In Depth: Linear Regression](05.06-Linear-Regression.ipynb) | [Contents](Index.ipynb) | [In-Depth: Decision Trees and Random Forests](05.08-Random-Forests.ipynb) >  <a href=\"https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.07-Support-Vector-Machines.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a> # In-Depth: Support Vector Machines Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression. In this section, we will develop the intuition behind support vector machines and their use in classification problems.  We begin with the standard imports: ## Motivating Support Vector Machines As part of our disussion of Bayesian classification (see [In Depth: Naive Bayes Classification](05.05-Naive-Bayes.ipynb)), we learned a simple model describing the distribution of each underlying class, and used these generative models to probabilistically determine labels for new points. That was an example of *generative classification*; here we will consider instead *discriminative classification*: rather than modeling each class, we simply find a line or curve (in two dimensions) or manifold (in multiple dimensions) that divides the classes from each other.  As an example of this, consider the simple case of a classification task, in which the two classes of points are well separated: A linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification. For two dimensional data like that shown here, this is a task we could do by hand. But immediately we see a problem: there is more than one possible dividing line that can perfectly discriminate between the two classes!  We can draw them as follows: These are three *very* different separators which, nevertheless, perfectly discriminate between these samples. Depending on which you choose, a new data point (e.g., the one marked by the \"X\" in this plot) will be assigned a different label! Evidently our simple intuition of \"drawing a line between classes\" is not enough, and we need to think a bit deeper. ## Support Vector Machines: Maximizing the *Margin*  Support vector machines offer one way to improve on this. The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a *margin* of some width, up to the nearest point. Here is an example of how this might look: In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model. Support vector machines are an example of such a *maximum margin* estimator. ### Fitting a support vector machine  Let\\'s see the result of an actual fit to this data: we will use Scikit-Learn\\'s support vector classifier to train an SVM model on this data. For the time being, we will use a linear kernel and set the ``C`` parameter to a very large number (we\\'ll discuss the meaning of these in more depth momentarily). To better visualize what\\'s happening here, let\\'s create a quick convenience function that will plot SVM decision boundaries for us: This is the dividing line that maximizes the margin between the two sets of points. Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure. These points are the pivotal elements of this fit, and are known as the *support vectors*, and give the algorithm its name. In Scikit-Learn, the identity of these points are stored in the ``support_vectors_`` attribute of the classifier: A key to this classifier\\'s success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit! Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.  We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset: In the left panel, we see the model and the support vectors for 60 training points. In the right panel, we have doubled the number of training points, but the model has not changed: the three support vectors from the left panel are still the support vectors from the right panel. This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model. If you are running this notebook live, you can use IPython\\'s interactive widgets to view this feature of the SVM model interactively: ### Beyond linear boundaries: Kernel SVM  Where SVM becomes extremely powerful is when it is combined with *kernels*. We have seen a version of kernels before, in the basis function regressions of [In Depth: Linear Regression](05.06-Linear-Regression.ipynb). There we projected our data into higher-dimensional space defined by polynomials and Gaussian basis functions, and thereby were able to fit for nonlinear relationships with a linear classifier.  In SVM models, we can use a version of the same idea. To motivate the need for kernels, let\\'s look at some data that is not linearly separable: It is clear that no linear discrimination will *ever* be able to separate this data. But we can draw a lesson from the basis function regressions in [In Depth: Linear Regression](05.06-Linear-Regression.ipynb), and think about how we might project the data into a higher dimension such that a linear separator *would* be sufficient. For example, one simple projection we could use would be to compute a *radial basis function* centered on the middle clump: We can visualize this extra data dimension using a three-dimensional plot—if you are running this notebook live, you will be able to use the sliders to rotate the plot: We can see that with this additional dimension, the data becomes trivially linearly separable, by drawing a separating plane at, say, *r*=0.7.  Here we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results. In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use.  One strategy to this end is to compute a basis function centered at *every* point in the dataset, and let the SVM algorithm sift through the results. This type of basis function transformation is known as a *kernel transformation*, as it is based on a similarity relationship (or kernel) between each pair of points.  A potential problem with this strategy—projecting $N$ points into $N$ dimensions—is that it might become very computationally intensive as $N$ grows large. However, because of a neat little procedure known as the [*kernel trick*](https://en.wikipedia.org/wiki/Kernel_trick), a fit on kernel-transformed data can be done implicitly—that is, without ever building the full $N$-dimensional representation of the kernel projection! This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.  In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the ``kernel`` model hyperparameter: Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary. This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used. ### Tuning the SVM: Softening Margins  Our discussion thus far has centered around very clean datasets, in which a perfect decision boundary exists. But what if your data has some amount of overlap? For example, you may have data like this: To handle this case, the SVM implementation has a bit of a fudge-factor which \"softens\" the margin: that is, it allows some of the points to creep into the margin if that allows a better fit. The hardness of the margin is controlled by a tuning parameter, most often known as $C$. For very large $C$, the margin is hard, and points cannot lie in it. For smaller $C$, the margin is softer, and can grow to encompass some points.  The plot shown below gives a visual picture of how a changing $C$ parameter affects the final fit, via the softening of the margin: The optimal value of the $C$ parameter will depend on your dataset, and should be tuned using cross-validation or a similar procedure (refer back to [Hyperparameters and Model Validation](05.03-Hyperparameters-and-Model-Validation.ipynb)). ## Example: Face Recognition  As an example of support vector machines in action, let\\'s take a look at the facial recognition problem. We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures. A fetcher for the dataset is built into Scikit-Learn: Let\\'s plot a few of these faces to see what we\\'re working with: Each image contains [62×47] or nearly 3,000 pixels. We could proceed by simply using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here we will use a principal component analysis (see [In Depth: Principal Component Analysis](05.09-Principal-Component-Analysis.ipynb)) to extract 150 fundamental components to feed into our support vector machine classifier. We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline: For the sake of testing our classifier output, we will split the data into a training and testing set: Finally, we can use a grid search cross-validation to explore combinations of parameters. Here we will adjust ``C`` (which controls the margin hardness) and ``gamma`` (which controls the size of the radial basis function kernel), and determine the best model: The optimal values fall toward the middle of our grid; if they fell at the edges, we would want to expand the grid to make sure we have found the true optimum.  Now with this cross-validated model, we can predict the labels for the test data, which the model has not yet seen: Let\\'s take a look at a few of the test images along with their predicted values: Out of this small sample, our optimal estimator mislabeled only a single face (Bush’s face in the bottom row was mislabeled as Blair). We can get a better sense of our estimator\\'s performance using the classification report, which lists recovery statistics label by label: We might also display the confusion matrix between these classes: This helps us get a sense of which labels are likely to be confused by the estimator.  For a real-world facial recognition task, in which the photos do not come pre-cropped into nice grids, the only difference in the facial classification scheme is the feature selection: you would need to use a more sophisticated algorithm to find the faces, and extract features that are independent of the pixellation. For this kind of application, one good option is to make use of [OpenCV](http://opencv.org), which, among other things, includes pre-trained implementations of state-of-the-art feature extraction tools for images in general and faces in particular. ## Support Vector Machine Summary  We have seen here a brief intuitive introduction to the principals behind support vector machines. These methods are a powerful classification method for a number of reasons:  - Their dependence on relatively few support vectors means that they are very compact models, and take up very little memory. - Once the model is trained, the prediction phase is very fast. - Because they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is a challenging regime for other algorithms. - Their integration with kernel methods makes them very versatile, able to adapt to many types of data.  However, SVMs have several disadvantages as well:  - The scaling with the number of samples $N$ is $\\\\mathcal{O}[N^3]$ at worst, or $\\\\mathcal{O}[N^2]$ for efficient implementations. For large numbers of training samples, this computational cost can be prohibitive. - The results are strongly dependent on a suitable choice for the softening parameter $C$. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size. - The results do not have a direct probabilistic interpretation. This can be estimated via an internal cross-validation (see the ``probability`` parameter of ``SVC``), but this extra estimation is costly.  With those traits in mind, I generally only turn to SVMs once other simpler, faster, and less tuning-intensive methods have been shown to be insufficient for my needs. Nevertheless, if you have the CPU cycles to commit to training and cross-validating an SVM on your data, the method can lead to excellent results. <!--NAVIGATION--> < [In Depth: Linear Regression](05.06-Linear-Regression.ipynb) | [Contents](Index.ipynb) | [In-Depth: Decision Trees and Random Forests](05.08-Random-Forests.ipynb) >  <a href=\"https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.07-Support-Vector-Machines.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>',\n",
       "  'code_str': '%matplotlib inline import numpy as np import matplotlib.pyplot as plt from scipy import stats  # use seaborn plotting defaults import seaborn as sns; sns.set() from sklearn.datasets.samples_generator import make_blobs X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\\'autumn\\'); xfit = np.linspace(-1, 3.5) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\\'autumn\\') plt.plot([0.6], [2.1], \\'x\\', color=\\'red\\', markeredgewidth=2, markersize=10)  for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]: plt.plot(xfit, m * xfit + b, \\'-k\\')  plt.xlim(-1, 3.5); xfit = np.linspace(-1, 3.5) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\\'autumn\\')  for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]: yfit = m * xfit + b plt.plot(xfit, yfit, \\'-k\\') plt.fill_between(xfit, yfit - d, yfit + d, edgecolor=\\'none\\', color=\\'#AAAAAA\\', alpha=0.4)  plt.xlim(-1, 3.5); from sklearn.svm import SVC # \"Support vector classifier\" model = SVC(kernel=\\'linear\\', C=1E10) model.fit(X, y) def plot_svc_decision_function(model, ax=None, plot_support=True): \"\"\"Plot the decision function for a 2D SVC\"\"\" if ax is None: ax = plt.gca() xlim = ax.get_xlim() ylim = ax.get_ylim()  # create grid to evaluate model x = np.linspace(xlim[0], xlim[1], 30) y = np.linspace(ylim[0], ylim[1], 30) Y, X = np.meshgrid(y, x) xy = np.vstack([X.ravel(), Y.ravel()]).T P = model.decision_function(xy).reshape(X.shape)  # plot decision boundary and margins ax.contour(X, Y, P, colors=\\'k\\', levels=[-1, 0, 1], alpha=0.5, linestyles=[\\'--\\', \\'-\\', \\'--\\'])  # plot support vectors if plot_support: ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, linewidth=1, facecolors=\\'none\\'); ax.set_xlim(xlim) ax.set_ylim(ylim) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\\'autumn\\') plot_svc_decision_function(model); model.support_vectors_ def plot_svm(N=10, ax=None): X, y = make_blobs(n_samples=200, centers=2, random_state=0, cluster_std=0.60) X = X[:N] y = y[:N] model = SVC(kernel=\\'linear\\', C=1E10) model.fit(X, y)  ax = ax or plt.gca() ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\\'autumn\\') ax.set_xlim(-1, 4) ax.set_ylim(-1, 6) plot_svc_decision_function(model, ax)  fig, ax = plt.subplots(1, 2, figsize=(16, 6)) fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1) for axi, N in zip(ax, [60, 120]): plot_svm(N, axi) axi.set_title(\\'N = {0}\\'.format(N)) from ipywidgets import interact, fixed interact(plot_svm, N=[10, 200], ax=fixed(None)); from sklearn.datasets.samples_generator import make_circles X, y = make_circles(100, factor=.1, noise=.1)  clf = SVC(kernel=\\'linear\\').fit(X, y)  plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\\'autumn\\') plot_svc_decision_function(clf, plot_support=False); r = np.exp(-(X ** 2).sum(1)) from mpl_toolkits import mplot3d  def plot_3D(elev=30, azim=30, X=X, y=y): ax = plt.subplot(projection=\\'3d\\') ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap=\\'autumn\\') ax.view_init(elev=elev, azim=azim) ax.set_xlabel(\\'x\\') ax.set_ylabel(\\'y\\') ax.set_zlabel(\\'r\\')  interact(plot_3D, elev=[-90, 90], azip=(-180, 180), X=fixed(X), y=fixed(y)); clf = SVC(kernel=\\'rbf\\', C=1E6) clf.fit(X, y) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\\'autumn\\') plot_svc_decision_function(clf) plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=300, lw=1, facecolors=\\'none\\'); X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=1.2) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\\'autumn\\'); X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.8)  fig, ax = plt.subplots(1, 2, figsize=(16, 6)) fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)  for axi, C in zip(ax, [10.0, 0.1]): model = SVC(kernel=\\'linear\\', C=C).fit(X, y) axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\\'autumn\\') plot_svc_decision_function(model, axi) axi.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, lw=1, facecolors=\\'none\\'); axi.set_title(\\'C = {0:.1f}\\'.format(C), size=14) from sklearn.datasets import fetch_lfw_people faces = fetch_lfw_people(min_faces_per_person=60) print(faces.target_names) print(faces.images.shape) fig, ax = plt.subplots(3, 5) for i, axi in enumerate(ax.flat): axi.imshow(faces.images[i], cmap=\\'bone\\') axi.set(xticks=[], yticks=[], xlabel=faces.target_names[faces.target[i]]) from sklearn.svm import SVC from sklearn.decomposition import RandomizedPCA from sklearn.pipeline import make_pipeline  pca = RandomizedPCA(n_components=150, whiten=True, random_state=42) svc = SVC(kernel=\\'rbf\\', class_weight=\\'balanced\\') model = make_pipeline(pca, svc) from sklearn.cross_validation import train_test_split Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=42) from sklearn.grid_search import GridSearchCV param_grid = {\\'svc__C\\': [1, 5, 10, 50], \\'svc__gamma\\': [0.0001, 0.0005, 0.001, 0.005]} grid = GridSearchCV(model, param_grid)  %time grid.fit(Xtrain, ytrain) print(grid.best_params_) model = grid.best_estimator_ yfit = model.predict(Xtest) fig, ax = plt.subplots(4, 6) for i, axi in enumerate(ax.flat): axi.imshow(Xtest[i].reshape(62, 47), cmap=\\'bone\\') axi.set(xticks=[], yticks=[]) axi.set_ylabel(faces.target_names[yfit[i]].split()[-1], color=\\'black\\' if yfit[i] == ytest[i] else \\'red\\') fig.suptitle(\\'Predicted Names; Incorrect Labels in Red\\', size=14); from sklearn.metrics import classification_report print(classification_report(ytest, yfit, target_names=faces.target_names)) from sklearn.metrics import confusion_matrix mat = confusion_matrix(ytest, yfit) sns.heatmap(mat.T, square=True, annot=True, fmt=\\'d\\', cbar=False, xticklabels=faces.target_names, yticklabels=faces.target_names) plt.xlabel(\\'true label\\') plt.ylabel(\\'predicted label\\');',\n",
       "  'code_output_str': \"SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0,\\n   decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\\n   max_iter=-1, probability=False, random_state=None, shrinking=True,\\n   tol=0.001, verbose=False) SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0,\\n   decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\\n   max_iter=-1, probability=False, random_state=None, shrinking=True,\\n   tol=0.001, verbose=False) array([[ 0.44359863,  3.11530945],\\n        [ 2.33812285,  3.43116792],\\n        [ 2.06156753,  1.96918596]]) array([[ 0.44359863,  3.11530945],\\n        [ 2.33812285,  3.43116792],\\n        [ 2.06156753,  1.96918596]]) array([[ 0.44359863,  3.11530945],\\n        [ 2.33812285,  3.43116792],\\n        [ 2.06156753,  1.96918596]]) array([[ 0.44359863,  3.11530945],\\n        [ 2.33812285,  3.43116792],\\n        [ 2.06156753,  1.96918596]]) array([[ 0.44359863,  3.11530945],\\n        [ 2.33812285,  3.43116792],\\n        [ 2.06156753,  1.96918596]]) SVC(C=1000000.0, cache_size=200, class_weight=None, coef0=0.0,\\n   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\\n   max_iter=-1, probability=False, random_state=None, shrinking=True,\\n   tol=0.001, verbose=False) SVC(C=1000000.0, cache_size=200, class_weight=None, coef0=0.0,\\n   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\\n   max_iter=-1, probability=False, random_state=None, shrinking=True,\\n   tol=0.001, verbose=False) SVC(C=1000000.0, cache_size=200, class_weight=None, coef0=0.0,\\n   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\\n   max_iter=-1, probability=False, random_state=None, shrinking=True,\\n   tol=0.001, verbose=False) SVC(C=1000000.0, cache_size=200, class_weight=None, coef0=0.0,\\n   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\\n   max_iter=-1, probability=False, random_state=None, shrinking=True,\\n   tol=0.001, verbose=False) ['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'\\n  'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']\\n (1348, 62, 47)\\n ['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'\\n  'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']\\n (1348, 62, 47)\\n CPU times: user 47.8 s, sys: 4.08 s, total: 51.8 s\\n Wall time: 26 s\\n {'svc__gamma': 0.001, 'svc__C': 10}\\n CPU times: user 47.8 s, sys: 4.08 s, total: 51.8 s\\n Wall time: 26 s\\n {'svc__gamma': 0.001, 'svc__C': 10}\\n                    precision    recall  f1-score   support\\n \\n      Ariel Sharon       0.65      0.73      0.69        15\\n      Colin Powell       0.81      0.87      0.84        68\\n   Donald Rumsfeld       0.75      0.87      0.81        31\\n     George W Bush       0.93      0.83      0.88       126\\n Gerhard Schroeder       0.86      0.78      0.82        23\\n       Hugo Chavez       0.93      0.70      0.80        20\\n Junichiro Koizumi       0.80      1.00      0.89        12\\n        Tony Blair       0.83      0.93      0.88        42\\n \\n       avg / total       0.85      0.85      0.85       337\\n \\n                    precision    recall  f1-score   support\\n \\n      Ariel Sharon       0.65      0.73      0.69        15\\n      Colin Powell       0.81      0.87      0.84        68\\n   Donald Rumsfeld       0.75      0.87      0.81        31\\n     George W Bush       0.93      0.83      0.88       126\\n Gerhard Schroeder       0.86      0.78      0.82        23\\n       Hugo Chavez       0.93      0.70      0.80        20\\n Junichiro Koizumi       0.80      1.00      0.89        12\\n        Tony Blair       0.83      0.93      0.88        42\\n \\n       avg / total       0.85      0.85      0.85       337\\n \\n\"}}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['hits']['hits'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text \n",
    "normal_text = widgets.Text() #description=\"Search\", continuous_update=True)\n",
    "code = widgets.Text(description=\"Code\")\n",
    "markdown = widgets.Text(description=\"Markdown\")\n",
    "modules = widgets.Text(description=\"Modules\")\n",
    "\n",
    "# Buttons\n",
    "search_b_normal = widgets.Button(description=\"Search\")\n",
    "search_b_adv = widgets.Button(description=\"Search\")\n",
    "clear_b = widgets.Button(description=\"Clear\")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def on_normal_search_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        if normal_text.value == '':\n",
    "            print(\"Type a query\")\n",
    "        else:\n",
    "#             print(query_maker(normal_text.value,normal_text.value,normal_text.value,None))\n",
    "            display_results(multi_match_query(normal_text.value,normal_text.value,normal_text.value,True),20)\n",
    "\n",
    "def on_adv_search_clicked(b):\n",
    "    send_code = code.value\n",
    "    send_markdown = markdown.value\n",
    "    send_modules = modules.value\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        if code.value == \"\" and markdown.value=='' and modules.value=='':\n",
    "            print(\"Type a query\")\n",
    "        else:\n",
    "            if code.value == '':\n",
    "                send_code = None\n",
    "            if markdown.value == '':\n",
    "                send_markdown = None\n",
    "            if modules.value == '':\n",
    "                send_modules = None\n",
    "            print(query_string_query(send_code,send_markdown,send_modules,True))\n",
    "            display_results(query_string_query(send_code,send_markdown,send_modules,True),20)\n",
    "\n",
    "        \n",
    "def on_clear_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "    normal_text.value = ''\n",
    "    markdown.value = ''\n",
    "    code.value = ''\n",
    "    modules.value = ''\n",
    "    \n",
    "        \n",
    "def switch(check):\n",
    "    with output:                      # remove if you want results to stay when\n",
    "        output.clear_output()         # you switch between normal and adv search\n",
    "    if check['new']:\n",
    "        search_display.children=[row_adv,row_output]\n",
    "    else:\n",
    "        search_display.children=[row_normal,row_output]\n",
    "\n",
    "        \n",
    "def display_results(query,max_res):\n",
    "    result = es.search(body=query, size=max_res)\n",
    "    for i in range(len(result['hits']['hits'])): # for all hits\n",
    "        res = result['hits']['hits'][i]\n",
    "        modules = res['_source']['modules']\n",
    "        \n",
    "        res_layout = widgets.Layout(maring='0px')\n",
    "        \n",
    "        title = widgets.HTML(value='<b>'+res['_source']['file_name']+'</b>')#,layout=res_layout)\n",
    "        url_str = 'https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/'+res['_source']['file_name']\n",
    "        url = widgets.HTML(value='<a href='+url_str + ' target=\"_blank\">' + url_str + '</a>')\n",
    "        score = widgets.HTML(value=str(res['_score']))#,layout=res_layout)\n",
    "        \n",
    "        whole_layout = widgets.Layout(display='flex',\n",
    "                        flex_flow='column',\n",
    "                        align_items='stretch',\n",
    "                        border='grey solid 1px')\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            highlight_str = str(res['highlight'].values())\n",
    "            highlight = widgets.HTML(value=highlight_str,layout=res_layout)\n",
    "            single_result = widgets.VBox([title,score,url,highlight],layout=whole_layout)\n",
    "#             print(res['highlight'].keys())            \n",
    "#             print(highlight_str)            \n",
    "        except:\n",
    "            single_result = widgets.VBox([title,score,url],layout=whole_layout)\n",
    "        display(single_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a78496f3c24b95a3617073b567da0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Advanced Search')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f02852102c44ee3a247df94c2dea749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value=''), Button(description='Search', style=ButtonStyle()), Button(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_b_normal.on_click(on_normal_search_clicked)\n",
    "search_b_adv.on_click(on_adv_search_clicked)\n",
    "clear_b.on_click(on_clear_clicked)\n",
    "\n",
    "row_normal = widgets.HBox([normal_text,search_b_normal,clear_b])\n",
    "row_adv = widgets.HBox([markdown,code,modules,search_b_adv,clear_b])\n",
    "row_output = widgets.VBox([output])\n",
    "\n",
    "top_toggle = widgets.Checkbox(description='Advanced Search')\n",
    "top_toggle.observe(switch, names='value')\n",
    "display(top_toggle)\n",
    "\n",
    "normal_text.on_submit(on_normal_search_clicked)\n",
    "code.on_submit(on_adv_search_clicked)\n",
    "markdown.on_submit(on_adv_search_clicked)\n",
    "modules.on_submit(on_adv_search_clicked)\n",
    "\n",
    "\n",
    "search_display = widgets.VBox(children = [row_normal,row_output])\n",
    "display(search_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = es.search(body=query, size=max_res)\n",
    "#     transcripts = \"\"\n",
    "#     for i in range(len(result['hits']['hits'])): # for all hits\n",
    "#         res = result['hits']['hits'][i]\n",
    "#         modules = res['_source']['modules']\n",
    "#         #len(res['_source']['modules'])\n",
    "#         print(res['_score'],'https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/'+res['_source']['file_name'])\n",
    "#         print(res['highlight']['markdown_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_query = {'query': {'multi_match': {'fields': ['modules'], 'query': ' numpy'}}}\n",
    "# search_results(test_query,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enter toets gebruiken voor search:\n",
    "- link: https://stackoverflow.com/questions/47137370/getting-text-from-jupyter-text-widget\n",
    "- link: https://pythonprogs.blogspot.com/2017/01/widgets-for-jupyter-notebook-text-input.html\n",
    "\n",
    "advanced search toggle (advanced search tonen/verbergen):\n",
    "- link: https://stackoverflow.com/questions/54093955/ipywidgets-use-checkbox-to-display-or-hide-other-widgets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
